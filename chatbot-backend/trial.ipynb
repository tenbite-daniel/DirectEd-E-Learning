{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b9e22fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f896c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resources_links = [\n",
    "    # FastAPI\n",
    "    #\"https://projector-video-pdf-converter.datacamp.com/36910/chapter1.pdf\",\n",
    "    # Postman\n",
    "    \"https://learning.postman.com/docs/introduction/overview/\",\n",
    "    \"https://curl.se/docs/tutorial.html\",\n",
    "    \"https://everything.curl.dev/index.html\",\n",
    "    # Cors\n",
    "    \"https://www.geeksforgeeks.org/configuring-cors-in-fastapi/\",\n",
    "    \"https://pypi.org/project/fastapi-cors/\",\n",
    "    \"https://learn.microsoft.com/en-us/answers/questions/2115649/cors-error-in-fastapi-backend-from-react-frontend\",\n",
    "    \"https://fastapi.tiangolo.com/tutorial/\",\n",
    "    # Langchain\n",
    "    # \"https://archive.fosdem.org/2024/events/attachments/fosdem-2024-2384-langchain-from-0-to-1-unveiling-the-power-of-llm-programming/slides/22587/LangChain_From_0_To_1_public_1_PpuSgEN.pdf\",\n",
    "    ## LLMOps\n",
    "    \"https://code-b.dev/blog/llmops-for-machine-learning\",\n",
    "    \"https://signoz.io/guides/llmops/\",\n",
    "    \"https://www.analyticsvidhya.com/blog/2023/09/llmops-for-machine-learning-engineering/\",\n",
    "    \"https://www.datacamp.com/blog/llmops-essentials-guide-to-operationalizing-large-language-models\",\n",
    "    ## Data Management\n",
    "    \"https://www.ibm.com/topics/data-management\",\n",
    "    \"https://www.oracle.com/database/what-is-data-management/\",\n",
    "    \"https://docs.pinecone.io/docs/quickstart\",\n",
    "    \"https://weaviate.io/developers/weaviate/current/getting-started/quickstart.html\",\n",
    "    # Fine Tuning\n",
    "    \"https://cookbook.openai.com/articles/gpt-oss/fine-tune-transfomers\",\n",
    "    \"https://huggingface.co/blog/dvgodoy/fine-tuning-llm-hugging-face\",\n",
    "    \"https://huggingface.co/learn/llm-course/en/chapter3/1\",    \n",
    "\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.document_loaders import DirectoryLoader, PDFPlumberLoader, WebBaseLoader, TextLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09bf964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_loader = WebBaseLoader(resources_links)\n",
    "pdf_loaders = DirectoryLoader(\n",
    "    \"./knowledge/\",\n",
    "    loader_cls=PDFPlumberLoader,\n",
    "    glob=\"*.pdf\"\n",
    ")\n",
    "text_loader = DirectoryLoader(\n",
    "    \"./knowledge/\",\n",
    "    loader_cls=TextLoader,\n",
    "    glob=\"*.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "017ea2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_data = web_loader.load()\n",
    "pdf_data = pdf_loaders.load()\n",
    "text_data = text_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e5d92a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postman documentation overview | Postman DocsProductPricingEnterpriseResources and SupportAPI NetworkContact SalesSign InSign Up for FreePostman DocsIntroductionOverviewResourcesFeedbackTroubleshootingGet startedWelcomePostman first stepsOverviewDownloadSend a requestWrite a testSign inCreate a collectionExplore the Postman API NetworkNext stepsPostman basicsOverviewThe Postman interfacePostman elementsThe Postman AgentPostbotPostman Agent ModeCollaboration basicsSyncingLightweight API ClientScratch PadYour Postman accountOverviewSign upSettingsInstall and configureOverviewInstallation and updatesSettingsCustomize your profileProxy server configurationTry Postman EnterpriseOverviewFAQImport and export dataOverviewData import methodsSoapUI importHoppscotch importInsomnia importThunder Client importGit importNew Relic importcURL command importSwagger API importData exportSend requestsOverviewCreate requestsOverviewRequest basicsParameters and body dataRequest headersTest data storageCustomize request settingsGroup requests in collectionsGenerate client codeAuthentication and authorizationOverviewAdd certificatesRequest authorizationPublic API authorizationAuthorization typesDigestOAuth 1.0OAuth 2.0HawkAWS SignatureNTLMAkamai EdgegridASAP (Atlassian)Response data and cookiesOverviewAPI response structureVisualize request responsesCreate request response examplesCreate and send cookiesDebug requestsVariables and environmentsOverviewStore values in variablesCreate and use environmentsSet environment variablesManage team environmentsPinned environmentsPostman VaultOverviewManage vault keyVault secretsVault secrets for public APIsPostman Vault integrationsIntegrations overview1PasswordAWS Secrets ManagerAzure Key VaultHashiCorp VaultManage integrationsTroubleshoot vault secretsCapture API trafficOverviewCapture traffic with Postman proxyCapture HTTP requestsCapture HTTPS trafficCapture traffic with Postman InterceptorSync cookiesGraphQLOverviewCreate GraphQL requestsThe GraphQL interfaceOther GraphQL requestsgRPCOverviewThe gRPC interfaceInvoke a gRPC requestManage gRPC service definitionsTest gRPC requestsWrite tests for gRPC requestsPostman Sandbox APIgRPC mock serversgRPC request-response examplesgRPC protobuf typesWebSocketOverviewCreate a WebSocket requestCreate a Socket.IO requestWork with WebSocket messagesListen to Socket.IO eventsUse variables in WebSocket requestsAdd details to a WebSocket requestSave WebSocket requestsDocument WebSocket requestsTroubleshoot WebSocket requestsMQTTOverviewManage MQTT requestsCreate an MQTT requestSOAPSend SOAP requestsTests and scriptsOverviewWrite scriptsOverviewWrite pre-request scriptsWrite testsUse packages in scriptsOverviewReuse internal scriptsUse packages from external registriesScript examplesDynamic variablesPostman JavaScript referenceTest APIsOverviewIntegration testingEnd-to-end testingRegression testingPerformance testingRun and automate testsOverviewRun tests manuallyRun tests on a scheduleRun tests in CI/CD pipelinesRun tests with monitorsPostman CollectionsOverviewCreate and manage collectionsOverviewCreate collectionsAdd requests to collectionsManage and organize collectionsCollaborate with collectionsView collection activity and elementsRun collectionsOverviewCollection RunnerAutomate collection runsView scheduled collection runsSchedule collection runs with monitorsTrigger collection runsCustomize collection run orderRun a collection with imported dataTest API performanceOverviewRun a performance testView performance test metricsDebug performance test errorsInject data into virtual usersPostman CLIOverviewInstallCommand optionsRun a collectionBuilt-in reportersNewman CLIRun and test collections with Newman CLIInstall and run NewmanNewman command referenceUpload files with NewmanUse Newman built-in reportersUse Newman external and custom reportersNewman with DockerCI with NewmanTravis CI with NewmanJenkins with NewmanMonitor collectionsOverviewSet up a monitorView monitor resultsManage monitorsRun monitors using static IPsTroubleshoot monitorsMonitoring FAQCollaborate in PostmanOverviewCollaboration conceptsCollaboration featuresDefine rolesUse teamsManage user groupsManage accessShare your workCollaborate using Live SessionsComment on APIsDocument APIsOverviewDocument a collectionAdd API documentationWrite documentationPublish documentationView documentationHost API documentationSet up authentication for public APIsPrivate API NetworkOverviewExplore your Private API NetworkOrganize your Private API NetworkManage your Private API NetworkRequest additions to your Private API NetworkAutomate publishing to your Private API NetworkVersion controlOverviewFork Postman elementsCreate pull requestsReview pull requestsWatch pull requestsPublic elementsWorkspacesOverviewCreate workspacesUse workspacesWorkspace updatesView team activityManage workspacesPartner WorkspacesOverviewSet up a Partner WorkspaceCollaborate as a partnerMulti-partner WorkspacesManage Partner WorkspacesMigrate Partner WorkspacesPublic workspacesDesign and build APIsOverviewDesign APIs with collectionsOverviewTurn on types in collectionsAdd properties to parameters and headersAdd properties to body dataView properties and documentationGenerate API specificationsDesign APIs with specificationsOverviewCreate a specificationImport a specificationEdit a specificationAdd files to a specificationValidate a specificationView live documentationGenerate collectionsDevelop APIs with the API BuilderOverviewCreate an APIImport an APIAPI version controlOverviewCloud-hosted Git repositoryOn-premises Git repositoryManage multiple APIsManage Git changes in PostmanPublish an API versionDevelop an APIOverviewManage API definitionsAdd elements to an APIGenerate server-side codeValidate an APITest an APIDeploy an APIOverviewAmazon API GatewayApigeeAzure API ManagementObserve an APIOverviewNew RelicDatadogManage and share APIsMock an APIOverviewSet up mock serversMake calls to mock serversCreate dynamic mock responsesMatching algorithmMock server tutorialsOverviewMock with response examplesMock with the Postman APIAdminister PostmanOverviewOnboarding checklistMigrate data between teamsTeam managementOverviewCreate teamsManage team membersOverviewInvite membersManage rolesScale teamsManage team resourcesManage product accessConfigure team settingsManage API keysSecret ScannerOverviewHow the Secret Scanner worksThe Secret Scanner dashboardSecret Scanner patternsBYOK EncryptionAudit logsBillingOverviewManage billingPurchase PostmanAbout resource usageAbout Postman Flows usageIntegrationsOverviewCustom webhooksInstalled appsCI integrationsPostman integrationsAll integrationsAPIMaticApigeeAWS API GatewayAzure API ManagementAzure DevOpsAzure PipelinesBigPandaBitbucketBitbucket PipelinesCircleCICoralogixDatadogDropboxGitHubOverviewCollaborate on your collectionsOverviewConnect a repositoryCollaborate with your teamBack up your collectionGitHub ActionsGitLabGitLab CI/CDJiraOverviewConnect your Jira accountCreate a Jira issueView your Jira issuesJenkinsKeenMicrosoft Power AutomateMicrosoft TeamsOverviewPostman app for TeamsSend notifications to TeamsNew RelicOpenAPIOpsgeniePagerDutySlackOverviewPostman app for SlackGet Slack notificationsPersonal notification typesPost monitoring resultsPost team activitySplunkSplunk On-CallStatuspageTravis CIPostman EnterpriseOverviewEnterprise plansEnterprise onboardingEnterprise deploymentPostman EU Data ResidencyAPI GovernanceOverviewAPI rulesOverviewOpenAPI 3 rulesOpenAPI 2 rulesAPI request warningsOverviewSecurity warningsConfigurable rulesOverviewAPI Governance rulesAPI Security rulesCustom governance functionsSpectralReportsOverviewOverview reportMembers overview reportContent activity reportAPI Governance reportAPI Security reportBilling overview reportPublic workspace metrics reportAPI Builder reportSecret Scanner reportSingle Sign-On (SSO)Intro to SSOConfigure SSO for a teamSign in to an SSO teamSSO and SCIM FAQsMicrosoft AD FSMicrosoft Entra IDCustom SAMLDuoGoogle WorkspaceOktaOneLoginPing IdentitySCIM provisioningOverviewConfigure SCIM with OktaConfigure SCIM with Microsoft Entra IDConfigure SCIM with OneLoginDomain verification and captureOverviewAdd and verify a domainEnable domain captureDomain capture support and FAQsSecurityOverviewSecurity for developersSecurity for teamsAdmin FAQsPostman FlowsOverviewGet started with FlowsOverviewBuild your first flow moduleExplore the Flows CatalogBuild flowsOverviewCreate blocksCreate connectionsCreate a requestLoopsLoops overviewLoops with external dataBreak out of a loop in FlowsVisualize dataOrganize flowsVersion flows with snapshotsCopy flows with clonesDemo flows with slidesDeploy flows as actionsConfigure values for actionsCheck actions health and activityAssign input valuesWork with date and timeFind and filter dataTroubleshoot flowsShare and embed flowsPostbot in FlowsFlows referenceOverviewBlocksBlocks referenceActionsAction blocks overviewHTTP RequestFlow ModuleTriggerTrigger blocks overviewOutputRequestResponseStartAIAI blocks overviewAI AgentCreate with AIAI RequestLogicLogic blocks overviewConditionValidateIfEvaluateDelayLoopingLooping blocks overviewRepeatForCollectVisualizeVisualize blocks overviewDisplayLogDataData blocks overviewStringBoolNumberNullSelectNowDateDate and timeListRecordCreate variableGet variableTemplateGet configurationTypeScript in flowsUse TypeScript in flowsFlows Query LanguageOverviewGet basic valuesSelect conditional dataReturn structured dataManipulate dataFunction referenceLearn Flows with tutorialsOverviewBeginner tutorialsOverviewSend a requestCalculate years since milestoneCreate a count-based loopCreate a list-based loopAdvanced tutorialsOverviewExchange data between systemsCreate a dashboard in FlowsAutomate repetitive tasksRun requests in sequenceVideo tutorialsOverviewCreate a Postman FlowCreate with AI in flowsFlows scenarios and test dataFlows snapshots and modulesFlows actionsPostman AI developer toolsOverviewSend requests to AI modelsOverviewCreate AI requestsAdd MCP serversInteract with AI modelsManage AI request settingsSend requests to MCP serversOverviewCreate MCP requestsInteract with MCP serversManage MCP request settingsExport MCP server configsGenerate MCP serversOverviewCreate MCP serversStart MCP serversInteract with MCP servers and AI modelsPromote MCP serversCreate AI request blocksOverviewCreate your AI request blocksInteract with AI modelsCreate MCP servers with FlowsOverviewMCP servers in Postman FlowsCreate an MCP server flowPostman InsightsOverviewGet started with InsightsOverviewKubernetesOverviewDaemonsetSidecarAWS ECSAWS EC2AWS Elastic BeanstalkDebug with InsightsCustomize InsightsTroubleshoot InsightsOverviewMemory errorsTraffic errorsUninstall Insights AgentData handlingOverviewAccessRedactionsRepro mode securityDaemonSet securityReferenceOverviewInsights appOverviewOverview tabErrors tabLatency tabEndpoints tabRequests tabAlerts tabDiagnostics tabSettings tabGlobal insightsInsights agentOverviewapidumpkube injectkube helm-fragmentkube tf-fragmentecs addecs task-defecs cf-fragmentcompletionFAQPostman API NetworkOverviewExplore the Postman API NetworkOverviewFind public APIsOverviewSearch for public APIsBrowse public APIsConsume public APIsPublish to the Postman API NetworkOverviewPrepare your public APIsOverviewPrepare and verify your publisher teamPrepare your public workspacePrepare your public collectionsCurate your public collectionsOverviewCurate your public collection's overviewCurate your public collection's requestsCurate your public collection's developer experiencePrepare your team workspacePublish your public APIsOverviewPublish your public APIsAdd Run in Postman buttonOverviewCreate Run in Postman buttonCustomize Run in Postman buttonPublish interactive articlesOverviewCreate interactive articlesDraft interactive articlesOverviewFormat contentAdd input blocksAdd request blocksAdd code runner blocksPublish interactive articlesMaintain your public APIsExplore your publisher toolsCheck developer engagementDeveloper resourcesOverviewPostman API referencePostman API overviewPostman API authenticationPostman API access rate limitsMake a call with the Postman APIPostman MCP ServerOverviewSet up the serverTips and best practicesPostman Echo servicePostman Collection SDKPostman Runtime libraryPostman VS Code extensionOverviewInstall and sign inImport dataOverviewImport Postman dataImport environment variablesSend requestsCreate and manage collectionsCreate and manage environmentsShare your work and manage accessDocument APIsTest APIsOverviewWrite scriptsUse packages in scriptsRun collectionsFeedbackPostman code generatorAPI format conversionHome / IntroductionPostman documentation overviewWelcome to the Postman Docs! This is the place to find official information on how to use Postman in your API projects.\n",
      "If you're learning to carry out a specific task or workflow in Postman, check out the following topics to find resources:\n",
      "Get started\n",
      "To get started using Postman, check out the Get started section.\n",
      "Send requests\n",
      "You can send requests in Postman to connect to APIs you are working with. To learn more about how to send requests, see Send API requests and get response data in Postman and learn how to send your first request.\n",
      "Write scripts\n",
      "Postman has a powerful runtime based on Node.js that enables you to add dynamic behavior to requests and collections. You can write scripts that run before or after requests to perform API tests, build requests that can contain dynamic parameters, pass data between requests, and more. To learn more about scripts, see Use scripts to add logic and tests to Postman requests.\n",
      "Use collections\n",
      "Postman Collections are groups of saved requests. You can use collections to organize and group your requests. They can then be run together. You can run collections manually, on a schedule, from a CI/CD pipeline, or from a webhook. To learn more about collections, see Organize and automate API requests in Postman Collections.\n",
      "Use Postman Flows\n",
      "Postman Flows is a visual tool for creating API workflows. You can use flows to chain requests, handle data, and create real-world workflows in your Postman workspace.\n",
      "To learn more about Postman Flows, see Build API applications visually using Postman Flows.\n",
      "Use the Postman CLI\n",
      "The Postman CLI is a secure command-line companion for Postman. You can use the Postman CLI to run a collection, send run results to Postman, check API definitions against configured API Governance and API Security rules, and more.\n",
      "To learn more about the Postman CLI, see Explore Postman's command-line companion.\n",
      "Ask questions, share knowledge, and connect with developersAsk questions, share knowledge, and connect with developersJoin the Community\n",
      "Collaborate in Postman\n",
      "Postman provides a variety of tools to enable and enhance collaboration within your team. You can create internal workspaces, where team members can share their work and collaborate on API projects. Users can also discuss their work directly in Postman by commenting on collections and APIs, including on specific requests, versions, and inline on API definitions and scripts.\n",
      "To learn more about collaboration in Postman, see Collaborate in Postman.\n",
      "Design and develop your API\n",
      "Postman supports API-first development with Spec Hub, types in collections, and the API Builder. Use Spec Hub or the API Builder to design your API's structure in an API specification or definition, respectively. Either can act as the single source of truth for your API project. You can also use types in collections to design your API with the Postman Collection format.\n",
      "To learn more about API-first development, see Design and build your APIs in Postman.\n",
      "Document your API\n",
      "Documentation is an important part of any collection or API. Good documentation helps the people who use your collection understand what it does and how each request works. And comprehensive API documentation lets your consumers know what endpoints are available and how to interact with them.\n",
      "Once you've generated documentation for your collection or API, users can view the documentation in Postman. By default your documentation is private, so you must share a collection or API with others before they can access it. If you're creating a public API, you can publish your documentation to make it publicly available to anyone with a web browser.\n",
      "To learn more about documenting your API, see Document your APIs in Postman.\n",
      "Monitor your API\n",
      "Postman Monitors give you continuous visibility into the health and performance of your APIs. Monitors enable you to run API test scripts, chain together multiple requests, and validate critical API flows. A monitor runs a series of requests from the Postman cloud on a schedule you set. To learn more about using monitors, see Monitor health and performance of your APIs in Postman.\n",
      "API Governance and API Security\n",
      "API governance is the practice of defining and applying development rules that promote consistent API behaviors across your organization's API landscape. A robust API security posture means that your organization has development rules that promote security-first API behaviors. The Postman API Governance and Postman API Security features can identify inconsistencies or weaknesses in your APIs, then recommend possible fixes or improvements.\n",
      "To learn more about Postman's API Governance and API Security features, see API Governance and API Security in Postman.\n",
      "Administration\n",
      "Postman provides a variety of options to customize your team's experience, from the initial setup to ongoing team and plan management. As a Team Admin, you can streamline the onboarding process for your team, manage access control, and keep your team up to date with the latest version of Postman.\n",
      "To learn more about administering your Postman team, see the Team management overview.\n",
      "Use reports\n",
      "Postman generates reports that enable you to visualize data for how your team uses Postman. These reports give you insights into the state of your APIs, including tests, documentation, and monitoring coverage. They also enable you to track performance and SLA adherence.\n",
      "To learn more about how to use reports, see View reports on usage, security, and billing in Postman.\n",
      "Developer resources\n",
      "If you're integrating Postman with your CI/CD workflow or are developing with Postman APIs or libraries, check out Postman developer resources.\n",
      "Integrations\n",
      "You can connect Postman to your API workflows with integrations for popular third-party solutions. Use integrations to automatically share data between Postman and the other tools you rely on for API development. For more information, see Integrate Postman with third-party solutions.Last modified: 2025/05/09On this pageGet startedSend requestsWrite scriptsUse collectionsUse Postman FlowsUse the Postman CLICollaborate in PostmanDesign and develop your APIDocument your APIMonitor your APIAPI Governance and API SecurityAdministrationUse reportsDeveloper resourcesIntegrationsAdditional resourcesExplore ready-to-use Collection Templates, build API-first workflows with Postman Flows, and more!Collection TemplatesIntegration testingREST API basicsAPI documentationAuthorization methodsFlows TemplatesAdd HubSpot contacts to Mailchimp listsCreate Airtable records from new deals in HubSpotCreate or update HubSpot contacts from customers on StripeSearch Zendesk for tickets with Tag using Create with AIPopular videosPostman Flows: Build API Applications VisuallyStreamline LLM Integration with Postman's AI ProtocolAPI Basics: What is an API and How Does It Work?ProductWhat is Postman?EnterpriseSpec HubNewFlowsPostbotVS Code ExtensionPostman CLIIntegrationsToolsAPI GovernanceWorkspacesPlans and pricingAPI NetworkApp SecurityArtificial IntelligenceCommunicationData AnalyticsDatabaseDeveloper ProductivityDevOpsEcommerceeSignatureFinancial ServicesPaymentsTravelResourcesPostman DocsAcademyTemplatesIntergalacticVideosMCP ServersNewLegal and SecurityTerms of ServiceTrust and SafetyPrivacy policyCookie noticeCompanyAboutCareers and cultureContact usPartner programCustomer storiesStudent programsPress and mediaDownload PostmanYour privacy choices© 2025 Postman, Inc.\n",
      "\n",
      "\n",
      " curl - Tutorial\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Docs Overview\n",
      "\n",
      "Project\n",
      "\n",
      "Bug Bounty\n",
      "Bug Report\n",
      "Code of conduct\n",
      "Dependencies\n",
      "Donate\n",
      "FAQ\n",
      "Features\n",
      "Governance\n",
      "History\n",
      "Install\n",
      "Known Bugs\n",
      "Logo\n",
      "TODO\n",
      "website Info\n",
      "\n",
      "\n",
      "\n",
      "Protocols\n",
      "\n",
      "CA Extract\n",
      "HTTP cookies\n",
      "HTTP/3\n",
      "MQTT\n",
      "SSL certs\n",
      "SSL libs compared\n",
      "URL syntax\n",
      "WebSocket\n",
      "\n",
      "\n",
      "\n",
      "Releases\n",
      "\n",
      "Changelog\n",
      "curl CVEs\n",
      "Release Table\n",
      "Version Numbering\n",
      "Vulnerabilities\n",
      "\n",
      "\n",
      "\n",
      "Tool\n",
      "\n",
      "Comparison Table\n",
      "curl man page\n",
      "HTTP Scripting\n",
      "mk-ca-bundle\n",
      "Tutorial\n",
      "When options were added\n",
      "\n",
      "\n",
      "\n",
      "Who and Why\n",
      "\n",
      "Companies\n",
      "Copyright\n",
      "Sponsors\n",
      "Thanks\n",
      "The name\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "curl / Docs / Tool / Tutorial\n",
      "\n",
      "Related:\n",
      "FAQ\n",
      "man page\n",
      "\n",
      "\n",
      "curl tutorial\n",
      "Simple Usage\n",
      "Get the main page from a web-server:\n",
      "curl https://www.example.com/\n",
      "Get a README file from an FTP server:\n",
      "curl ftp://ftp.example.com/README\n",
      "Get a webpage from a server using port 8000:\n",
      "curl http://www.example.com:8000/\n",
      "Get a directory listing of an FTP site:\n",
      "curl ftp://ftp.example.com/\n",
      "Get the all terms matching curl from a dictionary:\n",
      "curl dict://dict.example.com/m:curl\n",
      "Get the definition of curl from a dictionary:\n",
      "curl dict://dict.example.com/d:curl\n",
      "Fetch two documents at once:\n",
      "curl ftp://ftp.example.com/ http://www.example.com:8000/\n",
      "Get a file off an FTPS server:\n",
      "curl ftps://files.are.example.com/secrets.txt\n",
      "or use the more appropriate FTPS way to get the same file:\n",
      "curl --ssl-reqd ftp://files.are.example.com/secrets.txt\n",
      "Get a file from an SSH server using SFTP:\n",
      "curl -u username sftp://example.com/etc/issue\n",
      "Get a file from an SSH server using SCP using a private key (not\n",
      "password-protected) to authenticate:\n",
      "curl -u username: --key ~/.ssh/id_rsa scp://example.com/~/file.txt\n",
      "Get a file from an SSH server using SCP using a private key\n",
      "(password-protected) to authenticate:\n",
      "curl -u username: --key ~/.ssh/id_rsa --pass private_key_password\n",
      "scp://example.com/~/file.txt\n",
      "Get the main page from an IPv6 web server:\n",
      "curl \"http://[2001:1890:1112:1::20]/\"\n",
      "Get a file from an SMB server:\n",
      "curl -u \"domain\\username:passwd\" smb://server.example.com/share/file.txt\n",
      "Download to a File\n",
      "Get a webpage and store in a local file with a specific name:\n",
      "curl -o thatpage.html http://www.example.com/\n",
      "Get a webpage and store in a local file, make the local file get the\n",
      "name of the remote document (if no filename part is specified in the\n",
      "URL, this fails):\n",
      "curl -O http://www.example.com/index.html\n",
      "Fetch two files and store them with their remote names:\n",
      "curl -O www.haxx.se/index.html -O curl.se/download.html\n",
      "Using Passwords\n",
      "FTP\n",
      "To ftp files using name and password, include them in the URL\n",
      "like:\n",
      "curl ftp://name:passwd@ftp.server.example:port/full/path/to/file\n",
      "or specify them with the -u flag like\n",
      "curl -u name:passwd ftp://ftp.server.example:port/full/path/to/file\n",
      "FTPS\n",
      "It is just like for FTP, but you may also want to specify and use\n",
      "SSL-specific options for certificates etc.\n",
      "Note that using FTPS:// as prefix is the\n",
      "implicit way as described in the standards while the\n",
      "recommended explicit way is done by using FTP://\n",
      "and the --ssl-reqd option.\n",
      "SFTP / SCP\n",
      "This is similar to FTP, but you can use the --key option\n",
      "to specify a private key to use instead of a password. Note that the\n",
      "private key may itself be protected by a password that is unrelated to\n",
      "the login password of the remote system; this password is specified\n",
      "using the --pass option. Typically, curl automatically\n",
      "extracts the public key from the private key file, but in cases where\n",
      "curl does not have the proper library support, a matching public key\n",
      "file must be specified using the --pubkey option.\n",
      "HTTP\n",
      "curl also supports user and password in HTTP URLs, thus you can pick\n",
      "a file like:\n",
      "curl http://name:passwd@http.server.example/full/path/to/file\n",
      "or specify user and password separately like in\n",
      "curl -u name:passwd http://http.server.example/full/path/to/file\n",
      "HTTP offers many different methods of authentication and curl\n",
      "supports several: Basic, Digest, NTLM and Negotiate (SPNEGO). Without\n",
      "telling which method to use, curl defaults to Basic. You can also ask\n",
      "curl to pick the most secure ones out of the ones that the server\n",
      "accepts for the given URL, by using --anyauth.\n",
      "Note! According to the URL specification, HTTP URLs\n",
      "can not contain a user and password, so that style does not work when\n",
      "using curl via a proxy, even though curl allows it at other times. When\n",
      "using a proxy, you must use the -u style for user\n",
      "and password.\n",
      "HTTPS\n",
      "Probably most commonly used with private certificates, as explained\n",
      "below.\n",
      "Proxy\n",
      "curl supports both HTTP and SOCKS proxy servers, with optional\n",
      "authentication. It does not have special support for FTP proxy servers\n",
      "since there are no standards for those, but it can still be made to work\n",
      "with many of them. You can also use both HTTP and SOCKS proxies to\n",
      "transfer files to and from FTP servers.\n",
      "Get an ftp file using an HTTP proxy named my-proxy that uses port\n",
      "888:\n",
      "curl -x my-proxy:888 ftp://ftp.example.com/README\n",
      "Get a file from an HTTP server that requires user and password, using\n",
      "the same proxy as above:\n",
      "curl -u user:passwd -x my-proxy:888 http://www.example.com/\n",
      "Some proxies require special authentication. Specify by using -U as\n",
      "above:\n",
      "curl -U user:passwd -x my-proxy:888 http://www.example.com/\n",
      "A comma-separated list of hosts and domains which do not use the\n",
      "proxy can be specified as:\n",
      "curl --noproxy example.com -x my-proxy:888 http://www.example.com/\n",
      "If the proxy is specified with --proxy1.0 instead of\n",
      "--proxy or -x, then curl uses HTTP/1.0 instead\n",
      "of HTTP/1.1 for any CONNECT attempts.\n",
      "curl also supports SOCKS4 and SOCKS5 proxies with\n",
      "--socks4 and --socks5.\n",
      "See also the environment variables curl supports that offer further\n",
      "proxy control.\n",
      "Most FTP proxy servers are set up to appear as a normal FTP server\n",
      "from the client's perspective, with special commands to select the\n",
      "remote FTP server. curl supports the -u, -Q\n",
      "and --ftp-account options that can be used to set up\n",
      "transfers through many FTP proxies. For example, a file can be uploaded\n",
      "to a remote FTP server using a Blue Coat FTP proxy with the options:\n",
      "curl -u \"username@ftp.server.example Proxy-Username:Remote-Pass\"\n",
      "  --ftp-account Proxy-Password --upload-file local-file\n",
      "  ftp://my-ftp.proxy.example:21/remote/upload/path/\n",
      "See the manual for your FTP proxy to determine the form it expects to\n",
      "set up transfers, and curl's -v option to see exactly what\n",
      "curl is sending.\n",
      "Piping\n",
      "Get a key file and add it with apt-key (when on a system\n",
      "that uses apt for package management):\n",
      "curl -L https://apt.example.org/llvm-snapshot.gpg.key | sudo apt-key add -\n",
      "The '|' pipes the output to STDIN. - tells\n",
      "apt-key that the key file should be read from STDIN.\n",
      "Ranges\n",
      "HTTP 1.1 introduced byte-ranges. Using this, a client can request to\n",
      "get only one or more sub-parts of a specified document. curl supports\n",
      "this with the -r flag.\n",
      "Get the first 100 bytes of a document:\n",
      "curl -r 0-99 http://www.example.com/\n",
      "Get the last 500 bytes of a document:\n",
      "curl -r -500 http://www.example.com/\n",
      "curl also supports simple ranges for FTP files as well. Then you can\n",
      "only specify start and stop position.\n",
      "Get the first 100 bytes of a document using FTP:\n",
      "curl -r 0-99 ftp://www.example.com/README\n",
      "Uploading\n",
      "FTP / FTPS / SFTP / SCP\n",
      "Upload all data on stdin to a specified server:\n",
      "curl -T - ftp://ftp.example.com/myfile\n",
      "Upload data from a specified file, login with user and password:\n",
      "curl -T uploadfile -u user:passwd ftp://ftp.example.com/myfile\n",
      "Upload a local file to the remote site, and use the local filename at\n",
      "the remote site too:\n",
      "curl -T uploadfile -u user:passwd ftp://ftp.example.com/\n",
      "Upload a local file to get appended to the remote file:\n",
      "curl -T localfile -a ftp://ftp.example.com/remotefile\n",
      "curl also supports ftp upload through a proxy, but only if the proxy\n",
      "is configured to allow that kind of tunneling. If it does, you can run\n",
      "curl in a fashion similar to:\n",
      "curl --proxytunnel -x proxy:port -T localfile ftp.example.com\n",
      "SMB / SMBS\n",
      "curl -T file.txt -u \"domain\\username:passwd\"\n",
      "  smb://server.example.com/share/\n",
      "HTTP\n",
      "Upload all data on stdin to a specified HTTP site:\n",
      "curl -T - http://www.example.com/myfile\n",
      "Note that the HTTP server must have been configured to accept PUT\n",
      "before this can be done successfully.\n",
      "For other ways to do HTTP data upload, see the POST section\n",
      "below.\n",
      "Verbose / Debug\n",
      "If curl fails where it is not supposed to, if the servers do not let\n",
      "you in, if you cannot understand the responses: use the -v\n",
      "flag to get verbose fetching. curl outputs lots of info and what it\n",
      "sends and receives in order to let the user see all client-server\n",
      "interaction (but it does not show you the actual data).\n",
      "curl -v ftp://ftp.example.com/\n",
      "To get even more details and information on what curl does, try using\n",
      "the --trace or --trace-ascii options with a\n",
      "given filename to log to, like this:\n",
      "curl --trace my-trace.txt www.haxx.se\n",
      "Detailed Information\n",
      "Different protocols provide different ways of getting detailed\n",
      "information about specific files/documents. To get curl to show detailed\n",
      "information about a single file, you should use\n",
      "-I/--head option. It displays all available\n",
      "info on a single file for HTTP and FTP. The HTTP information is a lot\n",
      "more extensive.\n",
      "For HTTP, you can get the header information (the same as\n",
      "-I would show) shown before the data by using\n",
      "-i/--include. curl understands the\n",
      "-D/--dump-header option when getting files\n",
      "from both FTP and HTTP, and it then stores the headers in the specified\n",
      "file.\n",
      "Store the HTTP headers in a separate file (headers.txt in the\n",
      "example):\n",
      "  curl --dump-header headers.txt curl.se\n",
      "Note that headers stored in a separate file can be useful at a later\n",
      "time if you want curl to use cookies sent by the server. More about that\n",
      "in the cookies section.\n",
      "POST (HTTP)\n",
      "It is easy to post data using curl. This is done using the\n",
      "-d <data> option. The post data must be\n",
      "urlencoded.\n",
      "Post a simple name and phone guestbook.\n",
      "curl -d \"name=Rafael%20Sagula&phone=3320780\" http://www.example.com/guest.cgi\n",
      "Or automatically URL encode the\n",
      "data.\n",
      "curl --data-urlencode \"name=Rafael Sagula&phone=3320780\"\n",
      "  http://www.example.com/guest.cgi\n",
      "How to post a form with curl, lesson #1:\n",
      "Dig out all the <input> tags in the form that you\n",
      "want to fill in.\n",
      "If there is a normal post, you use -d to post.\n",
      "-d takes a full post string, which is in the format\n",
      "<variable1>=<data1>&<variable2>=<data2>&...\n",
      "The variable names are the names set with \"name=\" in the\n",
      "<input> tags, and the data is the contents you want\n",
      "to fill in for the inputs. The data must be properly URL\n",
      "encoded. That means you replace space with + and that you replace weird\n",
      "letters with %XX where XX is the hexadecimal\n",
      "representation of the letter's ASCII code.\n",
      "Example:\n",
      "(say if http://example.com had the following html)\n",
      "<form action=\"post.cgi\" method=\"post\">\n",
      "  <input name=user size=10>\n",
      "  <input name=pass type=password size=10>\n",
      "  <input name=id type=hidden value=\"blablabla\">\n",
      "  <input name=ding value=\"submit\">\n",
      "</form>\n",
      "We want to enter user foobar with password\n",
      "12345.\n",
      "To post to this, you would enter a curl command line like:\n",
      "curl -d \"user=foobar&pass=12345&id=blablabla&ding=submit\"\n",
      "  http://example.com/post.cgi\n",
      "While -d uses the application/x-www-form-urlencoded\n",
      "mime-type, generally understood by CGI's and similar, curl also supports\n",
      "the more capable multipart/form-data type. This latter type supports\n",
      "things like file upload.\n",
      "-F accepts parameters like\n",
      "-F \"name=contents\". If you want the contents to be read\n",
      "from a file, use @filename as contents. When specifying a\n",
      "file, you can also specify the file content type by appending\n",
      ";type=<mime type> to the filename. You can also post\n",
      "the contents of several files in one field. For example, the field name\n",
      "coolfiles is used to send three files, with different\n",
      "content types using the following syntax:\n",
      "curl -F \"coolfiles=@fil1.gif;type=image/gif,fil2.txt,fil3.html\"\n",
      "  http://www.example.com/postit.cgi\n",
      "If the content-type is not specified, curl tries to guess from the\n",
      "file extension (it only knows a few), or use the previously specified\n",
      "type (from an earlier file if several files are specified in a list) or\n",
      "else it uses the default type application/octet-stream.\n",
      "Emulate a fill-in form with -F. Let's say you fill in\n",
      "three fields in a form. One field is a filename which to post, one field\n",
      "is your name and one field is a file description. We want to post the\n",
      "file we have written named cooltext.txt. To let curl do the\n",
      "posting of this data instead of your favorite browser, you have to read\n",
      "the HTML source of the form page and find the names of the input fields.\n",
      "In our example, the input field names are file,\n",
      "yourname and filedescription.\n",
      "curl -F \"file=@cooltext.txt\" -F \"yourname=Daniel\"\n",
      "  -F \"filedescription=Cool text file with cool text inside\"\n",
      "  http://www.example.com/postit.cgi\n",
      "To send two files in one post you can do it in two ways:\n",
      "Send multiple files in a single field with a single field name:\n",
      "curl -F \"pictures=@dog.gif,cat.gif\" $URL\n",
      "Send two fields with two field names\n",
      "curl -F \"docpicture=@dog.gif\" -F \"catpicture=@cat.gif\" $URL\n",
      "To send a field value literally without interpreting a leading\n",
      "@ or <, or an embedded ;type=,\n",
      "use --form-string instead of -F. This is\n",
      "recommended when the value is obtained from a user or some other\n",
      "unpredictable source. Under these circumstances, using -F\n",
      "instead of --form-string could allow a user to trick curl\n",
      "into uploading a file.\n",
      "Referrer\n",
      "An HTTP request has the option to include information about which\n",
      "address referred it to the actual page. curl allows you to specify the\n",
      "referrer to be used on the command line. It is especially useful to fool\n",
      "or trick stupid servers or CGI scripts that rely on that information\n",
      "being available or contain certain data.\n",
      "curl -e www.example.org http://www.example.com/\n",
      "User Agent\n",
      "An HTTP request has the option to include information about the\n",
      "browser that generated the request. curl allows it to be specified on\n",
      "the command line. It is especially useful to fool or trick stupid\n",
      "servers or CGI scripts that only accept certain browsers.\n",
      "Example:\n",
      "curl -A 'Mozilla/3.0 (Win95; I)' http://www.bank.example.com/\n",
      "Other common strings:\n",
      "\n",
      "Mozilla/3.0 (Win95; I) - Netscape Version 3 for Windows\n",
      "95\n",
      "Mozilla/3.04 (Win95; U) - Netscape Version 3 for\n",
      "Windows 95\n",
      "Mozilla/2.02 (OS/2; U) - Netscape Version 2 for\n",
      "OS/2\n",
      "Mozilla/4.04 [en] (X11; U; AIX 4.2; Nav) - Netscape for\n",
      "AIX\n",
      "Mozilla/4.05 [en] (X11; U; Linux 2.0.32 i586) -\n",
      "Netscape for Linux\n",
      "\n",
      "Note that Internet Explorer tries hard to be compatible in every\n",
      "way:\n",
      "\n",
      "Mozilla/4.0 (compatible; MSIE 4.01; Windows 95) - MSIE\n",
      "for W95\n",
      "\n",
      "Mozilla is not the only possible User-Agent name:\n",
      "\n",
      "Konqueror/1.0 - KDE File Manager desktop client\n",
      "Lynx/2.7.1 libwww-FM/2.14 - Lynx command line\n",
      "browser\n",
      "\n",
      "Cookies\n",
      "Cookies are generally used by web servers to keep state information\n",
      "at the client's side. The server sets cookies by sending a response line\n",
      "in the headers that looks like Set-Cookie: <data>\n",
      "where the data part then typically contains a set of\n",
      "NAME=VALUE pairs (separated by semicolons ;\n",
      "like NAME1=VALUE1; NAME2=VALUE2;). The server can also\n",
      "specify for what path the cookie should be used for (by specifying\n",
      "path=value), when the cookie should expire\n",
      "(expire=DATE), for what domain to use it\n",
      "(domain=NAME) and if it should be used on secure\n",
      "connections only (secure).\n",
      "If you have received a page from a server that contains a header\n",
      "like:\n",
      "Set-Cookie: sessionid=boo123; path=\"/foo\";\n",
      "it means the server wants that first pair passed on when we get\n",
      "anything in a path beginning with /foo.\n",
      "Example, get a page that wants my name passed in a cookie:\n",
      "curl -b \"name=Daniel\" www.example.com\n",
      "curl also has the ability to use previously received cookies in\n",
      "following sessions. If you get cookies from a server and store them in a\n",
      "file in a manner similar to:\n",
      "curl --dump-header headers www.example.com\n",
      "... you can then in a second connect to that (or another) site, use\n",
      "the cookies from the headers.txt file like:\n",
      "curl -b headers.txt www.example.com\n",
      "While saving headers to a file is a working way to store cookies, it\n",
      "is however error-prone and not the preferred way to do this. Instead,\n",
      "make curl save the incoming cookies using the well-known Netscape cookie\n",
      "format like this:\n",
      "curl -c cookies.txt www.example.com\n",
      "Note that by specifying -b you enable the cookie engine\n",
      "and with -L you can make curl follow a\n",
      "location: (which often is used in combination with\n",
      "cookies). If a site sends cookies and a location field, you can use a\n",
      "non-existing file to trigger the cookie awareness like:\n",
      "curl -L -b empty.txt www.example.com\n",
      "The file to read cookies from must be formatted using plain HTTP\n",
      "headers OR as Netscape's cookie file. curl determines what kind it is\n",
      "based on the file contents. In the above command, curl parses the header\n",
      "and store the cookies received from www.example.com. curl sends the stored\n",
      "cookies which match the request to the server as it follows the\n",
      "location. The file empty.txt may be a nonexistent file.\n",
      "To read and write cookies from a Netscape cookie file, you can set\n",
      "both -b and -c to use the same file:\n",
      "curl -b cookies.txt -c cookies.txt www.example.com\n",
      "Progress Meter\n",
      "The progress meter exists to show a user that something actually is\n",
      "happening. The different fields in the output have the following\n",
      "meaning:\n",
      "% Total    % Received % Xferd  Average Speed          Time             Curr.\n",
      "                               Dload  Upload Total    Current  Left    Speed\n",
      "0  151M    0 38608    0     0   9406      0  4:41:43  0:00:04  4:41:39  9287\n",
      "From left-to-right:\n",
      "\n",
      "% - percentage completed of the whole transfer\n",
      "Total - total size of the whole expected transfer\n",
      "% - percentage completed of the download\n",
      "Received - currently downloaded amount of bytes\n",
      "% - percentage completed of the upload\n",
      "Xferd - currently uploaded amount of bytes\n",
      "Average Speed Dload - the average transfer speed of the\n",
      "download\n",
      "Average Speed Upload - the average transfer speed of\n",
      "the upload\n",
      "Time Total - expected time to complete the\n",
      "operation\n",
      "Time Current - time passed since the invoke\n",
      "Time Left - expected time left to completion\n",
      "Curr.Speed - the average transfer speed the last 5\n",
      "seconds (the first 5 seconds of a transfer is based on less time of\n",
      "course.)\n",
      "\n",
      "The -# option displays a totally different progress bar\n",
      "that does not need much explanation!\n",
      "Speed Limit\n",
      "curl allows the user to set the transfer speed conditions that must\n",
      "be met to let the transfer keep going. By using the switch\n",
      "-y and -Y you can make curl abort transfers if\n",
      "the transfer speed is below the specified lowest limit for a specified\n",
      "time.\n",
      "To have curl abort the download if the speed is slower than 3000\n",
      "bytes per second for 1 minute, run:\n",
      "curl -Y 3000 -y 60 www.far-away.example.com\n",
      "This can be used in combination with the overall time limit, so that\n",
      "the above operation must be completed in whole within 30 minutes:\n",
      "curl -m 1800 -Y 3000 -y 60 www.far-away.example.com\n",
      "Forcing curl not to transfer data faster than a given rate is also\n",
      "possible, which might be useful if you are using a limited bandwidth\n",
      "connection and you do not want your transfer to use all of it (sometimes\n",
      "referred to as bandwidth throttle).\n",
      "Make curl transfer data no faster than 10 kilobytes per second:\n",
      "curl --limit-rate 10K www.far-away.example.com\n",
      "or\n",
      "curl --limit-rate 10240 www.far-away.example.com\n",
      "Or prevent curl from uploading data faster than 1 megabyte per\n",
      "second:\n",
      "curl -T upload --limit-rate 1M ftp://uploads.example.com\n",
      "When using the --limit-rate option, the transfer rate is\n",
      "regulated on a per-second basis, which causes the total transfer speed\n",
      "to become lower than the given number. Sometimes of course substantially\n",
      "lower, if your transfer stalls during periods.\n",
      "Config File\n",
      "curl automatically tries to read the .curlrc file (or\n",
      "_curlrc file on Microsoft Windows systems) from the user's\n",
      "home directory on startup.\n",
      "The config file could be made up with normal command line switches,\n",
      "but you can also specify the long options without the dashes to make it\n",
      "more readable. You can separate the options and the parameter with\n",
      "spaces, or with = or :. Comments can be used\n",
      "within the file. If the first letter on a line is a\n",
      "#-symbol the rest of the line is treated as a comment.\n",
      "If you want the parameter to contain spaces, you must enclose the\n",
      "entire parameter within double quotes (\"). Within those\n",
      "quotes, you specify a quote as \\\".\n",
      "NOTE: You must specify options and their arguments on the same\n",
      "line.\n",
      "Example, set default time out and proxy in a config file:\n",
      "# We want a 30 minute timeout:\n",
      "-m 1800\n",
      "#. .. and we use a proxy for all accesses:\n",
      "proxy = proxy.our.domain.example.com:8080\n",
      "Whitespaces ARE significant at the end of lines, but all whitespace\n",
      "leading up to the first characters of each line are ignored.\n",
      "Prevent curl from reading the default file by using -q as the first\n",
      "command line parameter, like:\n",
      "curl -q www.example.org\n",
      "Force curl to get and display a local help page in case it is invoked\n",
      "without URL by making a config file similar to:\n",
      "# default url to get\n",
      "url = \"http://help.with.curl.example.com/curlhelp.html\"\n",
      "You can specify another config file to be read by using the\n",
      "-K/--config flag. If you set config filename\n",
      "to - it reads the config from stdin, which can be handy if\n",
      "you want to hide options from being visible in process tables etc:\n",
      "echo \"user = user:passwd\" | curl -K - http://that.secret.example.com\n",
      "Extra Headers\n",
      "When using curl in your own programs, you may end up needing to pass\n",
      "on your own custom headers when getting a webpage. You can do this by\n",
      "using the -H flag.\n",
      "Example, send the header X-you-and-me: yes to the server\n",
      "when getting a page:\n",
      "curl -H \"X-you-and-me: yes\" love.example.com\n",
      "This can also be useful in case you want curl to send a different\n",
      "text in a header than it normally does. The -H header you\n",
      "specify then replaces the header curl would normally send. If you\n",
      "replace an internal header with an empty one, you prevent that header\n",
      "from being sent. To prevent the Host: header from being\n",
      "used:\n",
      "curl -H \"Host:\" server.example.com\n",
      "FTP and Path Names\n",
      "Do note that when getting files with a ftp:// URL, the\n",
      "given path is relative to the directory you enter. To get the file\n",
      "README from your home directory at your ftp site, do:\n",
      "curl ftp://user:passwd@my.example.com/README\n",
      "If you want the README file from the root directory of that same\n",
      "site, you need to specify the absolute filename:\n",
      "curl ftp://user:passwd@my.example.com//README\n",
      "(I.e with an extra slash in front of the filename.)\n",
      "SFTP and SCP and Path Names\n",
      "With sftp: and scp: URLs, the path name given is the absolute name on\n",
      "the server. To access a file relative to the remote user's home\n",
      "directory, prefix the file with /~/ , such as:\n",
      "curl -u $USER sftp://home.example.com/~/.bashrc\n",
      "FTP and Firewalls\n",
      "The FTP protocol requires one of the involved parties to open a\n",
      "second connection as soon as data is about to get transferred. There are\n",
      "two ways to do this.\n",
      "The default way for curl is to issue the PASV command which causes\n",
      "the server to open another port and await another connection performed\n",
      "by the client. This is good if the client is behind a firewall that does\n",
      "not allow incoming connections.\n",
      "curl ftp.example.com\n",
      "If the server, for example, is behind a firewall that does not allow\n",
      "connections on ports other than 21 (or if it just does not support the\n",
      "PASV command), the other way to do it is to use the\n",
      "PORT command and instruct the server to connect to the\n",
      "client on the given IP number and port (as parameters to the PORT\n",
      "command).\n",
      "The -P flag to curl supports a few different options.\n",
      "Your machine may have several IP-addresses and/or network interfaces and\n",
      "curl allows you to select which of them to use. Default address can also\n",
      "be used:\n",
      "curl -P - ftp.example.com\n",
      "Download with PORT but use the IP address of our\n",
      "le0 interface (this does not work on Windows):\n",
      "curl -P le0 ftp.example.com\n",
      "Download with PORT but use 192.168.0.10 as our IP\n",
      "address to use:\n",
      "curl -P 192.168.0.10 ftp.example.com\n",
      "Network Interface\n",
      "Get a webpage from a server using a specified port for the\n",
      "interface:\n",
      "curl --interface eth0:1 http://www.example.com/\n",
      "or\n",
      "curl --interface 192.168.1.10 http://www.example.com/\n",
      "HTTPS\n",
      "Secure HTTP requires a TLS library to be installed and used when curl\n",
      "is built. If that is done, curl is capable of retrieving and posting\n",
      "documents using the HTTPS protocol.\n",
      "Example:\n",
      "curl https://secure.example.com\n",
      "curl is also capable of using client certificates to get/post files\n",
      "from sites that require valid certificates. The only drawback is that\n",
      "the certificate needs to be in PEM-format. PEM is a standard and open\n",
      "format to store certificates with, but it is not used by the most\n",
      "commonly used browsers. If you want curl to use the certificates you use\n",
      "with your favorite browser, you may need to download/compile a converter\n",
      "that can convert your browser's formatted certificates to PEM formatted\n",
      "ones.\n",
      "Example on how to automatically retrieve a document using a\n",
      "certificate with a personal password:\n",
      "curl -E /path/to/cert.pem:password https://secure.example.com/\n",
      "If you neglect to specify the password on the command line, you are\n",
      "prompted for the correct password before any data can be received.\n",
      "Many older HTTPS servers have problems with specific SSL or TLS\n",
      "versions, which newer versions of OpenSSL etc use, therefore it is\n",
      "sometimes useful to specify what TLS version curl should use.:\n",
      "curl --tlv1.0 https://secure.example.com/\n",
      "Otherwise, curl attempts to use a sensible TLS default version.\n",
      "Resuming File Transfers\n",
      "To continue a file transfer where it was previously aborted, curl\n",
      "supports resume on HTTP(S) downloads as well as FTP uploads and\n",
      "downloads.\n",
      "Continue downloading a document:\n",
      "curl -C - -o file ftp://ftp.example.com/path/file\n",
      "Continue uploading a document:\n",
      "curl -C - -T file ftp://ftp.example.com/path/file\n",
      "Continue downloading a document from a web server\n",
      "curl -C - -o file http://www.example.com/\n",
      "Time Conditions\n",
      "HTTP allows a client to specify a time condition for the document it\n",
      "requests. It is If-Modified-Since or\n",
      "If-Unmodified-Since. curl allows you to specify them with\n",
      "the -z/--time-cond flag.\n",
      "For example, you can easily make a download that only gets performed\n",
      "if the remote file is newer than a local copy. It would be made\n",
      "like:\n",
      "curl -z local.html http://remote.example.com/remote.html\n",
      "Or you can download a file only if the local file is newer than the\n",
      "remote one. Do this by prepending the date string with a -,\n",
      "as in:\n",
      "curl -z -local.html http://remote.example.com/remote.html\n",
      "You can specify a plain text date as condition. Tell curl to only\n",
      "download the file if it was updated since January 12, 2012:\n",
      "curl -z \"Jan 12 2012\" http://remote.example.com/remote.html\n",
      "curl accepts a wide range of date formats. You always make the date\n",
      "check the other way around by prepending it with a dash\n",
      "(-).\n",
      "DICT\n",
      "For fun try\n",
      "curl dict://dict.org/m:curl\n",
      "curl dict://dict.org/d:heisenbug:jargon\n",
      "curl dict://dict.org/d:daniel:gcide\n",
      "Aliases for m are match and\n",
      "find, and aliases for d are\n",
      "define and lookup. For example,\n",
      "curl dict://dict.org/find:curl\n",
      "Commands that break the URL description of the RFC (but not the DICT\n",
      "protocol) are\n",
      "curl dict://dict.org/show:db\n",
      "curl dict://dict.org/show:strat\n",
      "Authentication support is still missing\n",
      "LDAP\n",
      "If you have installed the OpenLDAP library, curl can take advantage\n",
      "of it and offer ldap:// support. On Windows, curl uses\n",
      "WinLDAP from Platform SDK by default.\n",
      "Default protocol version used by curl is LDAP version 3. Version 2 is\n",
      "used as a fallback mechanism in case version 3 fails to connect.\n",
      "LDAP is a complex thing and writing an LDAP query is not an easy\n",
      "task. Familiarize yourself with the exact syntax description elsewhere.\n",
      "One such place might be: RFC\n",
      "2255, The LDAP URL Format\n",
      "To show you an example, this is how to get all people from an LDAP\n",
      "server that has a certain subdomain in their email address:\n",
      "curl -B \"ldap://ldap.example.com/o=frontec??sub?mail=*sth.example.com\"\n",
      "You also can use authentication when accessing LDAP catalog:\n",
      "curl -u user:passwd \"ldap://ldap.example.com/o=frontec??sub?mail=*\"\n",
      "curl \"ldap://user:passwd@ldap.example.com/o=frontec??sub?mail=*\"\n",
      "By default, if user and password are provided, OpenLDAP/WinLDAP uses\n",
      "basic authentication. On Windows you can control this behavior by\n",
      "providing one of --basic, --ntlm or\n",
      "--digest option in curl command line\n",
      "curl --ntlm \"ldap://user:passwd@ldap.example.com/o=frontec??sub?mail=*\"\n",
      "On Windows, if no user/password specified, auto-negotiation mechanism\n",
      "is used with current logon credentials (SSPI/SPNEGO).\n",
      "Environment Variables\n",
      "curl reads and understands the following proxy related environment\n",
      "variables:\n",
      "http_proxy, HTTPS_PROXY, FTP_PROXY\n",
      "They should be set for protocol-specific proxies. General proxy\n",
      "should be set with\n",
      "ALL_PROXY\n",
      "A comma-separated list of hostnames that should not go through any\n",
      "proxy is set in (only an asterisk, * matches all hosts)\n",
      "NO_PROXY\n",
      "If the hostname matches one of these strings, or the host is within\n",
      "the domain of one of these strings, transactions with that node is not\n",
      "done over the proxy. When a domain is used, it needs to start with a\n",
      "period. A user can specify that both www.example.com and foo.example.com\n",
      "should not use a proxy by setting NO_PROXY to\n",
      ".example.com. By including the full name you can exclude\n",
      "specific hostnames, so to make www.example.com not use a\n",
      "proxy but still have foo.example.com do it, set\n",
      "NO_PROXY to www.example.com.\n",
      "The usage of the -x/--proxy flag overrides\n",
      "the environment variables.\n",
      "Netrc\n",
      "Unix introduced the .netrc concept a long time ago. It\n",
      "is a way for a user to specify name and password for commonly visited\n",
      "FTP sites in a file so that you do not have to type them in each time\n",
      "you visit those sites. You realize this is a big security risk if\n",
      "someone else gets hold of your passwords, therefore most Unix programs\n",
      "do not read this file unless it is only readable by yourself (curl does\n",
      "not care though).\n",
      "curl supports .netrc files if told to (using the\n",
      "-n/--netrc and --netrc-optional\n",
      "options). This is not restricted to just FTP, so curl can use it for all\n",
      "protocols where authentication is used.\n",
      "A simple .netrc file could look something like:\n",
      "machine curl.se login iamdaniel password mysecret\n",
      "Custom Output\n",
      "To better allow script programmers to get to know about the progress\n",
      "of curl, the -w/--write-out option was\n",
      "introduced. Using this, you can specify what information from the\n",
      "previous transfer you want to extract.\n",
      "To display the amount of bytes downloaded together with some text and\n",
      "an ending newline:\n",
      "curl -w 'We downloaded %{size_download} bytes\\n' www.example.com\n",
      "Kerberos FTP Transfer\n",
      "curl supports kerberos4 and kerberos5/GSSAPI for FTP transfers. You\n",
      "need the kerberos package installed and used at curl build time for it\n",
      "to be available.\n",
      "First, get the krb-ticket the normal way, like with the\n",
      "kinit/kauth tool. Then use curl in way similar\n",
      "to:\n",
      "curl --krb private ftp://krb4site.example.com -u username:fakepwd\n",
      "There is no use for a password on the -u switch, but a\n",
      "blank one makes curl ask for one and you already entered the real\n",
      "password to kinit/kauth.\n",
      "TELNET\n",
      "The curl telnet support is basic and easy to use. curl passes all\n",
      "data passed to it on stdin to the remote server. Connect to a remote\n",
      "telnet server using a command line similar to:\n",
      "curl telnet://remote.example.com\n",
      "Enter the data to pass to the server on stdin. The result is sent to\n",
      "stdout or to the file you specify with -o.\n",
      "You might want the -N/--no-buffer option to\n",
      "switch off the buffered output for slow connections or similar.\n",
      "Pass options to the telnet protocol negotiation, by using the\n",
      "-t option. To tell the server we use a vt100 terminal, try\n",
      "something like:\n",
      "curl -tTTYPE=vt100 telnet://remote.example.com\n",
      "Other interesting options for it -t include:\n",
      "\n",
      "XDISPLOC=<X display> Sets the X display\n",
      "location.\n",
      "NEW_ENV=<var,val> Sets an environment\n",
      "variable.\n",
      "\n",
      "NOTE: The telnet protocol does not specify any way to login with a\n",
      "specified user and password so curl cannot do that automatically. To do\n",
      "that, you need to track when the login prompt is received and send the\n",
      "username and password accordingly.\n",
      "Persistent Connections\n",
      "Specifying multiple files on a single command line makes curl\n",
      "transfer all of them, one after the other in the specified order.\n",
      "libcurl attempts to use persistent connections for the transfers so\n",
      "that the second transfer to the same host can use the same connection\n",
      "that was already initiated and was left open in the previous transfer.\n",
      "This greatly decreases connection time for all but the first transfer\n",
      "and it makes a far better use of the network.\n",
      "Note that curl cannot use persistent connections for transfers that\n",
      "are used in subsequent curl invokes. Try to stuff as many URLs as\n",
      "possible on the same command line if they are using the same host, as\n",
      "that makes the transfers faster. If you use an HTTP proxy for file\n",
      "transfers, practically all transfers are persistent.\n",
      "Multiple\n",
      "Transfers With A Single Command Line\n",
      "As is mentioned above, you can download multiple files with one\n",
      "command line by simply adding more URLs. If you want those to get saved\n",
      "to a local file instead of just printed to stdout, you need to add one\n",
      "save option for each URL you specify. Note that this also goes for the\n",
      "-O option (but not --remote-name-all).\n",
      "For example: get two files and use -O for the first and\n",
      "a custom file name for the second:\n",
      "curl -O http://example.com/file.txt ftp://example.com/moo.exe -o moo.jpg\n",
      "You can also upload multiple files in a similar fashion:\n",
      "curl -T local1 ftp://example.com/moo.exe -T local2 ftp://example.com/moo2.txt\n",
      "IPv6\n",
      "curl connects to a server with IPv6 when a host lookup returns an\n",
      "IPv6 address and fall back to IPv4 if the connection fails. The\n",
      "--ipv4 and --ipv6 options can specify which\n",
      "address to use when both are available. IPv6 addresses can also be\n",
      "specified directly in URLs using the syntax:\n",
      "http://[2001:1890:1112:1::20]/overview.html\n",
      "When this style is used, the -g option must be given to\n",
      "stop curl from interpreting the square brackets as special globbing\n",
      "characters. Link local and site local addresses including a scope\n",
      "identifier, such as fe80::1234%1, may also be used, but the\n",
      "scope portion must be numeric or match an existing network interface on\n",
      "Linux and the percent character must be URL escaped. The previous\n",
      "example in an SFTP URL might look like:\n",
      "sftp://[fe80::1234%251]/\n",
      "IPv6 addresses provided other than in URLs (e.g. to the\n",
      "--proxy, --interface or\n",
      "--ftp-port options) should not be URL encoded.\n",
      "Mailing Lists\n",
      "For your convenience, we have several open mailing lists to discuss\n",
      "curl, its development and things relevant to this. Get all info at https://curl.se/mail/.\n",
      "Please direct curl questions, feature requests and trouble reports to\n",
      "one of these mailing lists instead of mailing any individual.\n",
      "Available lists include:\n",
      "curl-users\n",
      "Users of the command line tool. How to use it, what does not work,\n",
      "new features, related tools, questions, news, installations,\n",
      "compilations, running, porting etc.\n",
      "curl-library\n",
      "Developers using or developing libcurl. Bugs, extensions,\n",
      "improvements.\n",
      "curl-announce\n",
      "Low-traffic. Only receives announcements of new public versions. At\n",
      "worst, that makes something like one or two mails per month, but usually\n",
      "only one mail every second month.\n",
      "curl-and-php\n",
      "Using the curl functions in PHP. Everything curl with a PHP angle. Or\n",
      "PHP with a curl angle.\n",
      "curl-and-python\n",
      "Python hackers using curl with or without the python binding\n",
      "pycurl.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Everything curl - everything curl\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. Everything curl2. How to read3. The cURL project❱3.1. How it started3.2. The name3.3. What does curl do?3.4. Project communication3.5. Mailing list etiquette3.6. Mailing lists3.7. Reporting bugs3.8. Commercial support3.9. Releases3.10. Security3.11. Trust3.12. Code of Conduct3.13. Development3.14. The development team3.15. Users of curl3.16. Future4. Network and protocols❱4.1. Networking simplified4.2. Protocols4.3. curl protocols4.4. HTTP basics5. Install curl and libcurl❱5.1. Linux5.2. Windows❱5.2.1. MSYS25.2.2. vcpkg5.3. macOS5.4. Container6. Source code❱6.1. Open Source❱6.1.1. License6.1.2. Copyright6.2. Code layout6.3. Handling build options6.4. Code style6.5. Contributing6.6. Reporting vulnerabilities6.7. Website7. Build curl and libcurl❱7.1. Autotools7.2. CMake7.3. Separate install7.4. Windows7.5. Dependencies7.6. TLS libraries❱7.6.1. BoringSSL8. Command line concepts❱8.1. Differences8.2. Command line options❱8.2.1. Short options8.2.2. Long options8.2.3. Arguments to options8.2.4. Negative options8.3. Help8.4. Options depend on version8.5. URLs❱8.5.1. Scheme8.5.2. Name and password8.5.3. Host8.5.4. Port number8.5.5. Path8.5.6. Query8.5.7. FTP type8.5.8. Fragment8.5.9. Browsers8.5.10. Many options and URLs8.5.11. URL globbing8.5.12. Connection reuse8.5.13. Parallel transfers8.5.14. trurl8.6. Config file8.7. Variables8.8. Passwords8.9. Progress meter8.10. Version8.11. Exit code8.12. Copy as curl9. Command line transfers❱9.1. Verbose❱9.1.1. Trace options9.1.2. Write out9.2. Downloads❱9.2.1. What is downloading?9.2.2. Storing downloads9.2.3. Download to a file named by the URL9.2.4. Use the target filename from the server9.2.5. HTML and charsets9.2.6. Shell redirects9.2.7. Multiple downloads9.2.8. My browser shows something else9.2.9. Maximum filesize9.2.10. Storing metadata in file system9.2.11. Raw9.2.12. Retry9.2.13. Resuming and ranges9.3. Uploads9.4. Transfer controls❱9.4.1. Stop slow transfers9.4.2. Rate limiting9.4.3. Request rate limiting9.4.4. Compression9.4.5. Skip download if already done9.5. Connections❱9.5.1. Setup9.5.2. Name resolve tricks9.5.3. Connection timeout9.5.4. Happy Eyeballs9.5.5. Network interface9.5.6. Local port number9.5.7. Keep alive9.6. Timeouts9.7. .netrc9.8. Proxies❱9.8.1. Discover your proxy9.8.2. PAC9.8.3. Captive portals9.8.4. Proxy type9.8.5. HTTP proxy9.8.6. SOCKS proxy9.8.7. MITM proxy9.8.8. Proxy authentication9.8.9. HTTPS proxy9.8.10. Proxy environment variables9.8.11. Proxy headers9.8.12. haproxy9.9. TLS❱9.9.1. Ciphers9.9.2. Enable TLS9.9.3. TLS versions9.9.4. Verifying server certificates9.9.5. Certificate pinning9.9.6. OCSP stapling9.9.7. Client certificates9.9.8. TLS auth9.9.9. TLS backends9.9.10. SSLKEYLOGFILE9.10. SCP and SFTP❱9.10.1. URLs9.10.2. Authentication9.10.3. Known hosts9.11. Reading email9.12. Sending email9.13. DICT9.14. IPFS9.15. MQTT9.16. TELNET9.17. TFTP10. Command line HTTP❱10.1. Method10.2. Responses10.3. Authentication10.4. HTTP versions❱10.4.1. HTTP/0.910.4.2. HTTP/210.4.3. HTTP/310.5. HTTP POST❱10.5.1. Simple POST10.5.2. Content-Type10.5.3. Posting binary10.5.4. JSON10.5.5. URL encode data10.5.6. Convert to GET10.5.7. Expect 100-continue10.5.8. Chunked encoded POSTs10.5.9. Hidden form fields10.5.10. Figure out what a browser sends10.5.11. JavaScript and forms10.5.12. Multipart formposts10.5.13. -d vs -F10.6. HTTP PUT10.7. Redirects10.8. Customize your HTTP❱10.8.1. Request method10.8.2. Request target10.8.3. Fragment10.8.4. Customize headers10.8.5. Referer10.8.6. User-agent10.8.7. Ranges10.8.8. Conditionals10.8.9. Compression10.9. Cookies❱10.9.1. Cookie engine10.9.2. Reading cookies from file10.9.3. Writing cookies to file10.9.4. New cookie session10.9.5. Cookie file format10.10. HTTPS❱10.10.1. HSTS10.10.2. Alternative Services10.11. Scripting browser-like tasks11. Command line FTP❱11.1. FTP Directory listing11.2. Uploading with FTP11.3. Custom FTP commands11.4. Two connections11.5. Directory traversing11.6. FTPS12. libcurl❱12.1. Header files12.2. Global initialization12.3. API compatibility12.4. --libcurl12.5. multi-threading12.6. CURLcode return codes12.7. Verbose operations12.8. Caches12.9. Performance12.10. for C++ programmers13. libcurl transfers❱13.1. Easy handle13.2. curl easy options❱13.2.1. Set numerical options13.2.2. Set string options13.2.3. TLS options13.2.4. All options13.2.5. Get option information13.3. Drive transfers❱13.3.1. Drive with easy13.3.2. Drive with multi13.3.3. Drive with multi_socket13.4. Callbacks❱13.4.1. Write data13.4.2. Read data13.4.3. Progress information13.4.4. Header data13.4.5. Debug13.4.6. sockopt13.4.7. SSL context13.4.8. Seek and ioctl13.4.9. Network data conversion13.4.10. Opensocket and closesocket13.4.11. SSH key13.4.12. RTSP interleaved data13.4.13. FTP wildcard matching13.4.14. Resolver start13.4.15. Sending trailers13.4.16. HSTS13.4.17. Prereq13.5. Connection control❱13.5.1. How libcurl connects13.5.2. Local address and port number13.5.3. Connection reuse13.5.4. Keep alive13.5.5. Name resolving13.5.6. Proxies13.6. Transfer control❱13.6.1. Stop13.6.2. Stop slow transfers13.6.3. Rate limit13.6.4. Progress meter13.6.5. Progress callback13.7. Authentication13.8. Cleanup13.9. Post transfer info14. libcurl HTTP❱14.1. Responses14.2. Requests14.3. Versions14.4. Ranges14.5. Authentication14.6. Cookies14.7. Download14.8. Upload14.9. Multiplexing14.10. HSTS14.11. alt-svc15. libcurl helpers❱15.1. Share data between handles15.2. URL API❱15.2.1. Include files15.2.2. Create, cleanup, duplicate15.2.3. Parse a URL15.2.4. Redirect to URL15.2.5. Get a URL15.2.6. Get URL parts15.2.7. Set URL parts15.2.8. Append to the query15.2.9. CURLOPT_CURLU15.3. WebSocket❱15.3.1. Support15.3.2. URLs15.3.3. Concept15.3.4. Options15.3.5. Read15.3.6. Meta15.3.7. Write15.4. Headers API❱15.4.1. Header struct15.4.2. Get a header15.4.3. Iterate over headers16. libcurl examples❱16.1. Get a simple HTTP page16.2. Get a response into memory16.3. Submit a login form over HTTP16.4. Get an FTP directory listing16.5. Non-blocking HTTP form-post17. libcurl bindings18. libcurl internals❱18.1. Easy handles and connections18.2. Everything is multi18.3. State machines18.4. Protocol handler18.5. Backends18.6. Caches and state18.7. Timeouts18.8. Windows vs Unix18.9. Memory debugging18.10. Content Encoding18.11. Structs18.12. Resolving hostnames18.13. Tests❱18.13.1. Test file format18.13.2. Build tests18.13.3. Run tests18.13.4. Debug builds18.13.5. Test servers18.13.6. curl tests18.13.7. libcurl tests18.13.8. Unit tests18.13.9. Valgrind18.13.10. Continuous Integration18.13.11. Autobuilds18.13.12. Torture19. Index\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Light\n",
      "Rust\n",
      "Coal\n",
      "Navy\n",
      "Ayu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "everything curl\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Everything curl\n",
      "Everything curl is an extensive guide for all things curl. The project\n",
      "itself, the command-line tool, the library, how everything started and how it\n",
      "came to be the useful tool it is today. It explains how we work on developing\n",
      "it further, what it takes to use it, how you can contribute with code or bug\n",
      "reports and why millions of existing users use it.\n",
      "This book is meant to be interesting and useful to both casual readers and\n",
      "somewhat more experienced developers. It offers something for everyone to pick\n",
      "and choose from.\n",
      "Do not read this book from front to back. Read the chapters or content you are\n",
      "curious about and flip back and forth as you see fit.\n",
      "This book is an open source project in itself: open, completely free to\n",
      "download and read. Free for anyone to comment on, and available for everyone\n",
      "to contribute to and help out with. Send your bug reports, ideas, pull\n",
      "requests or critiques to us and I or someone else will work on improving the\n",
      "book accordingly.\n",
      "This book will never be finished. I intend to keep working on it. While I may\n",
      "at some point consider it fairly complete, covering most aspects of the\n",
      "project (even if only that seems like an insurmountable goal), the curl\n",
      "project will continue to move so there will always be things to update in the\n",
      "book as well.\n",
      "This book project started at the end of September 2015.\n",
      "Site\n",
      "https://everything.curl.dev is the home of this\n",
      "book. It features the book online in a web version.\n",
      "This book is also provided as a PDF and an ePUB.\n",
      "The book website is hosted by Fastly. The book contents is rendered by\n",
      "mdBook since March 18th, 2024.\n",
      "Content\n",
      "All book content is hosted on GitHub in the\n",
      "https://github.com/curl/everything-curl\n",
      "repository.\n",
      "Author\n",
      "With the hope of becoming just a co-author of this material, I am Daniel\n",
      "Stenberg. I founded the curl project and I am a developer at heart—for fun and\n",
      "profit. I live and work in Stockholm, Sweden.\n",
      "All there is to know about Daniel can be found on daniel.haxx.se.\n",
      "Contribute\n",
      "If you find mistakes, omissions, errors or blatant lies in this document,\n",
      "please send us a refreshed version of the affected paragraph and we will amend\n",
      "and update. We give credit to and recognize everyone who helps out.\n",
      "Preferably, you could submit\n",
      "errors or pull\n",
      "requests on the book's\n",
      "GitHub page.\n",
      "Contributors\n",
      "Lots of people have reported bugs, improved sections or otherwise helped\n",
      "make this book the success it is. These friends include the following:\n",
      "AaronChen0 on github,\n",
      "alawvt on github,\n",
      "Amin Khoshnood,\n",
      "amnkh on github,\n",
      "Anders Roxell,\n",
      "Angad Gill,\n",
      "Aris (Karim) Merchant,\n",
      "auktis on github,\n",
      "Ben Bodenmiller\n",
      "Ben Peachey,\n",
      "bookofportals on github,\n",
      "Bruno Baguette,\n",
      "Carlton Gibson,\n",
      "Chris DeLuca,\n",
      "Citizen Esosa,\n",
      "Dan Fandrich,\n",
      "Daniel Brown,\n",
      "Daniel Sabsay,\n",
      "David Piano,\n",
      "DrDoom74 at GitHub,\n",
      "Emil Hessman,\n",
      "enachos71 on github,\n",
      "ethomag on github,\n",
      "Fabian Keil,\n",
      "faterer on github,\n",
      "Frank Dana,\n",
      "Frank Hassanabad,\n",
      "Gautham B A,\n",
      "Geir Hauge,\n",
      "Harry Wright,\n",
      "Helena Udd,\n",
      "Hubert Lin,\n",
      "i-ky on github,\n",
      "infinnovation-dev on GitHub,\n",
      "Jay Ottinger,\n",
      "Jay Satiro,\n",
      "Jeroen Ooms,\n",
      "Johan Wigert,\n",
      "John Simpson,\n",
      "JohnCoconut on github,\n",
      "Jonas Forsberg,\n",
      "Josh Vanderhook,\n",
      "JoyIfBam5,\n",
      "KJM on github,\n",
      "knorr3 on github,\n",
      "lowttl on github,\n",
      "Luca Niccoli,\n",
      "Manuel on github,\n",
      "Marius Žilėnas,\n",
      "Mark Koester,\n",
      "Martin van den Nieuwelaar,\n",
      "mehandes on github,\n",
      "Michael Kaufmann,\n",
      "Ms2ger,\n",
      "Mohammadreza Hendiani,\n",
      "Nick Travers,\n",
      "Nicolas Brassard,\n",
      "Oscar on github,\n",
      "Oskar Köök,\n",
      "Patrik Lundin,\n",
      "RekGRpth on github,\n",
      "Ryan McQuen,\n",
      "Saravanan Musuwathi Kesavan,\n",
      "Senthil Kumaran,\n",
      "Shusen Liu,\n",
      "Sonia Hamilton,\n",
      "Spiros Georgaras,\n",
      "Stephen,\n",
      "Steve Holme,\n",
      "Stian Hvatum,\n",
      "strupo on github,\n",
      "Viktor Szakats,\n",
      "Vitaliy T,\n",
      "Wayne Lai,\n",
      "Wieland Hoffmann,\n",
      "谭九鼎\n",
      "License\n",
      "This document is licensed under the Creative Commons Attribution 4.0\n",
      "license.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Configuring CORS in FastAPI - GeeksforGeeks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to content\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TutorialsPythonJavaDSAML & Data ScienceInterview CornerProgramming LanguagesWeb DevelopmentCS SubjectsDevOpsSoftware and ToolsSchool LearningPractice Coding ProblemsCoursesDSA to DevelopmentGet IBM CertificationNewly Launched!Master Django FrameworkBecome AWS CertifiedFor Working ProfessionalsInterview 101: DSA & System DesignJAVA Backend Development (Live)DevOps Engineering (LIVE)Data Structures & Algorithms in PythonFor StudentsPlacement Preparation CourseData Science (Live)Data Structure & Algorithm-Self Paced (C++/JAVA)Master Competitive Programming (Live)Full Stack Development with React & Node JS (Live)Full Stack DevelopmentData Science ProgramAll CoursesGo Premium\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Python TutorialInterview QuestionsPython QuizPython GlossaryPython ProjectsPractice PythonData Science With PythonPython Web DevDSA with PythonPython OOPs \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sign In\n",
      "\n",
      "\n",
      "▲\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open In App\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Configuring CORS in FastAPI\n",
      "\n",
      "\n",
      "\n",
      "Last Updated : \n",
      "23 Jul, 2025\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Comments\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Improve\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Suggest changes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Like Article\n",
      "\n",
      "\n",
      "\n",
      "Like\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In this article, we are going to see what is an Origin, what is CORS, and how CORS can be used to enable different origins to access and share resources. We will see why the CORS issue occurs when the frontend and backend is present in different origin and how the browser sends a request to the server.\n",
      "What is Origin in FastAPI? An origin is basically the combination of the protocol, domain, and port on which the application is running. So for two urls even if one of the three combinations is different those urls will said to be of a different origin. The following URLs are all said to be of a different origin:\n",
      "http://localhost/\n",
      "http://localhost/:5000\n",
      "https://localhost:5000\n",
      "What is CORS in FastAPI?CORS stands for cross-origin resource sharing. It is a mechanism that allows different origins to access and share resources. Browsers enforce the same origin policy which means you can only send requests to the origin from which you are generated, but in modern applications, different origin needs to share resources. For this CORS is used which allows a standardized way to specify which origins are allowed to access the resources.\n",
      "How does CORS Mechanism work?When a web page makes an HTTP request to a different origin, then the browser first makes a preflight request which is used to check whether the given request is allowed by the server or not. The server sends the HTTP OPTIONS request which contains the following headers:\n",
      "Access-Control_Request_Method: It contains the method(PUT, POST, DELETE) that the page wants to send to the server.Access_Control_Request-Headers: It contains the header that the actual request will contain.The server needs to be configured to handle the OPTIONS request. When the server receives the request it checks whether it will allow the following request or not. The server sends back a set of CORS headers if it approves the request, the main headers include : \n",
      "Access-Control-Allow-Origin : Which specifies the origins which are allowed to access servers resources. It can be a list of origins or can contains * which specifies any origin can access the resources.Access-Control-Allow-Methods : HTTP methods that can make cross origin request to the server.Access-Control-Allow-Header : Headers that are allowed by the server when making cross origin request.The browser then check the header returned by the server and see whether it can make the request or not. If the request cannot be make it throws an error.\n",
      "What is CORS Issue?Consider you have created a web application whose frontend is running on browser at location \n",
      "http://localhost/:8000and the backend is running at the URL\n",
      "http://localhost/:5000Now consider the frontend sends an API request to the backend, in this case if the backend is not configured to handle the cross origin requests then the browser will not allow the request to be send to the server and will raise the error on the frontend. For allowing cross-origin requests the server should be configured to have the list of origins and methods that are allowed by the client. So when a cross origin request is raised by the application, a preflight request is send by the browser and the server then send the allowed origin list back to the browser with the help of which browser can now determine whether to proceed with the HTTP request or not.\n",
      "CORSMiddleware in FastAPITo allow cross-origin requests you have to configure the server which can be done with the help of CORSMiddleware class which is present in the fastapi module. To import CORSMiddleware class copy paste the below code:\n",
      "from fastapi.middleware.cors import CORSMiddlewareWe have to define the list of origins that are allowed to make the cross origin requests. You can create a list of origin as show below:\n",
      "\n",
      "Python3\n",
      "\n",
      "list = [\"http://localhost/:5000\",\n",
      "        \"http://localhost/:8000\",\n",
      "        \"https://localhost:5000\"\n",
      "       ]\n",
      "\n",
      "Now you can use the app_middleware function present in the fastAPI class for defining the CORS configuration, such as the list of origins and methods that are allowed to make the cross origin request. Copy the below code and paste it on the top of your application for CORS.\n",
      "\n",
      "Python3\n",
      "\n",
      "from fastapi import fastAPI\n",
      "from fastapi.middleware.cors import CORSMiddleware\n",
      "app = fastAPI()\n",
      "list = [\"http://localhost/:5000\",\n",
      "        \"http://localhost/:8000\",\n",
      "        \"https://localhost:5000\"\n",
      "]\n",
      "app.app_midleware(\n",
      "  CORSMiddleware,\n",
      "  allow_origins = list,\n",
      "  allow_methods = [\"*\"],\n",
      "  allow_headers = [\"*\"]\n",
      ")\n",
      "\n",
      "In the above code we specified the origins that are allowed to make the cross origin request, list of methods that are allowed, in our case it is [\"*\"] which means all the HTTP methods are allowed by the server. The list of header \"allow_headers\" which can be sent by the client which is also set to [\"*\"].\n",
      "Any request that is coming to the server will be intercepted by the middleware, and if the request contains the CORS headers such as Origins and Access-Control_request_Method, the middleware will respond with the CORS headers we defined in the app_middleware function. \n",
      "Allowing Requests from any OriginWe might want our server to take request coming from any origin rather then to some specific sets of origins, in that case we can just set the value of allow_origins = [\"*\"] which will allow any cross origin request to hit on the server.\n",
      "\n",
      "Python3\n",
      "\n",
      "from fastapi import fastAPI\n",
      "from fastapi.middleware.cors import CORSMiddleware\n",
      "app = fastAPI()\n",
      "app.app_midleware(\n",
      "  CORSMiddleware,\n",
      "  allow_origins = [\"*\"],\n",
      "  allow_methods = [\"*\"],\n",
      "  allow_headers = [\"*\"]\n",
      ")\n",
      "\n",
      "If the server configured with the above code then requests coming from any origin will be allowed by the server. For example if the client is sending request from the origin https://gfg.com:8000 to server running at http:gfgserver.com:7000 the server will allow the request from the client and will run the decorator function for the \"/\" endpoint.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Comment\n",
      "    More infoAdvertise with us \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "anshulchouhan130 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Follow \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Improve\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Tags : \n",
      "\n",
      "\n",
      "Python\n",
      "\n",
      "\n",
      "Geeks Premier League\n",
      "\n",
      "\n",
      "FastAPI\n",
      "\n",
      "\n",
      "Geeks Premier League 2023\n",
      " \n",
      "\n",
      "\n",
      "Practice Tags : \n",
      "\n",
      "python \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Similar Reads\n",
      "\n",
      "Python FundamentalsPython IntroductionPython was created 1991 with focus on code readability and express concepts in fewer lines of code.Simple and readable syntax makes it beginner-friendly.Runs seamlessly on Windows, macOS and Linux.Includes libraries for tasks like web development, data analysis and machine learning.Variable types ar\n",
      "\n",
      "\n",
      "3 min read\n",
      "Input and Output in PythonUnderstanding input and output operations is fundamental to Python programming. With the print() function, we can display output in various formats, while the input() function enables interaction with users by gathering input during program execution. Taking input in PythonPython's input() function\n",
      "\n",
      "\n",
      "7 min read\n",
      "Python VariablesIn Python, variables are used to store data that can be referenced and manipulated during program execution. A variable is essentially a name that is assigned to a value. Unlike many other programming languages, Python variables do not require explicit declaration of type. The type of the variable i\n",
      "\n",
      "\n",
      "6 min read\n",
      "Python OperatorsIn Python programming, Operators in general are used to perform operations on values and variables. These are standard symbols used for logical and arithmetic operations. In this article, we will look into different types of Python operators. OPERATORS: These are the special symbols. Eg- + , * , /,\n",
      "\n",
      "\n",
      "6 min read\n",
      "Python KeywordsKeywords in Python are reserved words that have special meanings and serve specific purposes in the language syntax. Python keywords cannot be used as the names of variables, functions, and classes or any other identifier. Getting List of all Python keywordsWe can also get all the keyword names usin\n",
      "\n",
      "\n",
      "2 min read\n",
      "Python Data TypesPython Data types are the classification or categorization of data items. It represents the kind of value that tells what operations can be performed on a particular data. Since everything is an object in Python programming, Python data types are classes and variables are instances (objects) of thes\n",
      "\n",
      "\n",
      "9 min read\n",
      "Conditional Statements in PythonConditional statements in Python are used to execute certain blocks of code based on specific conditions. These statements help control the flow of a program, making it behave differently in different situations.If Conditional Statement in PythonIf statement is the simplest form of a conditional sta\n",
      "\n",
      "\n",
      "6 min read\n",
      "Loops in Python - For, While and Nested LoopsLoops in Python are used to repeat actions efficiently. The main types are For loops (counting through items) and While loops (based on conditions). In this article, we will look at Python loops and understand their working with the help of examples. For Loop in PythonFor loops is used to iterate ov\n",
      "\n",
      "\n",
      "9 min read\n",
      "Python FunctionsPython Functions is a block of statements that does a specific task. The idea is to put some commonly or repeatedly done task together and make a function so that instead of writing the same code again and again for different inputs, we can do the function calls to reuse code contained in it over an\n",
      "\n",
      "\n",
      "9 min read\n",
      "Recursion in PythonRecursion is a programming technique where a function calls itself either directly or indirectly to solve a problem by breaking it into smaller, simpler subproblems.In Python, recursion is especially useful for problems that can be divided into identical smaller tasks, such as mathematical calculati\n",
      "\n",
      "\n",
      "6 min read\n",
      "Python Lambda FunctionsPython Lambda Functions are anonymous functions means that the function is without a name. As we already know the def keyword is used to define a normal function in Python. Similarly, the lambda keyword is used to define an anonymous function in Python. In the example, we defined a lambda function(u\n",
      "\n",
      "\n",
      "6 min read\n",
      "Python Data StructuresPython StringA string is a sequence of characters. Python treats anything inside quotes as a string. This includes letters, numbers, and symbols. Python has no character data type so single character is a string of length 1.Pythons = \"GfG\" print(s[1]) # access 2nd char s1 = s + s[0] # update print(s1) # printOut\n",
      "\n",
      "\n",
      "6 min read\n",
      "Python ListsIn Python, a list is a built-in dynamic sized array (automatically grows and shrinks). We can store all types of items (including another list) in a list. Can contain duplicate items.Mutable : We can modify, replace or delete the items. Ordered : Maintains the order of elements based on how they are\n",
      "\n",
      "\n",
      "6 min read\n",
      "Python TuplesA tuple in Python is an immutable ordered collection of elements. Tuples are similar to lists, but unlike lists, they cannot be changed after their creation (i.e., they are immutable). Tuples can hold elements of different data types. The main characteristics of tuples are being ordered , heterogene\n",
      "\n",
      "\n",
      "6 min read\n",
      "Dictionaries in PythonPython dictionary is a data structure that stores the value in key: value pairs. Values in a dictionary can be of any data type and can be duplicated, whereas keys can't be repeated and must be immutable. Example: Here, The data is stored in key:value pairs in dictionaries, which makes it easier to\n",
      "\n",
      "\n",
      "7 min read\n",
      "Python SetsPython set is an unordered collection of multiple items having different datatypes. In Python, sets are mutable, unindexed and do not contain duplicates. The order of elements in a set is not preserved and can change.Creating a Set in PythonIn Python, the most basic and efficient method for creating\n",
      "\n",
      "\n",
      "10 min read\n",
      "Python ArraysLists in Python are the most flexible and commonly used data structure for sequential storage. They are similar to arrays in other languages but with several key differences:Dynamic Typing: Python lists can hold elements of different types in the same list. We can have an integer, a string and even\n",
      "\n",
      "\n",
      "9 min read\n",
      "List Comprehension in PythonList comprehension is a concise and powerful way to create new lists by applying an expression to each item in an existing iterable (like a list, tuple or range). It helps you write clean, readable and efficient code compared to traditional loops.Syntax[expression for item in iterable if condition]P\n",
      "\n",
      "\n",
      "4 min read\n",
      "Advanced PythonPython OOP ConceptsObject Oriented Programming is a fundamental concept in Python, empowering developers to build modular, maintainable, and scalable applications. OOPs is a way of organizing code that uses objects and classes to represent real-world entities and their behavior. In OOPs, object has attributes thing th\n",
      "\n",
      "\n",
      "11 min read\n",
      "Python Exception HandlingPython Exception Handling handles errors that occur during the execution of a program. Exception handling allows to respond to the error, instead of crashing the running program. It enables you to catch and manage errors, making your code more robust and user-friendly. Let's look at an example:Handl\n",
      "\n",
      "\n",
      "6 min read\n",
      "File Handling in PythonFile handling refers to the process of performing operations on a file, such as creating, opening, reading, writing and closing it through a programming interface. It involves managing the data flow between the program and the file system on the storage device, ensuring that data is handled safely a\n",
      "\n",
      "\n",
      "4 min read\n",
      "Python Database TutorialPython being a high-level language provides support for various databases. We can connect and run queries for a particular database using Python and without writing raw queries in the terminal or shell of that particular database, we just need to have that database installed in our system.A database\n",
      "\n",
      "\n",
      "4 min read\n",
      "Python MongoDB TutorialMongoDB is a popular NoSQL database designed to store and manage data flexibly and at scale. Unlike traditional relational databases that use tables and rows, MongoDB stores data as JSON-like documents using a format called BSON (Binary JSON). This document-oriented model makes it easy to handle com\n",
      "\n",
      "\n",
      "2 min read\n",
      "Python MySQLMySQL is a widely used open-source relational database for managing structured data. Integrating it with Python enables efficient data storage, retrieval and manipulation within applications. To work with MySQL in Python, we use MySQL Connector, a driver that enables seamless integration between the\n",
      "\n",
      "\n",
      "9 min read\n",
      "Python PackagesPython packages are a way to organize and structure code by grouping related modules into directories. A package is essentially a folder that contains an __init__.py file and one or more Python files (modules). This organization helps manage and reuse code effectively, especially in larger projects.\n",
      "\n",
      "\n",
      "12 min read\n",
      "Python ModulesPython Module is a file that contains built-in functions, classes,its and variables. There are many Python modules, each with its specific work.In this article, we will cover all about Python modules, such as How to create our own simple module, Import Python modules, From statements in Python, we c\n",
      "\n",
      "\n",
      "7 min read\n",
      "Python DSA LibrariesData Structures and Algorithms (DSA) serve as the backbone for efficient problem-solving and software development. Python, known for its simplicity and versatility, offers a plethora of libraries and packages that facilitate the implementation of various DSA concepts. In this article, we'll delve in\n",
      "\n",
      "\n",
      "15 min read\n",
      "List of Python GUI Library and PackagesGraphical User Interfaces (GUIs) play a pivotal role in enhancing user interaction and experience. Python, known for its simplicity and versatility, has evolved into a prominent choice for building GUI applications. With the advent of Python 3, developers have been equipped with lots of tools and li\n",
      "\n",
      "\n",
      "11 min read\n",
      "Data Science with PythonNumPy Tutorial - Python LibraryNumPy is a core Python library for numerical computing, built for handling large arrays and matrices efficiently.ndarray object â€“ Stores homogeneous data in n-dimensional arrays for fast processing.Vectorized operations â€“ Perform element-wise calculations without explicit loops.Broadcasting â€“ Apply\n",
      "\n",
      "\n",
      "3 min read\n",
      "Pandas TutorialPandas (stands for Python Data Analysis) is an open-source software library designed for data manipulation and analysis. Revolves around two primary Data structures: Series (1D) and DataFrame (2D)Built on top of NumPy, efficiently manages large datasets, offering tools for data cleaning, transformat\n",
      "\n",
      "\n",
      "6 min read\n",
      "Matplotlib TutorialMatplotlib is an open-source visualization library for the Python programming language, widely used for creating static, animated and interactive plots. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter, Qt, GTK and wxPython. It\n",
      "\n",
      "\n",
      "5 min read\n",
      "Python Seaborn TutorialSeaborn is a library mostly used for statistical plotting in Python. It is built on top of Matplotlib and provides beautiful default styles and color palettes to make statistical plots more attractive.In this tutorial, we will learn about Python Seaborn from basics to advance using a huge dataset of\n",
      "\n",
      "\n",
      "15+ min read\n",
      "StatsModel Library- TutorialStatsmodels is a useful Python library for doing statistics and hypothesis testing. It provides tools for fitting various statistical models, performing tests and analyzing data. It is especially used for tasks in data science ,economics and other fields where understanding data is important. It is\n",
      "\n",
      "\n",
      "4 min read\n",
      "Learning Model Building in Scikit-learnBuilding machine learning models from scratch can be complex and time-consuming. Scikit-learn which is an open-source Python library which helps in making machine learning more accessible. It provides a straightforward, consistent interface for a variety of tasks like classification, regression, clu\n",
      "\n",
      "\n",
      "8 min read\n",
      "TensorFlow TutorialTensorFlow is an open-source machine-learning framework developed by Google. It is written in Python, making it accessible and easy to understand. It is designed to build and train machine learning (ML) and deep learning models. It is highly scalable for both research and production.It supports CPUs\n",
      "\n",
      "\n",
      "2 min read\n",
      "PyTorch TutorialPyTorch is an open-source deep learning framework designed to simplify the process of building neural networks and machine learning models. With its dynamic computation graph, PyTorch allows developers to modify the networkâ€™s behavior in real-time, making it an excellent choice for both beginners an\n",
      "\n",
      "\n",
      "7 min read\n",
      "Web Development with PythonFlask TutorialFlask is a lightweight and powerful web framework for Python. Itâ€™s often called a \"micro-framework\" because it provides the essentials for web development without unnecessary complexity. Unlike Django, which comes with built-in features like authentication and an admin panel, Flask keeps things mini\n",
      "\n",
      "\n",
      "8 min read\n",
      "Django Tutorial | Learn Django FrameworkDjango is a Python framework that simplifies web development by handling complex tasks for you. It follows the \"Don't Repeat Yourself\" (DRY) principle, promoting reusable components and making development faster. With built-in features like user authentication, database connections, and CRUD operati\n",
      "\n",
      "\n",
      "10 min read\n",
      "Django ORM - Inserting, Updating & Deleting DataDjango's Object-Relational Mapping (ORM) is one of the key features that simplifies interaction with the database. It allows developers to define their database schema in Python classes and manage data without writing raw SQL queries. The Django ORM bridges the gap between Python objects and databas\n",
      "\n",
      "\n",
      "4 min read\n",
      "Templating With Jinja2 in FlaskFlask is a lightweight WSGI framework that is built on Python programming. WSGI simply means Web Server Gateway Interface. Flask is widely used as a backend to develop a fully-fledged Website. And to make a sure website, templating is very important. Flask is supported by inbuilt template support na\n",
      "\n",
      "\n",
      "6 min read\n",
      "Django TemplatesTemplates are the third and most important part of Django's MVT Structure. A Django template is basically an HTML file that can also include CSS and JavaScript. The Django framework uses these templates to dynamically generate web pages that users interact with. Since Django primarily handles the ba\n",
      "\n",
      "\n",
      "7 min read\n",
      "Python | Build a REST API using FlaskPrerequisite: Introduction to Rest API REST stands for REpresentational State Transfer and is an architectural style used in modern web development. It defines a set or rules/constraints for a web application to send and receive data. In this article, we will build a REST API in Python using the Fla\n",
      "\n",
      "\n",
      "3 min read\n",
      "How to Create a basic API using Django Rest Framework ?Django REST Framework (DRF) is a powerful extension of Django that helps you build APIs quickly and easily. It simplifies exposing your Django models as RESTfulAPIs, which can be consumed by frontend apps, mobile clients or other services.Before creating an API, there are three main steps to underst\n",
      "\n",
      "\n",
      "4 min read\n",
      "Python PracticePython QuizThese Python quiz questions are designed to help you become more familiar with Python and test your knowledge across various topics. From Python basics to advanced concepts, these topic-specific quizzes offer a comprehensive way to practice and assess your understanding of Python concepts. These Pyt\n",
      "\n",
      "\n",
      "3 min read\n",
      "Python Coding Practice ProblemsThis collection of Python coding practice problems is designed to help you improve your overall programming skills in Python.The links below lead to different topic pages, each containing coding problems, and this page also includes links to quizzes. You need to log in first to write your code. Your\n",
      "\n",
      "\n",
      "1 min read\n",
      "Python Interview Questions and AnswersPython is the most used language in top companies such as Intel, IBM, NASA, Pixar, Netflix, Facebook, JP Morgan Chase, Spotify and many more because of its simplicity and powerful libraries. To crack their Online Assessment and Interview Rounds as a Python developer, we need to master important Pyth\n",
      "\n",
      "\n",
      "15+ min read\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Like\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Corporate & Communications Address:\n",
      "\n",
      "                      A-143, 7th Floor, Sovereign Corporate Tower, Sector- 136, Noida, Uttar Pradesh (201305)                    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Registered Address:\n",
      "                        K 061, Tower K, Gulshan Vivante Apartment, Sector 137, Noida, Gautam Buddh Nagar, Uttar Pradesh, 201305                      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advertise with us\n",
      "\n",
      "\n",
      "\n",
      "CompanyAbout UsLegalPrivacy PolicyIn MediaContact UsAdvertise with usGFG Corporate SolutionPlacement Training ProgramLanguagesPythonJavaC++PHPGoLangSQLR LanguageAndroid TutorialTutorials ArchiveDSADSA TutorialBasic DSA ProblemsDSA RoadmapTop 100 DSA Interview ProblemsDSA Roadmap by Sandeep JainAll Cheat SheetsData Science & MLData Science With PythonData Science For BeginnerMachine LearningML MathsData VisualisationPandasNumPyNLPDeep LearningWeb TechnologiesHTMLCSSJavaScriptTypeScriptReactJSNextJSBootstrapWeb DesignPython TutorialPython Programming ExamplesPython ProjectsPython TkinterPython Web ScrapingOpenCV TutorialPython Interview QuestionDjangoComputer ScienceOperating SystemsComputer NetworkDatabase Management SystemSoftware EngineeringDigital Logic DesignEngineering MathsSoftware DevelopmentSoftware TestingDevOpsGitLinuxAWSDockerKubernetesAzureGCPDevOps RoadmapSystem DesignHigh Level DesignLow Level DesignUML DiagramsInterview GuideDesign PatternsOOADSystem Design BootcampInterview QuestionsInteview PreparationCompetitive ProgrammingTop DS or Algo for CPCompany-Wise Recruitment ProcessCompany-Wise PreparationAptitude PreparationPuzzlesSchool SubjectsMathematicsPhysicsChemistryBiologySocial ScienceEnglish GrammarCommerceGeeksforGeeks VideosDSAPythonJavaC++Web DevelopmentData ScienceCS Subjects \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "@GeeksforGeeks, Sanchhaya Education Private Limited, All rights reserved\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        We use cookies to ensure you have the best browsing experience on our website. By using our site, you\n",
      "        acknowledge that you have read and understood our\n",
      "        Cookie Policy &\n",
      "        Privacy Policy\n",
      "\n",
      "\n",
      "        Got It !\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Improvement\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Suggest changes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Suggest Changes\n",
      "Help us improve. Share your suggestions to enhance the article. Contribute your expertise and make a difference in the GeeksforGeeks portal.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Create Improvement\n",
      "Enhance the article with your expertise. Contribute to the GeeksforGeeks community and help create better learning resources for all.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Suggest Changes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "min 4 words, max Words Limit:1000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Thank You!\n",
      "Your suggestions are valuable to us.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "What kind of Experience do you want to share?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Interview Experiences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Admission Experiences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Career Journeys\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Work Experiences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Campus Experiences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Competitive Exam Experiences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Client Challenge\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "JavaScript is disabled in your browser.\n",
      "Please enable JavaScript to proceed.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      A required part of this site couldn’t load. This may be due to a browser\n",
      "      extension, network issues, or browser settings. Please check your\n",
      "      connection, disable any ad blockers, or try using a different browser.\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "CORS Error in FastAPI Backend from React Frontend via APIM - Microsoft Q&A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tSkip to main content\n",
      "\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This browser is no longer supported.\n",
      "\n",
      "\t\t\t\t\t\tUpgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.\n",
      "\t\t\t\t\t\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\tDownload Microsoft Edge\n",
      "\t\t\t\t\t\t\n",
      "\n",
      "\t\t\t\t\t\t\tMore info about Internet Explorer and Microsoft Edge\n",
      "\t\t\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Add\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Add\n",
      "\n",
      "\n",
      "Share via\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Facebook\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x.com\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LinkedIn\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Email\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CORS Error in FastAPI Backend from React Frontend via APIM\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "KT\n",
      "\n",
      "\n",
      "190\n",
      "Reputation points\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 2024-11-05T11:11:14.96+00:00 \n",
      "\n",
      "\n",
      "Hi all,\n",
      "I have a setup with a React frontend App Service and a FastAPI backend App Service on Azure. My goal is to restrict access to the backend so that only the frontend App Service can reach it. However, I encountered a 403 IP forbidden error when trying this approach.\n",
      "To work around this, I tried using an API Management (APIM) service as a proxy between the frontend and backend. Despite adding the URLs of both the frontend App Service and APIM in the backend's CORS settings, I keep getting CORS errors when accessing the API from the browser. I also configured CORS settings in both the APIM policies and within the FastAPI code.\n",
      "Notably, this CORS error only occurs when I call the API from the browser; if I make the API call from the SSH terminal in the frontend App Service, it works without issues.\n",
      "For simplicity, I removed private endpoint restrictions on the backend and changed it to accept all traffic temporarily. I noticed that if I click on the red \"message\" text, a new tab opens and successfully shows the response from the backend API.\n",
      "\n",
      "Does anyone have suggestions on how to resolve this CORS error when calling the API from the browser?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Azure App Service\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tAzure App Service\n",
      "\t\t\t\n",
      "\n",
      "\t\t\t\tAzure App Service is a service used to create and deploy scalable, mission-critical web apps.\n",
      "\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tSign in to follow\n",
      "\t\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\tFollow\n",
      "\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\tSign in to follow\n",
      "\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\tFollow question\n",
      "\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 comments\n",
      "No comments\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report a concern\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\tI have the same question\n",
      "\t\t\t\t\t\t\t\t\n",
      "\n",
      "\t\t\t\t\t\t\t\t\tI have the same question\n",
      "\t\t\t\t\t\t\t\t\n",
      "\n",
      "0\n",
      "{count} votes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tSign in to comment\n",
      "\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\tAdd comment\n",
      "\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\tComment\n",
      "\t\t\t\t\n",
      "Use comments to ask for clarification, additional information, or improvements to the question.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Discard draft\n",
      "\n",
      "\n",
      "\t\t\tAdd comment\n",
      "\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2 answers\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sort by: \n",
      "\t\t\n",
      "\t\t\tMost helpful\n",
      "\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Most helpful Newest Oldest\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Ifeoluwa Oduwaiye\n",
      " •  \n",
      "\n",
      "\n",
      "\t\tFollow\n",
      "\t\n",
      "\n",
      "\n",
      "\n",
      "0\n",
      "Reputation points\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 2024-11-05T12:34:05.94+00:00 \n",
      "\n",
      "\n",
      "\n",
      "Hi KT,\n",
      "Can you confirm if you have enabled a CORS policy on APIM? If not, you can follow the steps here to do so: https://learn.microsoft.com/en-us/azure/api-management/enable-cors-developer-portal?wt.mc_id=studentamb_271760\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\tPlease sign in to rate this answer.\n",
      "\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Yes\n",
      "\n",
      "\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3 comments\n",
      "\n",
      "\t\t\t\t\t\tShow comments for this answer\n",
      "\t\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report a concern\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "KT\n",
      " •  \n",
      "\n",
      "\n",
      "\t\tFollow\n",
      "\t\n",
      "\n",
      "\n",
      "\n",
      "190\n",
      "Reputation points\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 2024-11-05T13:51:05.58+00:00 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hi Ifeoluwa Oduwaiye, \n",
      "Thank you for your message.\n",
      "Yes, I set CORS in the APIM. I also add URLs of both APIM and frontend App service in CORS of the backend App Service. I also added it in the code.\n",
      "APIM\n",
      "<policies>\n",
      "    <inbound>\n",
      "        <base />\n",
      "        <set-backend-service id=\"apim-generated-policy\" backend-id=\"WebApp_backend\" />\n",
      "        <cors allow-credentials=\"true\">\n",
      "            <allowed-origins>\n",
      "                <origin>https://frontapp.azurewebsites.net</origin>\n",
      "            </allowed-origins>\n",
      "            <allowed-methods preflight-result-max-age=\"300\">\n",
      "                <method>GET</method>\n",
      "                <method>POST</method>\n",
      "                <method>OPTIONS</method>\n",
      "            </allowed-methods>\n",
      "            <allowed-headers>\n",
      "                <header>*</header>\n",
      "            </allowed-headers>\n",
      "            <expose-headers>\n",
      "                <header>*</header>\n",
      "            </expose-headers>\n",
      "        </cors>\n",
      "    </inbound>\n",
      "    <backend>\n",
      "        <base />\n",
      "    </backend>\n",
      "    <outbound>\n",
      "        <base />\n",
      "    </outbound>\n",
      "    <on-error>\n",
      "        <base />\n",
      "    </on-error>\n",
      "</policies>\n",
      "\n",
      "FastAPI\n",
      "app.add_middleware(\n",
      "    CORSMiddleware,\n",
      "    allow_origins=[\"https://frontapp.azurewebsites.net\",\n",
      "                   \"https://myapim.azure-api.net\", \"*\"],\n",
      "    allow_credentials=True,\n",
      "    allow_methods=[\"*\"],\n",
      "    allow_headers=[\"*\"],\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 votes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report a concern\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shree Hima Bindu Maganti \n",
      " •  \n",
      "\n",
      "\n",
      "\t\tFollow\n",
      "\t\n",
      "\n",
      "\n",
      "\n",
      "5,480\n",
      "Reputation points • Microsoft External Staff\n",
      "\t\t• Moderator\n",
      "\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 2024-11-12T02:40:52.74+00:00 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hi @KT ,\n",
      "Following up to see if you have chance to check my previous response and help us with requested information to check and assist you further on this.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 votes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report a concern\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shree Hima Bindu Maganti \n",
      " •  \n",
      "\n",
      "\n",
      "\t\tFollow\n",
      "\t\n",
      "\n",
      "\n",
      "\n",
      "5,480\n",
      "Reputation points • Microsoft External Staff\n",
      "\t\t• Moderator\n",
      "\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 2024-11-13T00:42:48.2033333+00:00 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hi @KT ,\n",
      "Following up to see if you have chance to check my previous response and help us with requested information to check and assist you further on this.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 votes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report a concern\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tSign in to comment\n",
      "\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\tAdd comment\n",
      "\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\tComment\n",
      "\t\t\t\t\n",
      "Use comments to ask for clarification, additional information, or improvements to the question.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Discard draft\n",
      "\n",
      "\n",
      "\t\t\tAdd comment\n",
      "\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Shree Hima Bindu Maganti \n",
      " •  \n",
      "\n",
      "\n",
      "\t\tFollow\n",
      "\t\n",
      "\n",
      "\n",
      "\n",
      "5,480\n",
      "Reputation points • Microsoft External Staff\n",
      "\t\t• Moderator\n",
      "\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 2024-11-11T04:43:59.76+00:00 \n",
      "\n",
      "\n",
      "\n",
      "Hi @kT,  \n",
      "Thanks for sharing your detailed configuration!   \n",
      "Based on what you've set up, it looks like you've covered most of the things. However, here are a few suggestions that might help resolve the CORS issue when calling the API from the browser:  \n",
      "Remove \"*\" in allow_origins: Update the allow_origins in FastAPI to only include your frontend and APIM URLs:\n",
      "app.add_middleware(\n",
      "    CORSMiddleware,\n",
      "    allow_origins=[\"https://frontapp.azurewebsites.net\", \"https://myapim.azure-api.net\"],\n",
      "    allow_credentials=True,\n",
      "    allow_methods=[\"*\"],\n",
      "    allow_headers=[\"*\"],\n",
      ")\n",
      "\n",
      "Verify APIM URL: Ensure https://myapim.azure-api.net is the exact URL your frontend is accessing, with correct protocol and no typos.\n",
      "Check Preflight Response: Use the browser's developer tools to inspect the OPTIONS preflight request. Confirm that Access-Control-Allow-Origin and other CORS headers are returned correctly.\n",
      "Explicitly Define Headers in APIM Policy: Instead of using <header>*</header>, explicitly specify needed headers like Authorization and Content-Type in <allowed-headers> and <expose-headers>.\n",
      "Clear Browser Cache: Test in an incognito window or clear the browser cache to avoid cached CORS settings.\n",
      "Confirm Backend CORS Settings: Ensure any CORS settings in the backend App Service are consistent with your APIM and Fast API configurations.  \n",
      "Let me know for any further assistance.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\tPlease sign in to rate this answer.\n",
      "\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Yes\n",
      "\n",
      "\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 comments\n",
      "No comments\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report a concern\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tSign in to comment\n",
      "\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\tAdd comment\n",
      "\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\tComment\n",
      "\t\t\t\t\n",
      "Use comments to ask for clarification, additional information, or improvements to the question.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Discard draft\n",
      "\n",
      "\n",
      "\t\t\tAdd comment\n",
      "\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\tSign in to answer\n",
      "\t\t\t\t\t\t\t\t\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\tYour answer\n",
      "\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\tAnswer\n",
      "\t\t\t\t\n",
      "Answers can be marked as Accepted Answers by the question author, which helps users to know the answer solved the author's problem.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tPost your answer\n",
      "\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Discard draft\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\tAdditional resources\n",
      "\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question activity\n",
      "\n",
      "\n",
      "\t\t\tSign in to follow questions and users\n",
      "\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\tFollow question\n",
      "\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tAdditional resources\n",
      "\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "en-us\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your Privacy Choices\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Theme\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Light \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Dark \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " High contrast \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AI Disclaimer\n",
      "\n",
      "Previous Versions\n",
      " \n",
      "Blog\n",
      " \n",
      "Contribute\n",
      "\n",
      "Privacy\n",
      "\n",
      "Terms of Use\n",
      "\n",
      "Code of Conduct\n",
      "\n",
      "Trademarks\n",
      "\n",
      "© Microsoft 2025\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tutorial - User Guide - FastAPI\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "          Skip to content\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Follow @fastapi on Twitter to stay updated\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Follow FastAPI on LinkedIn to stay updated\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Subscribe to the FastAPI and friends newsletter 🎉\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sponsor\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sponsor\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sponsor\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sponsor\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sponsor\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sponsor\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sponsor\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sponsor\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sponsor\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sponsor\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sponsor\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            FastAPI\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            \n",
      "              Tutorial - User Guide\n",
      "            \n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "              en - English\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              az - azərbaycan dili\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              bn - বাংলা\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              de - Deutsch\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              es - español\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              fa - فارسی\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              fr - français\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              he - עברית\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              hu - magyar\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              id - Bahasa Indonesia\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              it - italiano\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              ja - 日本語\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              ko - 한국어\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              nl - Nederlands\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              pl - Polski\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              pt - português\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              ru - русский язык\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              tr - Türkçe\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              uk - українська мова\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              ur - اردو\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              vi - Tiếng Việt\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              yo - Yorùbá\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              zh - 简体中文\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              zh-hant - 繁體中文\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "              😉\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            Initializing search\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    fastapi/fastapi\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        \n",
      "  \n",
      "    \n",
      "  \n",
      "  FastAPI\n",
      "\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "        \n",
      "  \n",
      "    \n",
      "  \n",
      "  Features\n",
      "\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "          \n",
      "  \n",
      "    \n",
      "  \n",
      "  Learn\n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "          \n",
      "  \n",
      "    \n",
      "  \n",
      "  Reference\n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "        \n",
      "  \n",
      "    \n",
      "  \n",
      "  FastAPI People\n",
      "\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "          \n",
      "  \n",
      "    \n",
      "  \n",
      "  Resources\n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "          \n",
      "  \n",
      "    \n",
      "  \n",
      "  About\n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "        \n",
      "  \n",
      "    \n",
      "  \n",
      "  Release Notes\n",
      "\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    FastAPI\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    fastapi/fastapi\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "  \n",
      "    FastAPI\n",
      "  \n",
      "\n",
      "    \n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Features\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "  \n",
      "    Learn\n",
      "  \n",
      "\n",
      "    \n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            \n",
      "  \n",
      "    Learn\n",
      "  \n",
      "\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Python Types Intro\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Concurrency and async / await\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Environment Variables\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Virtual Environments\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "  \n",
      "    Tutorial - User Guide\n",
      "  \n",
      "\n",
      "    \n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            \n",
      "  \n",
      "    Tutorial - User Guide\n",
      "  \n",
      "\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      First Steps\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Path Parameters\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Query Parameters\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Request Body\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Query Parameters and String Validations\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Path Parameters and Numeric Validations\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Query Parameter Models\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Body - Multiple Parameters\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Body - Fields\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Body - Nested Models\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Declare Request Example Data\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Extra Data Types\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Cookie Parameters\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Header Parameters\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Cookie Parameter Models\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Header Parameter Models\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Response Model - Return Type\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Extra Models\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Response Status Code\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Form Data\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Form Models\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Request Files\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Request Forms and Files\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Handling Errors\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Path Operation Configuration\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      JSON Compatible Encoder\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Body - Updates\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "  \n",
      "    Dependencies\n",
      "  \n",
      "\n",
      "    \n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            \n",
      "  \n",
      "    Dependencies\n",
      "  \n",
      "\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Classes as Dependencies\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Sub-dependencies\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Dependencies in path operation decorators\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Global Dependencies\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Dependencies with yield\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "  \n",
      "    Security\n",
      "  \n",
      "\n",
      "    \n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            \n",
      "  \n",
      "    Security\n",
      "  \n",
      "\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Security - First Steps\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Get Current User\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Simple OAuth2 with Password and Bearer\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      OAuth2 with Password (and hashing), Bearer with JWT tokens\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Middleware\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      CORS (Cross-Origin Resource Sharing)\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      SQL (Relational) Databases\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Bigger Applications - Multiple Files\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Background Tasks\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Metadata and Docs URLs\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Static Files\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Testing\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Debugging\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "  \n",
      "    Advanced User Guide\n",
      "  \n",
      "\n",
      "    \n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            \n",
      "  \n",
      "    Advanced User Guide\n",
      "  \n",
      "\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Path Operation Advanced Configuration\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Additional Status Codes\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Return a Response Directly\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Custom Response - HTML, Stream, File, others\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Additional Responses in OpenAPI\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Response Cookies\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Response Headers\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Response - Change Status Code\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Advanced Dependencies\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "  \n",
      "    Advanced Security\n",
      "  \n",
      "\n",
      "    \n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            \n",
      "  \n",
      "    Advanced Security\n",
      "  \n",
      "\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      OAuth2 scopes\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      HTTP Basic Auth\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Using the Request Directly\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Using Dataclasses\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Advanced Middleware\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Sub Applications - Mounts\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Behind a Proxy\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Templates\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      WebSockets\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Lifespan Events\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Testing WebSockets\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Testing Events: startup - shutdown\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Testing Dependencies with Overrides\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Async Tests\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Settings and Environment Variables\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      OpenAPI Callbacks\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      OpenAPI Webhooks\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Including WSGI - Flask, Django, others\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Generating SDKs\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      FastAPI CLI\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "  \n",
      "    Deployment\n",
      "  \n",
      "\n",
      "    \n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            \n",
      "  \n",
      "    Deployment\n",
      "  \n",
      "\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      About FastAPI versions\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      About HTTPS\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Run a Server Manually\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Deployments Concepts\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Deploy FastAPI on Cloud Providers\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Server Workers - Uvicorn with Workers\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      FastAPI in Containers - Docker\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "  \n",
      "    How To - Recipes\n",
      "  \n",
      "\n",
      "    \n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            \n",
      "  \n",
      "    How To - Recipes\n",
      "  \n",
      "\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      General - How To - Recipes\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      GraphQL\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Custom Request and APIRoute class\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Conditional OpenAPI\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Extending OpenAPI\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Separate OpenAPI Schemas for Input and Output or Not\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Custom Docs UI Static Assets (Self-Hosting)\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Configure Swagger UI\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Testing a Database\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "  \n",
      "    Reference\n",
      "  \n",
      "\n",
      "    \n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            \n",
      "  \n",
      "    Reference\n",
      "  \n",
      "\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FastAPI class\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Request Parameters\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Status Codes\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "UploadFile class\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Exceptions - HTTPException and WebSocketException\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Dependencies - Depends() and Security()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "APIRouter class\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Background Tasks - BackgroundTasks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Request class\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      WebSockets\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "HTTPConnection class\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Response class\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Custom Response Classes - File, HTML, Redirect, Streaming, etc.\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Middleware\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "  \n",
      "    OpenAPI\n",
      "  \n",
      "\n",
      "    \n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            \n",
      "  \n",
      "    OpenAPI\n",
      "  \n",
      "\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      OpenAPI docs\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      OpenAPI models\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Security Tools\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Encoders - jsonable_encoder\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Static Files - StaticFiles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Templating - Jinja2Templates\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Test Client - TestClient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      FastAPI People\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "  \n",
      "    Resources\n",
      "  \n",
      "\n",
      "    \n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            \n",
      "  \n",
      "    Resources\n",
      "  \n",
      "\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Help FastAPI - Get Help\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Development - Contributing\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Full Stack FastAPI Template\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      External Links and Articles\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      FastAPI and friends newsletter\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Repository Management Tasks\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "  \n",
      "    About\n",
      "  \n",
      "\n",
      "    \n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            \n",
      "  \n",
      "    About\n",
      "  \n",
      "\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Alternatives, Inspiration and Comparisons\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      History, Design and Future\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Benchmarks\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Repository Management\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Release Notes\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Table of contents\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "          Run the code\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "          Install FastAPI\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "          Advanced User Guide\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    FastAPI\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Learn\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Tutorial - User Guide\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tutorial - User Guide¶\n",
      "This tutorial shows you how to use FastAPI with most of its features, step by step.\n",
      "Each section gradually builds on the previous ones, but it's structured to separate topics, so that you can go directly to any specific one to solve your specific API needs.\n",
      "It is also built to work as a future reference so you can come back and see exactly what you need.\n",
      "Run the code¶\n",
      "All the code blocks can be copied and used directly (they are actually tested Python files).\n",
      "To run any of the examples, copy the code to a file main.py, and start fastapi dev with:\n",
      "\n",
      "$ <font color=\"#4E9A06\">fastapi</font> dev <u style=\"text-decoration-style:solid\">main.py</u>\n",
      "\n",
      "  <span style=\"background-color:#009485\"><font color=\"#D3D7CF\"> FastAPI </font></span>  Starting development server 🚀\n",
      "\n",
      "             Searching for package file structure from directories\n",
      "             with <font color=\"#3465A4\">__init__.py</font> files\n",
      "             Importing from <font color=\"#75507B\">/home/user/code/</font><font color=\"#AD7FA8\">awesomeapp</font>\n",
      "\n",
      "   <span style=\"background-color:#007166\"><font color=\"#D3D7CF\"> module </font></span>  🐍 main.py\n",
      "\n",
      "     <span style=\"background-color:#007166\"><font color=\"#D3D7CF\"> code </font></span>  Importing the FastAPI app object from the module with\n",
      "             the following code:\n",
      "\n",
      "             <u style=\"text-decoration-style:solid\">from </u><u style=\"text-decoration-style:solid\"><b>main</b></u><u style=\"text-decoration-style:solid\"> import </u><u style=\"text-decoration-style:solid\"><b>app</b></u>\n",
      "\n",
      "      <span style=\"background-color:#007166\"><font color=\"#D3D7CF\"> app </font></span>  Using import string: <font color=\"#3465A4\">main:app</font>\n",
      "\n",
      "   <span style=\"background-color:#007166\"><font color=\"#D3D7CF\"> server </font></span>  Server started at <font color=\"#729FCF\"><u style=\"text-decoration-style:solid\">http://127.0.0.1:8000</u></font>\n",
      "   <span style=\"background-color:#007166\"><font color=\"#D3D7CF\"> server </font></span>  Documentation at <font color=\"#729FCF\"><u style=\"text-decoration-style:solid\">http://127.0.0.1:8000/docs</u></font>\n",
      "\n",
      "      <span style=\"background-color:#007166\"><font color=\"#D3D7CF\"> tip </font></span>  Running in development mode, for production use:\n",
      "             <b>fastapi run</b>\n",
      "\n",
      "             Logs:\n",
      "\n",
      "     <span style=\"background-color:#007166\"><font color=\"#D3D7CF\"> INFO </font></span>  Will watch for changes in these directories:\n",
      "             <b>[</b><font color=\"#4E9A06\">&apos;/home/user/code/awesomeapp&apos;</font><b>]</b>\n",
      "     <span style=\"background-color:#007166\"><font color=\"#D3D7CF\"> INFO </font></span>  Uvicorn running on <font color=\"#729FCF\"><u style=\"text-decoration-style:solid\">http://127.0.0.1:8000</u></font> <b>(</b>Press CTRL+C\n",
      "             to quit<b>)</b>\n",
      "     <span style=\"background-color:#007166\"><font color=\"#D3D7CF\"> INFO </font></span>  Started reloader process <b>[</b><font color=\"#34E2E2\"><b>383138</b></font><b>]</b> using WatchFiles\n",
      "     <span style=\"background-color:#007166\"><font color=\"#D3D7CF\"> INFO </font></span>  Started server process <b>[</b><font color=\"#34E2E2\"><b>383153</b></font><b>]</b>\n",
      "     <span style=\"background-color:#007166\"><font color=\"#D3D7CF\"> INFO </font></span>  Waiting for application startup.\n",
      "     <span style=\"background-color:#007166\"><font color=\"#D3D7CF\"> INFO </font></span>  Application startup complete.\n",
      "\n",
      "\n",
      "It is HIGHLY encouraged that you write or copy the code, edit it and run it locally.\n",
      "Using it in your editor is what really shows you the benefits of FastAPI, seeing how little code you have to write, all the type checks, autocompletion, etc.\n",
      "\n",
      "Install FastAPI¶\n",
      "The first step is to install FastAPI.\n",
      "Make sure you create a virtual environment, activate it, and then install FastAPI:\n",
      "\n",
      "$ pip install \"fastapi[standard]\"\n",
      "\n",
      "---> 100%\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "When you install with pip install \"fastapi[standard]\" it comes with some default optional standard dependencies, including fastapi-cloud-cli, which allows you to deploy to FastAPI Cloud.\n",
      "If you don't want to have those optional dependencies, you can instead install pip install fastapi.\n",
      "If you want to install the standard dependencies but without the fastapi-cloud-cli, you can install with pip install \"fastapi[standard-no-fastapi-cloud-cli]\".\n",
      "\n",
      "Advanced User Guide¶\n",
      "There is also an Advanced User Guide that you can read later after this Tutorial - User guide.\n",
      "The Advanced User Guide builds on this one, uses the same concepts, and teaches you some extra features.\n",
      "But you should first read the Tutorial - User Guide (what you are reading right now).\n",
      "It's designed so that you can build a complete application with just the Tutorial - User Guide, and then extend it in different ways, depending on your needs, using some of the additional ideas from the Advanced User Guide.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Back to top\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Previous\n",
      "              \n",
      "\n",
      "                Virtual Environments\n",
      "              \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Next\n",
      "              \n",
      "\n",
      "                First Steps\n",
      "              \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        The FastAPI trademark is owned by @tiangolo and is registered in the US and across other regions\n",
      "    \n",
      "    \n",
      "    Made with\n",
      "    \n",
      "        Material for MkDocs\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Introduction to LLMOps for machine learning : Beginners GuideHome BlogLLMOps in machine learningIntroduction to LLMOps for machine learning : Beginners GuideNikita DabariSoftware EngineerPublished On Updated On Table of Content\n",
      "Large Language Models (LLMs) like GPT and BERT are transforming the landscape of machine learning by enabling applications such as conversational AI, content generation, and advanced analytics. However, managing, deploying, and scaling these models comes with unique challenges, giving rise to the field of LLMOps.This article is a beginner's guide to understanding LLMOps—the specialized practices and tools for handling LLMs efficiently. Whether you're a data scientist, engineer, or AI enthusiast, this guide will help you grasp the fundamentals and unlock the potential of LLMOps in your machine learning projects.Benefits for ReadersBy reading this article, you'll learn:The importance of LLMOps in modern AI workflows.How to streamline processes like model fine-tuning, deployment, and monitoring.Tools and best practices to manage LLMs effectively.Ethical considerations and real-world applications of LLMOps.What We’ll CoverIntroduction to LLMOps: Understanding its role and how it differs from MLOps.Key Components of LLMOps: A breakdown of training, fine-tuning, deployment, and monitoring processes.Challenges and Solutions: Tackling computational costs, scalability, and model performance.Tools and Frameworks: Overview of popular tools like Hugging Face and LangChain.Best Practices and Tips: Actionable advice for beginners to implement LLMOps effectively.Future Trends: Insights into where LLMOps is headed in the evolving AI ecosystem.By the end of this article, you’ll have a solid foundation to get started with LLMOps and integrate it into your machine learning workflow, making the most out of the power of large language models.\n",
      "What is LLMOps?LLMOps, or Large Language Model Operations, is a system of practices and tools designed to manage large language models (LLMs) effectively. These models, like GPT or BERT, are the engines behind many modern AI applications such as chatbots, automated content generation, and virtual assistants. While they are incredibly powerful, they also come with unique challenges due to their size, complexity, and the resources they require.LLMOps focuses on simplifying and streamlining the entire lifecycle of LLMs. This includes tasks like preparing the data, fine-tuning the model for specific needs, deploying it into real-world systems, and ensuring it keeps performing well over time.\n",
      "Why Do We Need LLMOps?Large language models are shaping the future of artificial intelligence, but they are not easy to work with. Here’s why LLMOps is becoming so important:Managing Complexity:\n",
      "LLMs are much larger and more resource-intensive than traditional machine learning models. They require special tools and strategies to handle their size and computational needs.Customizing for Real-World Use:\n",
      "A generic language model might not be the best fit for every task. For example, a company might want a model that understands legal documents or customer reviews. LLMOps makes it easier to fine-tune these models for specific industries or tasks.Making Deployment Easier:\n",
      "Putting an LLM into action isn’t as simple as clicking a button. It needs to be optimized for speed, accuracy, and cost. LLMOps ensures the model runs efficiently in production environments.Keeping Models Up-to-Date:\n",
      "Over time, the data and needs of a business can change. LLMOps helps in retraining and updating models so they stay relevant and accurate.Ensuring Fair and Ethical AI:\n",
      "Large language models can unintentionally reflect biases in their training data. LLMOps provides tools to monitor, identify, and reduce bias, ensuring the models are fair and responsible.\n",
      "Why Does LLMOps Matter to Machine Learning?Imagine trying to operate a high-performance car without a proper maintenance plan—it might run well for a while, but it’s bound to break down eventually. That’s what happens when large language models are deployed without LLMOps. They may seem magical at first, but without proper management, they can become expensive, slow, or even unreliable.With LLMOps, businesses can make sure their AI systems are:Scalable: Ready to handle growing workloads.Cost-effective: Optimized to avoid wasting resources.Reliable: Consistently accurate and performing as expected.Ethical: Free from harmful biases or unintended consequences.\n",
      "Role of LLMOps in Machine LearningLarge Language Models (LLMs) are transforming the AI landscape with their ability to process and generate human-like text. However, managing these models effectively requires specialized approaches, as traditional machine learning workflows often fall short. This is where LLMOps comes in, providing targeted tools and practices for handling the unique challenges of large language models.\n",
      "How LLMOps Complements MLOpsLLMOps builds on the foundations of MLOps, extending its capabilities to address the specific needs of LLMs. While MLOps is designed for general machine learning model management, LLMOps focuses on the distinct requirements of large-scale language models.1. Handling Scale and ComplexityMLOps is well-suited for smaller, domain-specific models, but LLMs demand far greater computational resources and infrastructure due to their size and complexity. LLMOps introduces techniques like distributed training and inference optimization to manage these requirements effectively.2. Specialized Fine-TuningIn MLOps, fine-tuning typically involves retraining models on new data. With LLMOps, fine-tuning becomes more complex, often requiring domain-specific strategies and tools for adapting pre-trained models to niche tasks.3. Inference EfficiencyTraditional MLOps handles model deployment efficiently, but the sheer size of LLMs makes real-time inference a challenge. LLMOps optimizes inference pipelines through methods like model compression, quantization, and caching to ensure responsiveness.4. Monitoring and MaintenanceMLOps provides general monitoring tools to track model performance. LLMOps takes this further with advanced monitoring tailored for LLMs, focusing on aspects like output quality, latency, and ethical concerns.5. Ethical AI ManagementWhile MLOps includes basic mechanisms for bias detection, LLMOps emphasizes thorough auditing and mitigation strategies to ensure that large-scale models generate fair and responsible outputs.6. Collaboration Across RolesLLMOps bridges the gap between MLOps workflows and the unique requirements of LLMs, enabling better collaboration between AI teams, DevOps, and business stakeholders.\n",
      "Why LLMOps is EssentialBy complementing and extending MLOps, LLMOps ensures that the immense potential of large language models is realized efficiently and responsibly. It provides a tailored framework for managing LLMs at scale, enabling their effective integration into machine learning workflows and real-world applications.\n",
      "Key Components of LLMOpsManaging large language models (LLMs) effectively requires a well-structured approach to ensure they perform optimally in real-world applications. LLMOps breaks this process into key components, each addressing a specific stage in the lifecycle of an LLM. These components include model training, fine-tuning, deployment, and monitoring. Model TrainingWhat it is:\n",
      "Model training involves building an LLM from scratch or using an existing pre-trained model as a foundation. It requires vast datasets and substantial computational resources to ensure the model understands a wide range of language patterns.Key aspects:Data Preparation: Cleaning and organizing datasets to improve model quality.Infrastructure: Using distributed systems like GPUs or TPUs to handle the computational load.Optimization: Techniques like gradient descent and adaptive learning rates to enhance training efficiency.Challenges Addressed by LLMOps:Managing large datasets and ensuring their quality.Efficiently utilizing compute resources to reduce time and cost.\n",
      " Fine-TuningWhat it is:\n",
      "Fine-tuning is the process of adapting a pre-trained model to a specific task or domain by retraining it on specialized datasets. For example, an LLM can be fine-tuned to understand legal documents or customer service conversations.Key aspects:Task-Specific Data: Collecting domain-relevant data for retraining.Parameter Adjustment: Modifying model weights to optimize performance for the new task.Evaluation: Measuring task-specific performance to validate the fine-tuning process.Challenges Addressed by LLMOps:Automating and scaling fine-tuning workflows for multiple use cases.Ensuring that fine-tuning doesn’t degrade the model’s performance on general tasks.\n",
      "DeploymentWhat it is:\n",
      "Deployment is the process of making an LLM available for use in production systems. This could be through APIs, applications, or integrated systems.Key aspects:Model Serving: Hosting the model for real-time or batch processing.Latency Optimization: Reducing response times to meet user demands.Scalability: Ensuring the system can handle increasing loads without performance degradation.Challenges Addressed by LLMOps:Managing infrastructure costs while ensuring high performance.Optimizing models for deployment environments, such as edge devices or cloud servers.\n",
      "MonitoringWhat it is:\n",
      "Monitoring involves tracking the performance and behavior of an LLM in production to ensure it continues to meet expectations over time.Key aspects:Performance Metrics: Tracking latency, accuracy, and throughput.Drift Detection: Identifying changes in data patterns that might reduce the model’s effectiveness.Error Analysis: Investigating and resolving issues like incorrect or biased outputs.Challenges Addressed by LLMOps:Automating monitoring to quickly detect and respond to issues.Providing insights into model behavior to improve future iterations.\n",
      "Why These Components MatterThese key components—training, fine-tuning, deployment, and monitoring—form the backbone of LLMOps. Together, they ensure that large language models are built, customized, and managed in a way that maximizes their value while minimizing risks. By addressing each stage systematically, LLMOps enables organizations to unlock the full potential of LLMs in real-world scenarios.\n",
      "Challenges in Managing Large Language Models\n",
      "\n",
      "Large Language Models (LLMs) have revolutionized AI applications, but managing them effectively comes with significant challenges. These include the need for extensive computational resources, ensuring data privacy and security, and addressing cost management concerns. Let’s explore these challenges in detail.1. Computational ResourcesThe Challenge:\n",
      "LLMs are computationally intensive, requiring massive amounts of processing power and memory. Training or even fine-tuning these models demands high-performance hardware, often using GPUs or TPUs in distributed systems. Running these models in real-time for inference also places a heavy burden on infrastructure.Key Issues:Scalability: Managing large-scale distributed training across multiple nodes.Latency: Ensuring fast response times during inference, especially in real-time applications.Energy Consumption: High energy requirements make LLMs less sustainable.Possible Solutions:Leveraging optimized frameworks like PyTorch or TensorFlow for distributed training.Using techniques like model pruning, quantization, or knowledge distillation to reduce model size without sacrificing performance.Employing cloud-based platforms with scalable GPU/TPU resources.2. Data Privacy and SecurityThe Challenge:\n",
      "Training LLMs often involves using vast datasets, some of which may include sensitive or proprietary information. Ensuring that the data used for training and the outputs generated by the model comply with privacy laws and security standards is critical.Key Issues:Regulatory Compliance: Adhering to data protection laws such as GDPR, HIPAA, or CCPA.Sensitive Information: Preventing models from memorizing and leaking private data.Adversarial Attacks: Protecting models from being exploited to produce harmful or biased outputs.Possible Solutions:Implementing differential privacy techniques to anonymize data used during training.Regularly auditing models to ensure they don’t retain sensitive information.Using secure storage and encryption for datasets and model artifacts.3. Cost ManagementThe Challenge:\n",
      "Training and deploying LLMs are expensive. From the hardware required for training to the ongoing costs of running inference at scale, the financial burden can be prohibitive for many organizations.Key Issues:Training Costs: High compute requirements can make training an LLM cost millions of dollars.Deployment Costs: Inference, particularly at scale, requires continuous computational resources, leading to high operational expenses.Infrastructure Investment: Setting up on-premises hardware or paying for cloud resources adds to the overall cost.Possible Solutions:Utilizing pre-trained models and fine-tuning them instead of training from scratch.Optimizing inference with techniques like caching, batching, and serverless architecture.Monitoring and scaling resource usage based on actual demand to avoid unnecessary expenses.\n",
      "Building an LLMOps PipelineAn effective LLMOps pipeline ensures the smooth lifecycle management of large language models (LLMs), from preparing data to deploying and maintaining the model in production. Here's a breakdown of the key stages in building such a pipeline:1. Data Preparation and PreprocessingWhat It Involves:\n",
      "The foundation of any LLM is the data it’s trained on. Preparing and preprocessing this data ensures the model learns accurately and effectively.Steps:Data Collection: Gather diverse, high-quality datasets relevant to the target use case.Data Cleaning: Remove errors, duplicates, and inconsistencies from the data.Preprocessing: Tokenize text, normalize language (e.g., handling casing, punctuation), and format it for the LLM architecture.Data Augmentation: Enhance the dataset with techniques like paraphrasing or back-translation to improve robustness.Challenges:Ensuring data quality and eliminating bias.Managing large datasets efficiently.Complying with privacy regulations.2. Model Training and Fine-TuningWhat It Involves:\n",
      "Training involves building the model from scratch or using a pre-trained model, while fine-tuning adapts the model to specific tasks or domains.Steps:Training Setup: Define model architecture, training objectives, and hyperparameters.Distributed Training: Use multi-node setups with GPUs/TPUs for scalability.Fine-Tuning: Leverage pre-trained models and retrain them with task-specific data.Validation: Evaluate the model on test data to ensure it meets performance benchmarks.Challenges:High computational resource demands.Risk of overfitting during fine-tuning.Optimizing training time and cost.3. Model Serving and DeploymentWhat It Involves:\n",
      "Once the model is trained and fine-tuned, it needs to be deployed to serve predictions efficiently in real-world applications.Steps:Model Serialization: Save the trained model in a format optimized for serving, such as ONNX or TensorFlow SavedModel.Serving Infrastructure: Use platforms like TensorFlow Serving, TorchServe, or cloud-based services to host the model.Latency Optimization: Implement batching, caching, and quantization to improve inference speed.Scalability: Ensure the system can handle fluctuating workloads using auto-scaling.Challenges:Balancing latency and resource usage.Ensuring fault tolerance and high availability.Protecting the model from unauthorized access.4. Continuous Monitoring and UpdatesWhat It Involves:\n",
      "Monitoring the model in production ensures it continues to perform as expected over time. Regular updates keep the model aligned with new data and requirements.Steps:Performance Tracking: Monitor key metrics like accuracy, latency, and throughput.Drift Detection: Identify data or concept drift that may degrade model performance.Feedback Loop: Collect user feedback and use it to refine the model.Retraining and Updates: Periodically update the model with fresh data to maintain relevance.Challenges:Detecting subtle changes in data patterns.Automating the retraining process.Balancing update frequency with operational stability.\n",
      "\n",
      "Popular Tools for LLMOpsBuilding and managing large language models (LLMs) requires a combination of specialized frameworks and robust infrastructure tools. These tools simplify processes like training, fine-tuning, deploying, and scaling LLMs, making them essential components of LLMOps. Here’s a look at some of the most popular tools in the ecosystem:Frameworks for LLMOpsHugging FaceWhat It Is: A leading platform for working with pre-trained models and fine-tuning them for specific tasks.Key Features:Why Use It: Hugging Face simplifies the process of fine-tuning and deploying LLMs, making it ideal for beginners and professionals alike.LangChainWhat It Is: A framework for building applications powered by LLMs, with a focus on combining them with external data and workflows.Key Features:Why Use It: LangChain is perfect for creating advanced applications like chatbots and document analysis systems that rely on LLMs.OpenAI APIsWhat It Is: APIs provided by OpenAI to access models like GPT-3, GPT-4, and Codex.Key Features:Why Use It: OpenAI APIs remove the need for local infrastructure, offering scalable solutions for deploying LLMs quickly.Infrastructure Tools for LLMOpsKubernetesWhat It Is: An open-source system for automating deployment, scaling, and management of containerized applications.Key Features:Why Use It: Kubernetes is ideal for deploying and scaling LLMs in production environments, especially when dealing with high traffic.DockerWhat It Is: A platform for developing, shipping, and running applications in isolated containers.Key Features:Why Use It: Docker ensures consistency across development and production environments, making it easier to deploy and maintain LLMs.RayWhat It Is: A distributed computing framework for scaling Python workloads, including ML training and inference.Key Features:Why Use It: Ray is designed for scaling LLM workloads efficiently, making it a go-to tool for training and serving large models.\n",
      "\n",
      "Real-World Applications of LLMOpsLLMOps is revolutionizing how large language models (LLMs) are applied in real-world scenarios. By streamlining the development, deployment, and management of LLMs, LLMOps enables diverse and impactful applications. Here are some notable examples:1. Chatbots and Virtual AssistantsHow They Work:\n",
      "LLMs power conversational agents like chatbots and virtual assistants, enabling them to understand and generate human-like text.Applications:Role of LLMOps:\n",
      "LLMOps ensures efficient fine-tuning of LLMs for specific industries and manages real-time inference to maintain responsiveness.2. Content GenerationHow They Work:\n",
      "LLMs can generate articles, marketing copy, product descriptions, and even code snippets with minimal human input.Applications:Role of LLMOps:\n",
      "LLMOps helps manage workflows for training LLMs on specific writing styles or brand guidelines, optimizing their output for quality and consistency.3. Sentiment AnalysisHow It Works:\n",
      "LLMs analyze text to determine the sentiment (positive, neutral, or negative) expressed by users.Applications:Role of LLMOps:\n",
      "LLMOps ensures models are updated regularly with new data to improve accuracy and stay relevant to changing trends in user sentiment.\n",
      "\n",
      "Future of LLMOpsAs the field of LLMOps evolves, several exciting trends and innovations are shaping its future. Here are some key developments to watch:1. Scaling LLMs for EfficiencyTrend:\n",
      "Developing strategies to reduce the computational demands of LLMs without sacrificing performance.Innovations:2. Ethical AI PracticesTrend:\n",
      "Increasing focus on ensuring LLMs are fair, unbiased, and responsible in their outputs.Innovations:3. Real-Time AdaptationTrend:\n",
      "Enabling LLMs to adapt to new data and tasks without extensive retraining.Innovations:4. Integration with Emerging TechnologiesTrend:\n",
      "Combining LLMOps with advancements in other fields to unlock new possibilities.Innovations:5. Democratization of LLMsTrend:\n",
      "Making LLMs accessible to smaller organizations and individual developers.\n",
      "Comparing LLMOps and MLOpsBoth LLMOps (Large Language Model Operations) and MLOps (Machine Learning Operations) focus on streamlining the lifecycle management of machine learning models, but they cater to different needs. While MLOps provides a general framework for managing all types of ML models, LLMOps extends these principles to address the unique challenges posed by large language models (LLMs).Here’s a comparison of the key similarities and differences between the two practices:MLOpsLLMOpsModel TypeGeneral ML models (regression, classification).Large Language Models (e.g., GPT, BERT) with billions of parameters.ScaleWorks with models of varying sizes.Focused on extremely large models requiring massive resources.Fine-TuningFine-tuning is task-specific but less resource-intensive.Fine-tuning often requires domain-specific datasets and significant computational power.Training NeedsCan involve smaller datasets and fewer iterations.Relies on vast datasets and distributed training systems.Cost ManagementCosts are moderate and manageable for most models.Costs can skyrocket due to computational demands and resource usage.Specialized ToolsGeneral ML tools like TensorFlow, Scikit-learn.Frameworks like Hugging Face, LangChain, and OpenAI APIs.\n",
      "ConclusionLLMOps is an essential extension of MLOps, tailored specifically to manage the unique challenges of large language models. From efficient training and fine-tuning to scalable deployment and continuous monitoring, LLMOps provides the tools and practices needed to unlock the full potential of LLMs while addressing their complexity.This article highlights how LLMOps simplifies working with LLMs, ensuring better performance, scalability, and ethical use. By understanding and implementing LLMOps, you can effectively leverage large language models for real-world applications like chatbots, content generation, and sentiment analysis.How This Benefits You:For Beginners: Learn how to manage and deploy LLMs efficiently.For Practitioners: Gain insights into tools and best practices for handling large-scale models.For Businesses: Understand how to integrate LLMs into your workflows to drive innovation and efficiency.With LLMOps, you can confidently manage the lifecycle of large language models, ensuring they deliver maximum value in a cost-effective and ethical way.\n",
      "Schedule a call nowStart your offshore web & mobile app team with a free consultation from our solutions engineer.InternationalAfghanistanÅland IslandsAlbaniaAlgeriaAmerican SamoaAndorraAngolaAnguillaAntigua and BarbudaArgentinaArmeniaArubaAscension IslandAustraliaAustriaAzerbaijanBahamasBahrainBangladeshBarbadosBelarusBelgiumBelizeBeninBermudaBhutanBoliviaBonaire, Sint Eustatius and SabaBosnia and HerzegovinaBotswanaBrazilBritish Indian Ocean TerritoryBrunei DarussalamBulgariaBurkina FasoBurundiCambodiaCameroonCanadaCape VerdeCayman IslandsCentral African RepublicChadChileChinaChristmas IslandCocos (Keeling) IslandsColombiaComorosCongoCongo, Democratic Republic of theCook IslandsCosta RicaCote d'IvoireCroatiaCubaCuraçaoCyprusCzech RepublicDenmarkDjiboutiDominicaDominican RepublicEcuadorEgyptEl SalvadorEquatorial GuineaEritreaEstoniaEthiopiaFalkland IslandsFaroe IslandsFederated States of MicronesiaFijiFinlandFranceFrench GuianaFrench PolynesiaGabonGambiaGeorgiaGermanyGhanaGibraltarGreeceGreenlandGrenadaGuadeloupeGuamGuatemalaGuernseyGuineaGuinea-BissauGuyanaHaitiHoly See (Vatican City State)HondurasHong KongHungaryIcelandIndiaIndonesiaIranIraqIrelandIsle of ManIsraelItalyJamaicaJapanJerseyJordanKazakhstanKenyaKiribatiKosovoKuwaitKyrgyzstanLaosLatviaLebanonLesothoLiberiaLibyaLiechtensteinLithuaniaLuxembourgMacaoMadagascarMalawiMalaysiaMaldivesMaliMaltaMarshall IslandsMartiniqueMauritaniaMauritiusMayotteMexicoMoldovaMonacoMongoliaMontenegroMontserratMoroccoMozambiqueMyanmarNamibiaNauruNepalNetherlandsNew CaledoniaNew ZealandNicaraguaNigerNigeriaNiueNorfolk IslandNorth KoreaNorth MacedoniaNorthern Mariana IslandsNorwayOmanPakistanPalauPalestinePanamaPapua New GuineaParaguayPeruPhilippinesPolandPortugalPuerto RicoQatarReunionRomaniaRussiaRwandaSaint BarthélemySaint HelenaSaint Kitts and NevisSaint LuciaSaint Martin (French Part)Saint Pierre and MiquelonSaint Vincent and the GrenadinesSamoaSan MarinoSao Tome and PrincipeSaudi ArabiaSenegalSerbiaSeychellesSierra LeoneSingaporeSint MaartenSlovakiaSloveniaSolomon IslandsSomaliaSouth AfricaSouth KoreaSouth SudanSpainSri LankaSudanSurinameSvalbard and Jan MayenSwazilandSwedenSwitzerlandSyriaTaiwanTajikistanTanzaniaThailandTimor-LesteTogoTokelauTongaTrinidad and TobagoTristan da CunhaTunisiaTurkeyTurkmenistanTurks and Caicos IslandsTuvaluUgandaUkraineUnited Arab EmiratesUnited KingdomUnited StatesUruguayUzbekistanVanuatuVenezuelaVietnamVirgin Islands, BritishVirgin Islands, U.S.Wallis and FutunaWestern SaharaYemenZambiaZimbabweInternationalLet's BeginWe respect your privacy, and be assured that your data will not be sharedFrom innovative startups to multinational corporations, we deliver tailored software development solutions that drive success across industries. Let CodeB be your partner in building cutting-edge technology for the futureServicesMobile app developmentWeb app developmentFront end developmentBackend developmentBlogsHow to set up an Offshore Development CenterSetup your offshore team in IndiaOffshore Mobile App developmentMobile App development cost (India)Web development Cost (India)Find UsBunglow No. 23, MHADA Colony, Mulund East, Mumbai, Maharashtra 400081, India India: +91 (913)-759-5718Email: manager@code-b.devWrite for usCopyright © 2024 by Code B Solutions Pvt Ltd. All Rights Reserved.WhatsAppCall UsMail Us\n",
      "LLMOps - Mastering Large Language Model Operations | SigNozSigNozProductDocsResourcesPricingCustomer StoriesSearch.../Open main menuSearch.../Sign InGet Started - FreeBack to GuidesmonitoringAugust 14, 2024•17 min readLLMOps - Mastering Large Language Model OperationsAuthor:Sushant GauravLLMOps or Large Language Model Operations, is transforming how businesses maintain and deploy advanced AI models. This developing field addresses the special issues associated with large language models (LLMs), expanding existing MLOps (Machine Learning Operations) approaches to meet the specific needs of AI development. LLMOps covers the complete lifespan of LLMs, from data preparation and model training to deployment and ongoing monitoring.LLMOps overview image What is LLMOps? Understanding the BasicsLLMOps refers to the operational procedures for managing large language models. It builds on the foundation of MLOps (Machine Learning Operations) but focuses on the special requirements of AI systems driven by LLMs.Let us look at some of its main components:Data PreparationData Preparation includes choosing high-quality datasets for training and fine-tuning LLM models. Data quality is essential because it has a direct impact on model performance. Data preparation involves data collection, cleaning, augmentation, and diversity to prevent biases. For example, a diverse set of language styles, themes, and dialects is required in natural language processing to build a strong language model.Note:Data cleaning is a critical step in data preparation that involves identifying and correcting errors or inconsistencies in the dataset.Data augmentation involves creating new training examples from the existing dataset to improve the robustness and generalizability of machine learning models.Data diversity refers to the inclusion of a wide range of data types, styles, and contexts in the training dataset to ensure the model can generalize well across different scenarios.Model TrainingOptimizing the training process for large-scale language models is a difficult task that requires a huge amount of computational resources and expertise. This includes choosing proper algorithms, modifying hyperparameters, and utilizing distributed computing frameworks to deal with the massive amounts of data and parameters involved. Techniques such as transfer learning and fine-tuning pre-trained models can also be used to boost efficiency and performance.DeploymentEfficiently integrating LLMs into production contexts involves more than simply using a trained model. It requires a well-planned deployment pipeline that guarantees scalability, dependability, and simplicity of interface with current systems. To handle a large deployment, deployment techniques may involve employing containerization technologies such as Docker, orchestration platforms such as Kubernetes, and serverless architectures.MonitoringContinuously monitoring model performance and spotting faults is essential for ensuring the reliability and usefulness of LLMs in production. Monitoring involves tracking multiple performance parameters, detecting data drift, and verifying that model predictions are consistent over time. This can be accomplished via automated monitoring systems that provide real-time insights into model performance and notify teams of any irregularities.LLMOps play a crucial role in the AI development lifecycle, ensuring that language models are not only powerful but also reliable, scalable, and ethically sound. By implementing robust LLMOps practices, organizations can better manage the complexities associated with large language models and ensure their AI initiatives deliver value effectively and responsibly.The Evolution from MLOps to LLMOpsWhile MLOps and LLMOps share common goals, LLMOps addresses challenges that are related to large language models (LLMs). Let us look at some of these challenges and how LLMOps tackles them:Scale: LLMs often require massive computational resources for training and inference. Training an LLM can involve processing terabytes of data and utilizing thousands of GPUs over extended periods. This requires complex infrastructure, efficient resource management, and the ability to scale operations seamlessly. LLMOps practices include using distributed training frameworks, optimizing resource utilization, and leveraging cloud-based solutions to handle the scale.Complexity: Managing complex model architectures and large parameter spaces is a big problem for LLMs. These models can include billions of parameters, necessitating sophisticated strategies for model optimization, parameter tuning, and performance monitoring. LLMOps uses specific tools and frameworks built to manage this complexity, such as TensorFlow, PyTorch, and Hugging Face's Transformers library.Ethical Considerations: When working with LLMs, it is vital to minimize biases and ensure responsible AI use. These models may unintentionally learn and perpetuate biases found in the training data, resulting in ethical and societal difficulties. LLMOps highlights the need for ethical AI approaches, such as bias identification, fairness evaluation, and the establishment of rules for responsible AI use. Ethical LLMOps procedures include regular audits and transparency reports.Specialized Tools: Using frameworks built for language model creation is necessary for effective LLMOps. Hugging Face's Transformers, OpenAI's GPT and others offer pre-trained models, fine-tuning capabilities, and other features designed specifically for LLM development. These technologies make it easier to create, train, and deploy big language models.We can summarize the comparision as:AspectMLOpsLLMOpsScaleManages computational resources for standard ML models.Handles massive computational resources for LLMs, involving terabytes of data and thousands of GPUs.ComplexityFocuses on optimizing standard model architectures and parameter tuning.Manages complex model architectures with billions of parameters, requiring specialized optimization.Ethical ConsiderationsAddresses biases and fairness in ML models.Emphasizes minimizing biases in LLMs and ensuring responsible AI use through regular audits.Specialized ToolsUtilizes general ML frameworks like TensorFlow and PyTorch.Leverages tools like Hugging Face's Transformers and OpenAI's GPT for specialized LLM development.LLMOps highlights the importance of effective governance and compliance management, particularly in issues such as data protection and model explainability. This focus derives from greater scrutiny of AI systems and their potential societal consequences. Implementing LLMOps entails creating clear data handling norms and protocols, assuring model interpretability and transparency, and adhering to legislation like as GDPR and CCPA. GDPR (General Data Protection Regulation) and CCPA (California Consumer Privacy Act) are both data privacy regulations, but they apply in different regions and have some distinct features.Why LLMOps Matters: Benefits and Use CasesImplementing LLMOps provides various benefits to companies working with large language models, which can have a substantial impact on their AI initiatives and overall business results. Let us now take a look at the advantages and real-world applications:Improved Model Performance: Systematic fine-tuning and optimization of LLMs produces superior results. Organizations may increase the accuracy and relevance of their AI applications by regularly monitoring and refining the models. To achieve optimal performance approaches such as hyperparameter adjustments, gradient accumulation, and regular retraining with updated data sets are used.Example: In a customer service chatbot, regular fine-tuning based on user interactions helps the bot understand and respond more accurately to customer queries, enhancing the overall customer experience.Enhanced Scalability: LLMOps methods enable the efficient deployment of LLMs across several applications. Organizations may deploy LLMs at scale by leveraging containerization, orchestration tools such as Kubernetes, and scalable cloud infrastructure, guaranteeing they can manage large numbers of requests while providing constant performance.Example: In e-commerce, using LLMs to power personalized recommendation systems assures that the system can scale to handle millions of users while providing real-time suggestions.Better Governance: Compliance with AI legislation and ethical principles is easier to manage using LLMOps. This includes putting in place data privacy controls, making sure model decisions are transparent, and assessing models regularly for biases and ethical considerations.Example: In healthcare, employing LLMOps to comply with HIPAA rules ensures that patient data is managed securely and that models used to make diagnosis or treatment recommendations are transparent and objective.Faster Time-to-Market: LLMOps streamline procedures, accelerating the development of AI-powered solutions. Organizations can shorten the time it takes to transition from model development to production by automating workflows, defining best practices, and using continuous integration and deployment (CI/CD) pipelines.Example: In content production, utilizing LLMOps enables quick development and deployment of AI models that produce and summarize material for marketing campaigns, significantly reducing the time required to launch new content initiatives.Real-World Applications of LLMOpsLLMOps allow for the effective deployment of language models in numerous areas, revolutionizing how enterprises use AI to improve their operations and services:Customer Service: You can enable intelligent chatbots and virtual assistants to understand and respond to consumer requests in real-time which can increase customer happiness while lowering operational costs.Content Creation: Writing and summarizing text for marketing or publishing, allowing content teams to create high-quality content quickly and efficiently.Language Services: Improving translation and localization skills, enabling firms to operate in various languages and reach a worldwide audience.E-commerce: Creating tailored product recommendation systems, enhancing user experience, and increasing revenues by recommending relevant products based on user behaviour.How to Implement LLMOps: Best Practices and StrategiesImplementing LLMOps efficiently necessitates a systematic approach and adherence to best practices. Let us look at some of the crucial strategies for success:Establish a Robust Data Pipeline:Data Collection: Continuously collect data from many sources relevant to your use case. Ensure that the data is high-quality, accurate, and up-to-date. You can use numerous options for pipelines such as Azure DevOps Pipeline and Jenkins for automating data collection processes.Data Cleaning: Use automated methods to handle missing values, eliminate duplicates, and fix errors.Data Preprocessing: Normalize, tokenize, and encode data as needed for your model. For NLP tasks, text preprocessing techniques like stopword removal and stemming may be used.Example: You can use Pandas to preprocess data in a text-based dataset. For instance, using the pandas library in Python, you can remove null values and duplicates with commands like df.dropna() and df.drop_duplicates(). Additionally, Azure DevOps Pipeline can be set up to automate the entire data preprocessing workflow, ensuring data is consistently prepared for model training.Implement Version Control:Tools Git LFS is used to manage huge files, while DVC is used to version data and models.Tracking changes: Ensure that any changes to data, code, and model parameters are tracked (refer to example below). This helps with reproducibility and understanding of model evolution.Example: You can use DVC to track data and model versions. For example, setting up DVC in a project involves initializing DVC with dvc init, then adding data files with dvc add <file>. These files can be pushed to remote storage, ensuring all data changes are versioned alongside code in Git.Automate Testing and Evaluation:Testing Frameworks: Develop through test cases using frameworks like pytest to evaluate model performance, fairness, and safety.Continuous Integration: Integrate automated tests into your CI/CD pipeline to ensure that models meet performance standards before deployment. It is crucial to differentiate between unit testing and integration testing. Integration testing ensures different components of your system work together, while unit testing focuses on individual components.Example: You can write a simple pytest function to test model accuracy.def test_model_accuracy():\n",
      "    model = load_model('path_to_model')\n",
      "    test_data = load_data('path_to_test_data')\n",
      "    accuracy = evaluate_model(model, test_data)\n",
      "    assert accuracy > 0.85\n",
      "Deploy a Monitoring System:Platforms: Use platforms like SigNoz for real-time monitoring of model performance, drift detection, and alerting.Custom Metrics: Define and track custom metrics specific to your model's performance and operational health.Example: You can set up SigNoz for monitoring. Integrating SigNoz involves setting up the platform to collect metrics from your application. For instance, you can configure SigNoz to monitor your LLM's API endpoints, tracking metrics such as response time and error rates. Custom metrics can include specific indicators like model prediction confidence levels or data processing throughput, tailored to reflect the unique performance aspects of your language model. For more details, click here.Key Stages in the LLMOps LifecycleThe LLMOps lifecycle consists of several critical stages to ensure effective model management and deployment:Data Preparation: Ensure diversity and address biases in the administration of high-quality datasets. This includes gathering, sanitizing, and preparing data.Model Selection and Fine-Tuning: Select suitable pre-trained models and develop them further to meet particular needs. Hyperparameter tweaking, transfer learning, and iteration based on performance measurements are some methods for fine-tuning.Deployment and Integration: Integrate with current systems to serve models in production environments with efficiency. This entails containerization using deployment frameworks such as Docker, or TensorFlow Serving.Continuous Monitoring and Retraining: To preserve accuracy, regularly assess model performance and retrain as necessary. The use of broad monitoring and alerting systems ensures timely detection of performance degradation.LLMOps lifecycle overview Overcoming Challenges in LLMOps ImplementationThere are certain challenges involved in implementing LLMOps, but these can be successfully handled with the appropriate tactics and resources:High Computational Requirements:Scalable Infrastructure: To train and operate large language models, invest in high-performance processing resources, such as GPUs and TPUs.Cloud-Based Solutions: Make use of cloud computing platforms like as AWS, Azure, and Google Cloud, which provide services specifically tailored for AI workloads and scalable infrastructure.Example: Using SageMaker and EC2 to set up an AWS training environment. AWS SageMaker offers fully managed services for training and deploying machine learning models at scale. By setting up training jobs on EC2 instances with powerful GPUs, you can utilize SageMaker's distributed training capabilities to handle large datasets and complex models efficiently. This setup enables automatic scaling, spot instance usage for cost efficiency, and integration with other AWS services for a seamless workflow.Complex Versioning:Experiment Tracking: Make sure all versions of experiments, models, and datasets are documented by using tools like MLflow or Weights & Biases.Version Control Systems: Use Git LFS or DVC to version big model files and datasets.Example: Using MLflow to track experiments. MLflow provides a centralized repository to track experiments, recording parameters, metrics, artifacts, and models. When you are training a large language model, you can use MLflow as MLflow logs each experiment's configurations and outcomes which enables reproducibility and easy comparison of different model versions. This systematic tracking helps in fine-tuning models and ensures that the best-performing model configurations are identified and can be reproduced reliably.Data Privacy and Security:Data Protection Measures: Encrypt sensitive data and use access controls and anonymization methods to keep it safe over the lifecycle of the LLM.Accordance: Make sure that data privacy laws such as the CCPA and GDPR are followed.Example: Data encryption before storage. Implementing data encryption before storage involves using encryption algorithms to convert sensitive data into unreadable formats, ensuring that unauthorized users cannot access the original information. For instance, you can use AWS Key Management Service (KMS) to encrypt data stored in S3 buckets provides an additional layer of security.Ethical Considerations:Bias Mitigation: Put frameworks and instruments in place to evaluate and lessen biases in models and training data. To ensure impartiality, evaluate and update datasets regularly.Responsible AI: Create guidelines and procedures that support openness, responsibility, and moral application of AI.Example: Model fairness evaluation with Fairlearn. Fairlearn is an open-source toolkit that helps assess and remove unfairness in machine learning models. By integrating Fairlearn into your workflow, you can analyze your model's predictions across different demographic groups to identify potential biases. For example, after training a language model, you can use Fairlearn to evaluate its performance on subsets of data representing different genders or ethnicities. The toolkit provides metrics and visualizations to understand disparities and offers algorithms to adjust the model, promoting fairness and reducing bias.Monitoring and Observability in LLMOps with SigNozSustaining the performance and dependability of LLMs in production requires effective monitoring. SigNoz, an open-source observability platform, provides AI and ML systems with strong characteristics like:Distributed Tracing: In applications driven by LLM, monitor requests throughout microservices. This aids in comprehending the request flow and locating bottlenecks.Custom Dashboards: Showcase important indicators related to the effectiveness of language models. Dashboards shed light on the behaviour of models and the state of operations.Alerting: Configure alerts to be notified proactively when there may be problems or a decline in performance. Alerts guarantee that any irregularities are dealt with right away.Full Stack Observability: Gain comprehensive visibility across the entire stack, from infrastructure to applications and LLMs. This holistic view helps in identifying and resolving issues at any layer, ensuring seamless operations.Setting Up SigNoz for LLM MonitoringSigNoz cloud is the easiest way to run SigNoz. Sign up for a free account and get 30 days of unlimited access to all features.You can also install and self-host SigNoz yourself since it is open-source. With 20,000+ GitHub stars, open-source SigNoz is loved by developers. Find the instructions to self-host SigNoz.To configure SigNoz for LLM monitoring:Define Custom Metrics: Keep an eye on LLM-specific metrics such as error rates, token usage, and inference time.Build Dashboards: Construct visual aids that offer information about resource usage and model performance.Set Up Alerts: Establish guidelines to quickly alert teams to abnormalities or performance problems.Through the resolution of these issues and the utilization of monitoring instruments such as SigNoz, companies may guarantee that their lifecycle managers (LLMs) function effectively and dependably, yielding the intended results in real-world settings.Interested in doing LLM monitoring with SigNoz? Refer SigNoz’s official doc on LLM Monitoring for complete guide.Future Trends in LLMOpsThe need for increasingly advanced AI systems and technical developments is driving the ongoing evolution of the LLMOps area. Among the major new developments in LLMOps are the following:More Automation:Tools Powered by AI: utilizing AI to automate several model deployment, management, and optimization processes. This covers maintenance duties, deployment pipelines, and automated hyperparameter tweaking.Example: Automating model tuning and selection with AutoML frameworks.Community-Based Education:Confidentiality-Boosting Methods: By using federated learning, localized data may be used to train models across dispersed devices or servers. This method lowers the chance of data breaches while improving privacy.Example: Simple Flower-based federated learning configuration.Specialized Hardware:AI-specific Chips: Development and application of specialized hardware to boost the effectiveness and performance of huge language models, such as TPUs and AI accelerators. These processors are made to manage the computing demands of AI workloads more efficiently.Example: Making effective use of TPUs in TensorFlow to train models.Industry-Specific Platforms:Tailored LLMOps Solutions: Creating tools and platforms that are suited to particular industries, like the legal, finance, and healthcare fields. These technologies enable more efficient language model deployment and management by addressing the particular needs and legal restrictions of each business.Example: Setting up a healthcare data LLMOps pipeline tailored to the sector.Key TakeawaysLLMOps is essential for efficiently managing the lifecycle of large language models, assuring their dependability and performance.LLMOps implementation results in faster development cycles, improved model performance, and scalability.Creating strong data pipelines, putting version control in place, automating testing, and thorough monitoring are important best practices.Addressing complicated problems, high computational requirements, data protection, and ethical issues are necessary to overcome LLMOps obstacles.Tools such as SigNoz are essential to guarantee the effectiveness and dependability of LLMs in production settings.FAQsWhat's the difference between LLMOps and traditional MLOps?LLMOps focuses specifically on the unique challenges of large language models, such as massive computational requirements and complex ethical considerations. Traditional MLOps covers a broader range of machine learning models and may not address LLM-specific issues.How does LLMOps help in managing ethical concerns with AI?LLMOps incorporates processes for bias detection, fairness assessment, and transparency in model decisions. It emphasizes responsible AI development practices throughout the lifecycle of language models.What are the essential tools needed to get started with LLMOps?Key tools include version control systems (e.g., Git LFS), experiment tracking platforms (e.g., MLflow), containerization technologies (e.g., Docker), and monitoring solutions (e.g., SigNoz).How often should LLMs be retrained or fine-tuned in an LLMOps framework?The frequency of retraining depends on factors like data drift, task complexity, and performance requirements. Generally, continuous monitoring helps determine when retraining is necessary, which could be weekly, monthly, or quarterly.Was this page helpful?👍 Yes👎 NoOn this pageWhat is LLMOps? Understanding the BasicsData PreparationModel TrainingDeploymentMonitoringThe Evolution from MLOps to LLMOpsWhy LLMOps Matters: Benefits and Use CasesReal-World Applications of LLMOpsHow to Implement LLMOps: Best Practices and StrategiesKey Stages in the LLMOps LifecycleOvercoming Challenges in LLMOps ImplementationMonitoring and Observability in LLMOps with SigNozSetting Up SigNoz for LLM MonitoringFuture Trends in LLMOpsKey TakeawaysFAQsWhat's the difference between LLMOps and traditional MLOps?How does LLMOps help in managing ethical concerns with AI?What are the essential tools needed to get started with LLMOps?How often should LLMs be retrained or fine-tuned in an LLMOps framework?AuthorSushant GauravRelated ArticlesUnderstanding LLM Observability - Key Insights, Best Practices, & ToolsSeptember 12, 2024A Comprehensive Guide to Model Monitoring in ML ProductionAugust 7, 2024DocsIntroductionContributingMigrate from DatadogSigNoz APICommunitySupportSlackXLaunch WeekChangelogDashboard TemplatesDevOps WordleNewsletterMoreSigNoz vs DatadogSigNoz vs New RelicSigNoz vs GrafanaSigNoz vs DynatraceCareersAboutTermsPrivacySecurity & ComplianceSigNozAll systems operational\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLMOps For Machine Learning Engineering- Analytics Vidhya\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Master Generative AI with 10+ Real-world Projects in 2025!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "d\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "h\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "m\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        Download Projects\n",
      "                                            \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Free Courses\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Learning Paths\n",
      "\n",
      "\n",
      "\n",
      "GenAI Pinnacle Plus\n",
      "New\n",
      "\n",
      "\n",
      "Agentic AI Pioneer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DHS 2025\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Login\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                Switch Mode\n",
      "                            \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                Logout\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Interview PrepCareerGenAIPrompt EnggChatGPTLLMLangchainRAGAI AgentsMachine LearningDeep LearningGenAI ToolsLLMOpsPythonNLPSQLAIML Projects \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reading list\n",
      "\n",
      "                Introduction to Generative AIWhat is Generative AI?\n",
      "\n",
      "                Introduction to Generative AI applicationsOverview of generative AI applications and their impact\n",
      "\n",
      "                No-code Generative AI app developmentIntroduction to No-code AI Development\n",
      "\n",
      "                Code-focused Generative AI App DevelopmentIntroduction to LangChain, ChatGPT and Gemini Pro\n",
      "\n",
      "                Introduction to Responsible AIIntroduction to Responsible AI\n",
      "\n",
      "                LLMSWhat are Large Language Models?GPT modelsMistralLlamaGeminiHow to build diffferent LLM AppIications?\n",
      "\n",
      "                Prompt EngineeringIntroduction to Prompt EngineeringBest Practices and Guidelines for Prompt EngineeringN shot promptingChain of ThoughtTree of ThoughtsSkeleton of ThoughtsChain of Emotion\n",
      "\n",
      "                Finetuning LLMsIntroduction to Finetuning LLMsParameter-Efficient Finetuning (PEFT)LORAQLORAusing Unslothusing Huggingface\n",
      "\n",
      "                Training LLMs from ScratchWhat do you mean by Training LLMs from Scratch?\n",
      "\n",
      "                LangchainIntro to the LangChain EcosystemCore Components of LangChainApplications of LCEL ChainsRAG using LangChainLangGraphLangSmith\n",
      "\n",
      "                RAGIntroduction to RAG systemsEvaluation of RAG systems\n",
      "\n",
      "                LlamaIndexGetting Started with LlamaIndexComponents of LlamaIndexAdvanced approaches for powerful RAG system\n",
      "\n",
      "                Stable DiffusionIntroduction to Stable DiffusionGenerating image using Stable diffusionDiffusion modelsPrompt Engineering Concepts for Stable DiffusionMidJourneyUnderstanding Dalle 3  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Home\n",
      "\n",
      "LLMs                            \n",
      "\n",
      "                                A Beginners Guide to LLMOps For Machine Learning Engineering                                 \n",
      "\n",
      "\n",
      "\n",
      "A Beginners Guide to LLMOps For Machine Learning Engineering \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                            Sai Battula                             \n",
      " Last Updated : \n",
      "                                17 Feb, 2025 \n",
      "                            \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  12  min read\n",
      "                        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The release of OpenAI’s ChatGPT has inspired a lot of interest in large language models (LLMs), and everyone is now talking about artificial intelligence. But it’s not just friendly conversations; the machine learning (ML) community has introduced a new term called LLMOps for Machine learning engineering. We have all heard of MLOps, but what is LLMOps? Well, it’s all about how we treat and manage these powerful language models throughout their lifecycle.\n",
      "LLMs are converting the way we create and maintain AI-driven products, and this shift is leading to the need for new tools and best practices. In this article, we’ll melt down LLMOps and its background. We’ll also examine how building AI products with LLMs differs from traditional ML models. Plus, we’ll look at how MLOps (Machine Learning Operations) differs from LLMOps due to these differences. Finally, we’ll discuss what exciting developments we can expect in the world of LLMOps space shortly.\n",
      "Learning Objectives:\n",
      "\n",
      "Gain a sound understanding of LLMOps and its development.\n",
      "Learn to build a model using LLMOps through examples.\n",
      "Know the differences between LLMOps and MLOps.\n",
      "Get a sneak peek into the future of LLMOps.\n",
      "\n",
      "This article was published as a part of the Data Science Blogathon\n",
      "Table of ContentsWhat is LLMOps?What’s the Hype around LLMOps?What are the Steps Involved in LLMOps?Step 1: Selection of a Base ModelStep 2: Adapting to the Following TasksStep 3: Model EvaluationStep 4: Deployment and MonitoringHow is LLMOps Different from MLOps?Data ManagementExperimentationEvaluationCostSpeedThe Future of LLMOpsConclusionFrequently Asked Questions\n",
      "What is LLMOps?\n",
      "LLMOps stands for Large Language Model Operations, similar to MLOps but specifically designed for Large Language Models (LLMs). It requires using new tools and best practices to handle everything related to LLM-powered applications, from development to deployment and continuing maintenance.\n",
      "To understand this better, let’s break down what LLMs and MLOps mean:\n",
      "\n",
      "LLMs are large language models that can generate human languages. They have billions of parameters and are trained on billions of text data.\n",
      "MLOps (Machine Learning Operations) is a set of tools and practices used to manage the lifecycle of applications powered by machine learning.\n",
      "\n",
      "Now that we’ve explained the basics, let’s dive into this topic more deeply.\n",
      "What’s the Hype around LLMOps?\n",
      "Firstly, LLMs like BERT and GPT-2 have been around since 2018. Yet, it is now,   almost five years later, that we are encountering a flashing rise of the idea of LLMOps. The main reason is that LLMs obtained much media attention with the release of ChatGPT in December 2022.\n",
      "\n",
      "\n",
      "Since then, we have seen many different types of applications exploiting the power of LLMs. This includes chatbots ranging from familiar examples like ChatGPT, to more personal writing assistants for editing or summarization (e.g., Notion AI) and skilled ones for copywriting (e.g., Jasper and copy.ai). It also includes programming assistants for writing and debugging code (e.g., GitHub Copilot), testing the code (e.g., Codium AI), and identifying security trouble (e.g., Socket AI).\n",
      "With many people developing and carrying LLM-powered applications to production, people are contributing their experiences.\n",
      "\n",
      "It’s easy to make something cool with LLMs, but very hard to make something production-ready with them.  –   Chip Huyen\n",
      "\n",
      "It is clear that building production-ready LLM-powered applications comes with its own set of difficulties, distinct from building AI products with classical ML models. We must develop new tools and best practices to deal with these challenges to govern the LLM application lifecycle. Thus, we see an expanded use of the term “LLMOps.”\n",
      "What are the Steps Involved in LLMOps?\n",
      "The steps involved in LLMOps are at least similar to MLOps. However, the steps of building an LLM-powered application are different due to the beginning of the foundation models. Instead of training LLMs from scratch, the focus lies on domesticating pre-trained LLMs to the following tasks.\n",
      "Already over a year ago, Andrej Karpathy told how the process of building AI products will change in the future:\n",
      "“But the most important trend is that the whole setting of training a neural network from scratch on some target task is quickly becoming outdated due to finetuning, especially with the emergence of base models like GPT. These base models are trained by only a few institutions with substantial computing resources, and most applications are achieved via lightweight finetuning of part of the network, prompt engineering, or an optional step of data or model processing into smaller, special-purpose inference networks.”  -  Andrej Karpathy.\n",
      "This quote may be stunning the first time you read it. But it exactly summarizes everything that has been going on lately, so let’s describe it step by step in the following subsections.\n",
      "Step 1: Selection of a Base Model\n",
      "Foundation models or base models are LLMs pre-trained on large amounts of data that can be used for a wide range of tasks. Because training a base model from scratch is difficult, time-consuming, and extremely expensive, only a few institutions have the required training resources.\n",
      "To put it into perspective, according to a study from Lambda Labs in 2020, training OpenAI’s GPT-3 (with 175 billion parameters) would require 355 years and $4.6 million using a Tesla V100 cloud instance.\n",
      "AI is currently going through what the community calls its “Linux Moment.” Currently, developers have to choose between two types of base models based on an exchange between performance, cost, ease of use, and flexibility of proprietary models or open-source models.\n",
      "\n",
      "\n",
      "Exclusive or proprietary models are closed-source foundation models possessed by companies with large expert teams and big AI budgets. They usually are larger than open-source models and have better performance. They are also bought and generally rather easy to use. The main downside of proprietary models is their expensive APIs (application programming interfaces). Additionally, closed-source foundation models offer less or no elasticity for adaptation for developers.\n",
      "Examples of proprietary model providers are:\n",
      "\n",
      "OpenAI (GPT-3, GPT-4)\n",
      "co:here\n",
      "Anthropic (Claude)\n",
      "AI21 Labs (Jurassic-2)\n",
      "\n",
      "Open-source models are frequently organized and hosted on HuggingFace as a community hub. Usually, they are smaller models with lower capabilities than proprietary models. But on the upside, they are more economical than proprietary models and offer more flexibility for developers.\n",
      "Examples of open-source models are:\n",
      "\n",
      "Stable Diffusion by Stability AI\n",
      "BLOOM by BigScience\n",
      "LLaMA or OPT by Meta AI\n",
      "Flan-T5 by Google\n",
      "\n",
      "Code:\n",
      "This step involves importing all required libraries.\n",
      "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
      "\n",
      "# Can you load pre-trained GPT-3 model and tokenizer\n",
      "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
      "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
      "Output of the above code:\n",
      "\n",
      "\n",
      "Step 2: Adapting to the Following Tasks\n",
      "Once you have chosen your base model, you can access the LLM through its API. If you usually work with other APIs, working with LLM APIs will primarily feel a little weird because it is not always clear what input will cause what output earlier. Given any text prompt, the API will return a text completion, attempting to match your pattern.\n",
      "Here is an example of how you would use the OpenAI API. You give the API input as a prompt, e.g., prompt = “Correct this to standard English:\\n\\nHe no went to the market.”\n",
      "import openai\n",
      "openai.api_key = ...\n",
      "response = openai.Completion.create(\n",
      "\tengine = \"text-davinci-003\",\n",
      "\tprompt = \"Correct this to standard English:\\n\\nHe no went to the market.\",\n",
      "\t# ...\n",
      "\t)\n",
      "The API will output a reply containing the completion response[‘choices’][0][‘text’] = “He did not go to the market.”\n",
      "The main challenge is that LLMs aren’t mighty despite being powerful, and thus, the key question is: How do you get an LLM to give the output you want?\n",
      "One concern respondents mentioned in the LLM in-production survey was model accuracy and hallucination. That means getting the output from the LLM API in your desired format might take some iterations, and also, LLMs can hallucinate if they don’t have the required specific knowledge. To deal with these concerns, you can adapt the base models to the following tasks in the following ways:\n",
      "\n",
      "Prompt engineering is a technique to improve the input so that the output matches your expectations. You can use different tricks to improve your prompt (see OpenAI Cookbook). One method is to provide some examples of the expected output format. This is similar to zero-shot learning or few-shot learning. Tools like LangChain or HoneyHive are already available to help you manage and version your prompt templates.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fine-tuning pre-trained models is a technique seen in ML. It can help improve your model’s performance and accuracy on your specific task. Although this will increase the training efforts, it can decrease the cost of inference. The cost of LLM APIs is dependent on input and output sequence length. Thus, decreasing the number of input tokens reduces API costs because you no longer have to give examples in the prompt.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "External Data: Base models often short contextual information (e.g., access to some specific documents) and can become outdated rapidly. For example, GPT-4 was trained on data until September 2021. Because LLMs can imagine things if they don’t have sufficient information, we need to be able to give them access to important external data.\n",
      "Embeddings:  A slightly more complex way is to extract information in the form of embeddings from LLM APIs (e.g., product descriptions) and build applications on top of them (e.g., search, comparison, recommendations).\n",
      "Alternatives: As this field is quickly evolving, there are many more applications of LLMs in AI products. Some examples are instruction tuning/prompt tuning and model refining.\n",
      "\n",
      "Code:\n",
      "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, Trainer, TrainingArguments\n",
      "\n",
      "# Load your dataset\n",
      "dataset = TextDataset(tokenizer=tokenizer, file_path=\"your_dataset.txt\")\n",
      "\n",
      "# Fine-tune the model\n",
      "training_args = TrainingArguments(\n",
      "    output_dir=\"./your_fine_tuned_model\",\n",
      "    overwrite_output_dir=True,\n",
      "    num_train_epochs=3,\n",
      "    per_device_train_batch_size=4,\n",
      ")\n",
      "\n",
      "trainer = Trainer(\n",
      "    model=model,\n",
      "    args=training_args,\n",
      "    data_collator=data_collator,\n",
      "    train_dataset=dataset,\n",
      ")\n",
      "\n",
      "trainer.train()\n",
      "trainer.save_model()\n",
      "Step 3: Model Evaluation\n",
      "In classical MLOps, ML models are demonstrated on a hold-out validation set with a metric that denotes the models’ performance. But how do you evaluate the execution of an LLM? How do you decide whether an output is good or bad? Currently, it seems like organizations are A/B testing their models.\n",
      "To help evaluate LLMs, tools like HoneyHive or HumanLoop have emerged.\n",
      "Code:\n",
      "from transformers import pipeline\n",
      "\n",
      "# Create a text generation pipeline\n",
      "generator = pipeline(\"text-generation\", model=\"your_fine_tuned_model\")\n",
      "\n",
      "# Generate text and evaluate\n",
      "generated_text = generator(\"Prompt text\")\n",
      "print(generated_text)\n",
      "Step 4: Deployment and Monitoring\n",
      "The achievement of LLMs can extremely change between releases. For example, OpenAI has updated its models to relieve inappropriate content generation, e.g., hate speech. As a result, scanning for the phrase “as an AI language model” on Twitter now reveals countless bots.\n",
      "There are already tools for monitoring LLMs appearing, such as Whylabs or HumanLoop.\n",
      "Code:\n",
      "# Import your necessary libraries\n",
      "from flask import Flask, request, jsonify\n",
      "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
      "import logging\n",
      "\n",
      "# Initialize Flask app\n",
      "app = Flask(__name__)\n",
      "\n",
      "# you can load the fine-tuned GPT-2 model and tokenizer\n",
      "model = GPT2LMHeadModel.from_pretrained(\"./your_fine_tuned_model\")\n",
      "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
      "\n",
      "# Set up logging\n",
      "logging.basicConfig(filename='app.log', level=logging.INFO)\n",
      "\n",
      "# Define a route for text generation\n",
      "@app.route('/generate_text', methods=['POST'])\n",
      "def generate_text():\n",
      "    try:\n",
      "        data = request.get_json()\n",
      "        prompt = data['prompt']\n",
      "\n",
      "        # Generate text\n",
      "        generated_text = model.generate(\n",
      "            tokenizer.encode(prompt, return_tensors='pt'),\n",
      "            max_length=100,  # Adjust max length as needed\n",
      "            num_return_sequences=1,\n",
      "            no_repeat_ngram_size=2,\n",
      "            top_k=50,\n",
      "            top_p=0.95,\n",
      "        )[0]\n",
      "\n",
      "        generated_text = tokenizer.decode(generated_text, skip_special_tokens=True)\n",
      "        \n",
      "        # Log the request and response\n",
      "        logging.info(f\"Generated text for prompt: {prompt}\")\n",
      "        logging.info(f\"Generated text: {generated_text}\")\n",
      "\n",
      "        return jsonify({'generated_text': generated_text})\n",
      "\n",
      "    except Exception as e:\n",
      "        # Log any exceptions\n",
      "        logging.error(f\"Error: {str(e)}\")\n",
      "        return jsonify({'error': 'An error occurred'}), 500\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run(host='0.0.0.0', port=5000)\n",
      "Working of Above code:\n",
      "\n",
      "Import the necessary libraries: This means importing the required libraries and modules. Flask is used to build web applications, transformers are used to carry and handle the GPT-2 model, and logging is used to record the information.\n",
      "Initialize the Flask app.\n",
      "Load the model: You can load the pretrained GPT-2 model and corresponding tokenizer. You can replace them ./your_fine_tuned_model with the path to your real fine-tuned GPT-2 model.\n",
      "Set up logging: This denotes logging into the application. It sets the log file name to app.log and sets the logging level to INFO.\n",
      "Set up a route sketch using Flask: It specifies that when a POST request is made to the /generate_text endpoint, the generate_text function should be called.\n",
      "Generating text: This code extracts JSON data from the incoming POST request. It assumes that the JSON data includes a “prompt” field, which is the text that will be used to generate additional text.\n",
      "Text generation using GPT-2: This section uses the loaded GPT-2 model and tokenizer to generate text based on the provided prompt. It sets different generation parameters, such as the generated text of the maximum length, the number of Series to generate, and the sampling parameters.\n",
      "Decoding and returning generated text: After generating the text, it decodes the generated series and removes special tokens. Then, it returns the generated text as a JSON response.\n",
      "Logging the request and response: It logs the request’s prompt and the generated text in the log file.\n",
      "Handling exceptions: If any exceptions occur during the text generation process, they are Captured and logged as errors. A JSON output with an error message is returned along with a 500 status code to denote a server error.\n",
      "Running the Flask app: It ensures that the Flask app is only run when the script is executed Straightway. It runs the app on host ‘0.0.0.0’ and port 5000, making it convenient from any IP address.\n",
      "\n",
      "Output of above code:\n",
      "Input prompt:\n",
      "#{\n",
      "\n",
      "    \"prompt\": \"Once upon a time\"\n",
      "\n",
      "}\n",
      "\n",
      "Output prompt:\n",
      "\n",
      "{\n",
      "\n",
      "    \"generated_text\": \"Once upon a time, in a faraway land, there lived a...\"\n",
      "\n",
      "}import csv\n",
      "How is LLMOps Different from MLOps?\n",
      "The differences between MLOps and LLMOps arise from the differences in how we build AI products with classical ML models versus LLMs. The differences mostly affect data management, experimentation, evaluation, cost, and latency.\n",
      "Data Management\n",
      "In standard MLOps, we are used to data-hungry ML models. Training a neural network from scratch needs a lot of labeled data, and even fine-tuning a pre-trained model involves at least a few hundred samples. However, data cleaning is essential to the ML development process, as we know and accept that large datasets have defects.\n",
      "In LLMOps, fine-tuning is similar to MLOps. But prompt engineering is a zero-shot or few-shot learning circumstance. That means we have few but hand-picked samples.\n",
      "Experimentation\n",
      "In MLOps, the investigation looks similar to whether you train a model from scratch or fine-tune a pre-trained one. In both cases, you will route inputs, such as model architecture, hyperparameters, and data augmentations, and outputs, such as metrics.\n",
      "But in the LLMOps, the question is whether to engineer prompts or to fine-tune. However, fine-tuning will look similar to MLOps in LLMOps, while prompt engineering involves a different experimentation setup involving the management of prompts.\n",
      "Evaluation\n",
      "In classical MLOps, a hold-out validation set with an evaluation metric evaluates a model’s performance. Because the performance of LLMs is more difficult to evaluate, currently, organizations seem to be using A/B testing.\n",
      "Cost\n",
      "While the cost of traditional MLOps usually lies in data collection and model training, the cost of LLMOps lies in inference. Although we can expect some costs from using expensive APIs during experimentation, Chip Huyen shows that the cost of long prompts is in inference.\n",
      "Speed\n",
      "Another concern respondents mentioned in the LLM in the production survey was latency. The completion length of an LLM significantly affects latency. Although latency concerns exist in MLOps as well, they are much more prominent in LLMOps because this is a big issue for the experimentation velocity during development and the user experience in production.\n",
      "The Future of LLMOps\n",
      "LLMOps is an upcoming field. With the speed at which this space is evolving, making any predictions is difficult. It is even doubtful if the term “LLMOps” is here to stay. We are only sure that we will see a lot of new use cases of LLMs and tools and the best trials to manage the LLM lifecycle.\n",
      "The field of AI is rapidly growing, potentially making anything we write now outdated in a month. We’re still in the early stages of transporting LLM-powered applications to production. There are many questions we don’t have the answers to, and only time will tell how things will play out:\n",
      "\n",
      "Is the term “LLMOps” here to stay?\n",
      "How will LLMOps in light of MLOps evolve? Will they Transform together, or will they become separate sets of operations?\n",
      "How will AI’s “Linux Moment” play out?\n",
      "\n",
      "We can say with certainty that we will see many developments and new toolings and best practices soon. Also, we are already looking at efforts being made toward cost and latency reduction for base models. These are definitely interesting times!\n",
      "Conclusion\n",
      "Since the release of OpenAI’s ChatGPT, LLMs have become a hot topic in the field of AI. These deep learning models can generate outputs in human language, making them a strong tool for tasks such as conversational AI, programming assistants, and writing assistants.\n",
      "However, carrying LLM-powered applications to production presents its own set of challenges, which has led to the arrival of a new term, “LLMOps”. It refers to the set of tools and best practices used to manage the lifecycle of LLM-powered applications, including development, deployment, and maintenance.\n",
      "LLMOps can be seen as a subcategory of MLOps. However, the steps involved in building an LLM-powered application are different from those in building applications with base ML models. Instead of training an LLM from scratch, the focus is on adapting pre-trained LLMs to the following tasks. This involves selecting a foundation model, using LLMs in the following tasks, evaluating them, and deploying and monitoring the model. While LLMOps is still a relatively new field, it is sure to continue to develop and evolve as LLMs become more popular in the AI industry.\n",
      "Key Takeaways:\n",
      "\n",
      "LLMOps (Large Language Model Operations) is a scientific field that focuses on managing the lifecycle of mighty language models like ChatGPT, transforming the creation and maintenance of AI-driven products.\n",
      "The increase in applications utilizing Large Language Models (LLMs) like GPT-3, GPT-3.5, and GPT-4 has led to the rise of LLMOps.\n",
      "The process of LLMOps includes selecting a base model, adapting it to particular tasks, evaluating model performance through A/B testing, and informing the cost and latency anxiety associated with LLM-powered applications.\n",
      "LLMOps are different from traditional MLOps in terms of data management (few-shot learning), examination (prompt engineering), evaluation (A/B testing), cost (inference-related costs), and speed (latency reflection).\n",
      "\n",
      "Overall, the rise of LLMs and LLMOps describes a significant shift in building and maintaining AI-powered products. I hope you liked this article. You can connect with me here on LinkedIn.\n",
      "Frequently Asked Questions\n",
      "Q1. What are large language models (LLMs)? Ans. Large language models (LLMs) are recent improvements in deep learning models to work on human languages. A large language model is a trained deep-learning model that understands and generates text in a human-like fashion. Behind the scenes, a large transformer model does all the magic.  Q2. What are the key steps in LLMOps? Ans. The key steps followed in LLMOps are:1. Select a pre-trained Large Language Model as the base for your application.2. Modify the LLM for particular tasks using techniques like prompt engineering and fine-tuning.3. Frequently estimate the LLM’s performance through A/B testing and tools like HoneyHive.4. Deploy the LLM-powered application, continuously monitor its performance, and streamline it.  \n",
      "The media shown in this article is not owned by Analytics Vidhya and is used at the Author’s discretion.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                    Sai Battula                                  \n",
      "\n",
      "\n",
      "Hi, I’m Sai Durga Prasad. I’m a Machine Learning Engineer with a deep passion for AI and emerging technologies. I love exploring how machine learning, deep learning, NLP, and generative AI can solve real-world problems. I enjoy building smart solutions using tools like Python, Flask, MySQL, and GitHub, and I also have a good sense of how products come together, thanks to my experience in product management. I'm always curious, always learning, and excited to be part of projects where technology meets real impact. I'm committed, adaptable, and ready to take on challenges that help me grow while contributing meaningfully to the industry.\n",
      "\n",
      "\n",
      "Artificial IntelligenceBeginnerBest of TechGuideLarge Language ModelsLLMOpsLLMsMachine LearningMLops \n",
      "\n",
      "\n",
      "Login to continue reading and enjoy expert-curated content.\n",
      "Keep Reading for Free\n",
      "\n",
      "\n",
      "\n",
      "Free Courses\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                            4.7\n",
      "                                        \n",
      "\n",
      "\n",
      "Generative AI - A Way of Life\n",
      "Explore Generative AI for beginners: create text and images, use top AI tools, learn practical skills, and ethics.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                            4.5\n",
      "                                        \n",
      "\n",
      "\n",
      "Getting Started with Large Language Models\n",
      "Master Large Language Models (LLMs) with this course, offering clear guidance in NLP and model training made simple.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                            4.6\n",
      "                                        \n",
      "\n",
      "\n",
      "Building LLM Applications using Prompt Engineering \n",
      "This free course guides you on building LLM apps, mastering prompt engineering, and developing chatbots with enterprise data.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                            4.8\n",
      "                                        \n",
      "\n",
      "\n",
      "Improving Real World RAG Systems: Key Challenges & Practical Solutions\n",
      "Explore practical solutions, advanced retrieval strategies, and agentic RAG systems to improve context, relevance, and accuracy in AI-driven applications.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                            4.7\n",
      "                                        \n",
      "\n",
      "\n",
      "Microsoft Excel: Formulas & Functions\n",
      "Master MS Excel for data analysis with key formulas, functions, and LookUp tools in this comprehensive course.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Recommended ArticlesHow to Build a Multilingual Chatbot using Large...What is LangChain?Essential Practices for Building Robust LLM Pip...How to Move Generative AI Applications to Produ...LLMOPS vs MLOPS: Choosing the Best Path for AI ...What are Large Language Models(LLMs)?Build Large Language Models from ScratchA Survey of Large Language Models (LLMs)From GPT-3 to Future Generations of Language Mo...7 Most Common Questions Around LLMs \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Responses From Readers\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                  Become an Author\n",
      "                  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Share insights, grow your voice, and inspire the data community.\n",
      "\n",
      "\n",
      "\n",
      "Reach a Global Audience\n",
      "Share Your Expertise with the World\n",
      "Build Your Brand & Audience\n",
      "\n",
      "\n",
      "Join a Thriving AI Community\n",
      "Level Up Your AI Game\n",
      "Expand Your Influence in Genrative AI\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Flagship Programs \n",
      "\n",
      "GenAI Pinnacle Program|\n",
      "                    \n",
      "GenAI Pinnacle Plus Program|\n",
      "                    AI/ML BlackBelt Program|\n",
      "                    Agentic AI Pioneer Program\n",
      "\n",
      "\n",
      "\n",
      "Free Courses\n",
      "\n",
      "Generative AI|\n",
      "                    DeepSeek|\n",
      "                    OpenAI Agent SDK|\n",
      "                    LLM Applications using Prompt Engineering|\n",
      "                    DeepSeek from Scratch|\n",
      "                    Stability.AI|\n",
      "                    SSM & MAMBA|\n",
      "                    RAG Systems using LlamaIndex|\n",
      "                    Building LLMs for Code|\n",
      "                    Python|\n",
      "                    Microsoft Excel|\n",
      "                    Machine Learning|\n",
      "                    Deep Learning|\n",
      "                    Mastering Multimodal RAG|\n",
      "                    Introduction to Transformer Model|\n",
      "                    Bagging & Boosting|\n",
      "                    Loan Prediction|\n",
      "                    Time Series Forecasting|\n",
      "                    Tableau|\n",
      "                    Business Analytics|\n",
      "                    Vibe Coding in Windsurf|\n",
      "                    Model Deployment using FastAPI|\n",
      "                    Building Data Analyst AI Agent|\n",
      "                    Getting started with OpenAI o3-mini|\n",
      "                    Introduction to Transformers and Attention Mechanisms\n",
      "\n",
      "\n",
      "\n",
      "Popular Categories\n",
      "\n",
      "AI Agents|\n",
      "                    Generative AI|\n",
      "                    Prompt Engineering|\n",
      "                    Generative AI Application|\n",
      "                    News|\n",
      "                    Technical Guides|\n",
      "                    AI Tools|\n",
      "                    Interview Preparation|\n",
      "                    Research Papers|\n",
      "                    Success Stories|\n",
      "                    Quiz|\n",
      "                    Use Cases|\n",
      "                    Listicles\n",
      "\n",
      "\n",
      "\n",
      "Generative AI Tools and Techniques\n",
      "\n",
      "GANs|\n",
      "                    VAEs|\n",
      "                    Transformers|\n",
      "                    StyleGAN|\n",
      "                    Pix2Pix|\n",
      "                    Autoencoders|\n",
      "                    GPT|\n",
      "                    BERT|\n",
      "                    Word2Vec|\n",
      "                    LSTM|\n",
      "                    Attention Mechanisms|\n",
      "                    Diffusion Models|\n",
      "                    LLMs|\n",
      "                    SLMs|\n",
      "                    \n",
      "Encoder Decoder Models|\n",
      "                    Prompt Engineering|\n",
      "                    LangChain|\n",
      "                    LlamaIndex|\n",
      "                    RAG|\n",
      "                    Fine-tuning|\n",
      "                    LangChain AI Agent|\n",
      "                    Multimodal Models|\n",
      "                    RNNs|\n",
      "                    DCGAN|\n",
      "                    ProGAN|\n",
      "                    Text-to-Image Models|\n",
      "                    DDPM|\n",
      "                    Document Question Answering|\n",
      "                    Imagen|\n",
      "                    T5 (Text-to-Text Transfer Transformer)|\n",
      "                    Seq2seq Models|\n",
      "                    WaveNet|\n",
      "                    Attention Is All You Need (Transformer Architecture) |\n",
      "                    WindSurf|\n",
      "                    Cursor\n",
      "\n",
      "\n",
      "\n",
      "Popular GenAI Models\n",
      "\n",
      "Llama 4|\n",
      "                    Llama 3.1|\n",
      "\n",
      "                    GPT 4.5|\n",
      "                    GPT 4.1|\n",
      "                    GPT 4o|\n",
      "                    o3-mini|\n",
      "                    Sora|\n",
      "                    DeepSeek R1|\n",
      "                    DeepSeek V3|\n",
      "                    Janus Pro|\n",
      "                    Veo 2|\n",
      "                    Gemini 2.5 Pro|\n",
      "                    Gemini 2.0|\n",
      "                    Gemma 3|\n",
      "                    Claude Sonnet 3.7|\n",
      "                    Claude 3.5 Sonnet|\n",
      "                    Phi 4|\n",
      "                    Phi 3.5|\n",
      "                    Mistral Small 3.1|\n",
      "                    Mistral NeMo|\n",
      "                    Mistral-7b|\n",
      "                    Bedrock|\n",
      "                    Vertex AI|\n",
      "                    Qwen QwQ 32B|\n",
      "                    Qwen 2|\n",
      "                    Qwen 2.5 VL|\n",
      "                    Qwen Chat|\n",
      "                    Grok 3\n",
      "\n",
      "\n",
      "\n",
      "AI Development Frameworks\n",
      "\n",
      "n8n|\n",
      "                    LangChain|\n",
      "                    Agent SDK|\n",
      "                    A2A by Google|\n",
      "                    SmolAgents|\n",
      "                    LangGraph|\n",
      "                    CrewAI|\n",
      "                    Agno|\n",
      "                    LangFlow|\n",
      "                    AutoGen|\n",
      "                    LlamaIndex|\n",
      "                    Swarm|\n",
      "                    AutoGPT\n",
      "\n",
      "\n",
      "\n",
      "Data Science Tools and Techniques\n",
      "\n",
      "Python|\n",
      "                    R|\n",
      "                    SQL|\n",
      "                    Jupyter Notebooks|\n",
      "                    TensorFlow|\n",
      "                    Scikit-learn|\n",
      "                    PyTorch|\n",
      "                    Tableau|\n",
      "                    Apache Spark|\n",
      "                    Matplotlib|\n",
      "                    Seaborn|\n",
      "                    Pandas|\n",
      "                    Hadoop|\n",
      "                    Docker|\n",
      "                    Git|\n",
      "                    Keras|\n",
      "                    Apache Kafka|\n",
      "                    AWS|\n",
      "                    NLP|\n",
      "                    Random Forest|\n",
      "                    Computer Vision|\n",
      "                    Data Visualization|\n",
      "                    Data Exploration|\n",
      "                    Big Data|\n",
      "                    Common Machine Learning Algorithms|\n",
      "                    Machine Learning|\n",
      "                    Google Data Science Agent\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Company\n",
      "\n",
      "About Us\n",
      "Contact Us\n",
      "Careers\n",
      "\n",
      "\n",
      "\n",
      "Discover\n",
      "\n",
      "Blogs\n",
      "Expert Sessions\n",
      "Learning Paths\n",
      "Comprehensive Guides\n",
      "\n",
      "\n",
      "\n",
      "Learn\n",
      "\n",
      "\n",
      "\n",
      "Free Courses\n",
      "\n",
      "AI&ML Program\n",
      "Pinnacle Plus Program\n",
      "Agentic AI Program\n",
      "\n",
      "\n",
      "\n",
      "Engage\n",
      "\n",
      "Community\n",
      "Hackathons\n",
      "Events\n",
      "Podcasts\n",
      "\n",
      "\n",
      "\n",
      "Contribute\n",
      "\n",
      "Become an Author\n",
      "Become a Speaker\n",
      "Become a Mentor\n",
      "Become an Instructor\n",
      "\n",
      "\n",
      "\n",
      "Enterprise\n",
      "\n",
      "Our Offerings\n",
      "Trainings\n",
      "Data Culture\n",
      "AI Newsletter\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Terms & conditions\n",
      "\n",
      "\n",
      "\n",
      "Refund Policy\n",
      "\n",
      "\n",
      "\n",
      "Privacy Policy\n",
      "\n",
      "\n",
      "\n",
      "Cookies Policy\n",
      "© Analytics Vidhya 2025.All rights reserved.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                SKIP\n",
      "            \n",
      "\n",
      "\n",
      "Continue your learning for FREE\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Login with Google\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Login with Email\n",
      "            \n",
      "Forgot your password?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I accept the Terms and Conditions\n",
      "\n",
      "\n",
      "\n",
      "Receive updates on WhatsApp\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Enter email address to continue\n",
      "\n",
      "\n",
      "Email address\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                    Get OTP\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Enter OTP sent to\n",
      "\n",
      "\n",
      "Edit\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                    Wrong OTP.\n",
      "                    \n",
      "\n",
      "Enter the OTP\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Resend OTP\n",
      "Resend OTP in 45s\n",
      "\n",
      "\n",
      "\n",
      "                Verify OTP\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLMOps Essentials: A Practical Guide to Operationalizing Large Language Models | DataCampSkip to main contentENEnglishEspañolPortuguêsDeutschBetaFrançaisBetaMore InformationFound an Error?blogsBlogsTutorialsdocsPodcastsCheat Sheetscode-alongsNewsletterCategoryCategory About DataCamp Latest news about our products and teamCertificationDataCamp ClassroomsDataCamp DonatesFor BusinessLearner StoriesLife at DataCampProduct NewsCategory Technologies Discover content by tools and technologyAI AgentsAirflowAlteryxArtificial IntelligenceAWSAzureBusiness IntelligenceChatGPTDatabricksdbtDockerExcelFlinkGenerative AIGitGoogle Cloud PlatformHadoopJavaJuliaKafkaKubernetesLarge Language ModelsMongoDBMySQLNoSQLOpenAIPostgreSQLPower BIPySparkPythonRScalaSigmaSnowflakeSpreadsheetsSQLTableauCategory Topics Discover content by data science topicsAI for BusinessBig DataCareer ServicesCloudData AnalysisData EngineeringData GovernanceData LiteracyData ScienceData StorytellingData VisualizationDataCamp ProductDataLabDeep LearningMachine LearningMLOpsBrowse CoursescategoryHomeBlogArtificial IntelligenceLLMOps Essentials: A Practical Guide to Operationalizing Large Language ModelsExplore the essentials of LLMOps with our guide on operationalizing Large Language Models for efficiency and reliability in AI applications.\n",
      "List Feb 2, 2024  · 15 min readGroupTraining more people?Get your team access to the full DataCamp for business platform.For BusinessFor a bespoke solution book a demo.When we, as users, interact with ChatGPT, we simply type a prompt into the web interface and press enter. Typically, we start receiving a response within a few seconds. However, beneath this seamless interaction lies a complex and orderly series of steps that enable ChatGPT to deliver such an experience.\n",
      "The automatic execution of this series of steps, known as Large Language Model Operations (LLMOps), guarantees that the prompt not only reaches the model but is also processed efficiently, accurately, and reliably. This ensures the delivery of a well-crafted response within a reasonable timeframe.\n",
      "In this article, we will delve into the LLMOps paradigm by tracing the journey of a prompt through a Large Language Model (LLM) service, like ChatGPT. We will examine the crucial stages, including prompt preprocessing, model selection, response generation, and the often-overlooked but vital aspects such as load balancing, monitoring, and Continuous Integration.\n",
      "What is LLMOps?\n",
      "LLMOps are indeed an evolution of the well-known Machine Learning Operations (MLOps), tailored to meet the concrete challenges presented by LLMs. While MLOps is centered on the lifecycle management of general Machine Learning models, LLMOps incorporates aspects uniquely related to these types of models.\n",
      "It is crucial to understand that whenever we interact with a model from OpenAI or Google, whether through a web interface or API calls from our code, LLMOps are transparent to us. In this scenario, we say these models are provided as-a-service.\n",
      "On the other hand, if our goal is to provide our model for a specific use case without reliance on external providers, like an assistant for a company’s employees, then the responsibility of LLMOps falls on us.\n",
      "Regardless of the capabilities of our new model, its success as a service will highly depend on the presence of a robust and reliable LLMOps infrastructure.If you are interested in knowing more about MLOps, the tutorial MLOps Fundamentals is for you!\n",
      "Origin of LLMOps\n",
      "Early LLMs such as GPT-2 were introduced in 2018. However, they have become popular more recently, primarily due to the significant advancements in the capabilities of the newer versions of those models, from GPT3 and beyond.\n",
      "Multiple applications leveraging LLMs have emerged due to their impressive model capabilities. Examples include customer service chatbots, language translation services, and writing and coding assistants, among others.\n",
      "Developing production-ready applications powered by LLMs presents a unique set of challenges, different from those encountered in traditional ML models. To address these challenges, novel tools and best practices for managing the LLM application lifecycle were developed, leading to the concept “LLMOps.”\n",
      "Why LLMOps?\n",
      "LLMOps are essential for efficiently managing these complex models when deployed as-a-service for multiple reasons:\n",
      "1. LLMs are not just big in terms of the amount of data they handle but also in their number of parameters. LLMOps ensure that the infrastructure can support these models in terms of storage and bandwidth.\n",
      "2. Receiving an accurate response in the minimum time is crucial for users. LLMOps ensure that responses are delivered in a reasonable time, maintaining the fluidity of human-like interactions.\n",
      "3. Continuous monitoring under LLMOps is not just about tracking the operational aspects or glitches in the infrastructure. It also entails careful tracking of the model’s behavior to understand its decision-making processes and to further improve the model in future iterations.\n",
      "4. Running LLMs can be expensive due to the resources they need. LLMOps introduces cost-effective strategies to ensure that resources are used optimally without compromising on performance.\n",
      "Behind the Scenes of an LLM Service\n",
      "To understand LLMOps, it is important to be familiar with the “behind the scenes” of LLMs when offered as-a-service. That is the path that a prompt follows once it is provided to the model until the response is generated. The following schema represents this workflow:\n",
      "\n",
      "LLMOps workflow: The steps behind the scenes of a generic LLM as-a-service. User input (in green) undergoes some steps before it is input to the model. Similarly, the model output (in red) undergoes several transformations before being displayed to the user.\n",
      "As we can observe from the schema above, the prompt undergoes several steps before reaching the model. While the number of steps can vary, there are basic ones to ensure, for example, that the input is clearly understood and that the model’s response is contextually relevant. Let’s break down these steps:\n",
      "1. Pre-processing\n",
      "This step prepares the user’s prompt so that the model can understand and process it. It includes tokenization, where the prompt is segmented into smaller units called tokens. This step also involves data normalization, which includes removing or transforming noisy data, such as special characters, correcting typos, and standardizing the text.\n",
      "Finally, during encoding, tokens are converted into a numerical form that the model can understand. This is done by using embeddings, which represent each token as a vector in a high-dimensional space.\n",
      "2. Grounding\n",
      "This involves contextualizing the prompt based on previous conversation turns or external knowledge sources to ensure that the model’s response is coherent and contextually appropriate. Additionally, entity recognition and linking help the system identify entities (such as names, places, and dates) within the prompt and associate them with the relevant context.\n",
      "3. Responsible AI\n",
      "To ensure the usage of LLMs has good purposes, some services implement sanity checks on the user’s prompts. Typically, the prompt is evaluated against safety and compliance guidelines, particularly in scenarios involving sensitive information, inappropriate content, bias, or potential misinformation.\n",
      "Only after these steps, the prompt is finally forwarded to the model for processing. After the model generates the response, and before displaying it to the user, the response might redo again the steps on Grounding and Responsible AI, as well as an extra post-processing step:\n",
      "4. Post-Processing\n",
      "The response generated by the model is in numerical form due to the previously mentioned vector embeddings. Therefore, a decoding process is essential to convert this numerical data back into human-readable text. Following the decoding, a refinement step is needed to polish the response for grammar, style, or readability.\n",
      "Finally, the response is displayed to the user. The LLMOps infrastructure is responsible for executing these steps transparently to the user.\n",
      "Latency\n",
      "As of now, we have seen quite some steps that the LLMOps infrastructure needs to execute from the moment a user sends a prompt until they get the response back. At this point, a reasonable question to ask could be: How much time do these steps take?\n",
      "When using ChatGPT, the response time is normally almost immediate. This response time is known as latency. Latency is a critical performance metric, especially in user-facing applications where response time significantly affects the user experience. Choosing the appropriate latency is crucial, depending on our use case.\n",
      "When it comes to reducing the latency of the response, LLMOps employs various strategies and best practices to streamline the entire process from input reception to response delivery. LLMOps contribute to reducing model latency by executing all the required steps automatically and managing the resources needed efficiently so that no computation is waiting for available resources to run. Other best practices can also reduce latency:\n",
      "\n",
      "Caching: Storing frequently used or computationally intensive parts of the model's output so they don't need to be recalculated at each request.\n",
      "Concurrent processing: Processing multiple requests in parallel, for example when multiple users are sending requests at the same time, to make better use of available computational resources and reduce waiting time.\n",
      "Monitoring: Profiling different components of the model and the infrastructure to identify bottlenecks and optimize them as soon as possible.\n",
      "\n",
      "Reducing latency not only improves the user experience but also contributes to cost efficiency by optimizing resource usage.\n",
      "Key Components of LLMOps\n",
      "Choosing the Right Foundation Model\n",
      "Up to this point, we haven’t discussed which model should be present in our LLMOps setup. There are various types of models available, each optimized for specific use cases, different size options, etc. Selecting the appropriate model is crucial and will largely depend on our application and available resources.\n",
      "In addition, it seems that the number of LLMs and providers is increasing every day. That is why having a broad understanding of the different types of LLM and their providers is beneficial, especially when trying to identify the one that best suits our use case.\n",
      "LLM Providers\n",
      "LLM models and providers can be categorized into the following types:\n",
      "\n",
      "Proprietary models: This category includes companies like OpenAI (GPT models), Google (PaLM models), and Anthropic (Claude model), which train proprietary LLMs and offer them as-a-service through web interfaces or API endpoints.\n",
      "Open-source models: This category includes free models that are developed by the community, academia, or organizations such as Eleuther AI and Big Science. These organizations typically rely on donations for the computing infrastructure. Ideally, we could take an open-source model and engineer the service  ourselves,  including the LLMOps infrastructure.\n",
      "Companies providing infrastructure: These are companies that provide the LLMOps infrastructure for open-source LLMs. Therefore, they monetize by offering deployment services, as seen with companies like Together AI. Here, the company provides the opportunity for easy customization of your LLMOps infrastructure.\n",
      "\n",
      "Proprietary Models vs Open-Source\n",
      "The primary benefits of open-source models lie in the transparency and the possibility to customize them. Open-source models normally offer full control on all the components, enabling easy debugging and extensive customization through training or fine-tuning. This degree of flexibility allows for better alignment of the LLM with your specific needs, as opposed to conforming to the predefined options set by the LLM provider.\n",
      "However, managing open-source LLMs on your own can lead to significant engineering challenges and costs related to computing and storage. Even if there is a minimum infrastructure provided by open-source models, it is often challenging to compete with proprietary models in terms of latency, throughput, and inference costs.\n",
      "Model Selection Criteria\n",
      "There are many models out there to choose from. But how do we ensure we are choosing the right LLM?\n",
      "Indeed, there are some criteria to consider depending on our use case and possibilities:\n",
      "\n",
      "Cost: We should not only consider the costs of the model’s inference but also the engineering expenses related to maintenance, monitoring, and optimization of the model.\n",
      "Type of Tasks: The nature of the tasks for which the LLM will be employed, such as summarization, question answering, etc., should align with the model’s capabilities. Models already fine-tuned for our targeted task will save us time and money in performing our fine-tuning.\n",
      "Performance Metrics: Many providers disclose the performance metrics of their models, such as the speed of text generation (Time Per Output Token), and the time required to generate the first token (Time To First Token). We should make sure the model performs as expected for our use case.\n",
      "Licensing: It is crucial to select models that allow our intended usage. Even models that explicitly allow commercial use may have restrictions on specific types of use cases. For instance, the BigScience Open RAIL-M license imposes limitations on the use of the LLM in domains related to law enforcement, immigration, and asylum processes, among others.\n",
      "\n",
      "Fine-Tuning Strategies\n",
      "Whether proprietary or open-source, LLMs often require fine-tuning to be truly suited for specific applications.\n",
      "Pre-fine-tuned LLMs are available for certain user tasks, commonly including chat models and models specialized in summarization or sentiment analysis. Another variant to consider is long-context models. Modern LLMs typically handle context lengths ranging from 2,000 to 8,000 tokens, meaning inputs and outputs beyond this range can’t be processed directly. However, some models offer long-context variants. For instance, GPT 3.5 has a larger 16k context-size variant.\n",
      "Nevertheless, if the existing options don’t meet specific requirements, there is always the possibility of fine-tuning or even training a model ourselves. In this case, choosing an appropriate dataset is crucial, as it enables the model to understand the nature of the targeted task.\n",
      "If you are interested in training an LLM model from scratch, I really recommend you to read the DataCamp tutorial, How to train an LLM with PyTorch.\n",
      "Model customization\n",
      "If our application requires the fine-tuning of an existing model, the related steps should also be part of our LLMOps setup. Let’s add this customization step to our original diagram:\n",
      "\n",
      "LLMOps workflow: Including the Model Customizatin steps (in orange) into our generic workflow.\n",
      "Having a consistent fine-tuning pipeline can assist you in expanding your model’s knowledge over time as more data becomes available, allowing you to effortlessly upgrade your LLM version or make other modifications.\n",
      "When depending on third-party models, it is important to note that these models can change from their availability to their cost. This can potentially force us to switch to a different base model. A robust LLMOps setup will enable us to handle this critical situation smoothly by just replacing the “Model” box with a different LLM.\n",
      "Training data\n",
      "One could think that fine-tuning your LLM or training it from scratch is a first step outside the LLMOps infrastructure since the LLM will operate once it is deployed to production. Nevertheless, that is not true, especially for the aforementioned reason: a successful service will need improvements in the model and be resilient to changes in providers, if necessary.\n",
      "For effective training, fine-tuning, or model refinement in a generic LLMOps infrastructure, it is important to maintain data formatting consistent between training and posterior inference data. To do so, we typically format the training data in the JSON Lines (.jsonl) format. This format is highly suitable for fine-tuning LLMs due to its structure, allowing for efficient processing of large datasets. A typical .jsonl file for fine-tuning might look like this:\n",
      "{\"prompt\": \"Question: What is the capital of France?\", \"completion\": \"The capital of France is Paris.\"}\n",
      "{\"prompt\": \"Question: Who wrote Macbeth?\", \"completion\": \"Macbeth was written by William Shakespeare.\"}\n",
      "Each line in a .jsonl file is a distinct JSON object, representing a single training example, with prompt and completion keys indicating the input text and expected model response, respectively, as we would expect during inference. In addition, this format facilitates the incremental addition of new data to the model’s knowledge base.\n",
      "Training and Inference Parameters\n",
      "Model parameters are also important when setting our LLMOps infrastructure since they have an impact on characteristics like model size and resource consumption.\n",
      "Regarding training parameters, it is important to optimize training parameters to balance the model’s complexity with deployment limitations such as memory usage. This optimization is crucial for deploying models across diverse environments with varying resource capacities, ensuring that models are not just advanced but also practical for real-world applications.\n",
      "Regarding the inference parameters, adjusting parameters like temperature and max tokens allows for controlling the length and randomness of the responses. These settings are managed as part of the LLMOps process to align the model’s output with the specific application requirements and user intention.\n",
      "Prompt Engineering and Management\n",
      "Prompt engineering techniques have been shown to enhance the default capabilities of LLMs. One reason for this is that they help to contextualize the model. For example, instructing the model to behave as an expert in a specific domain or guiding the model towards a desired output. Prompt Engineering and Management are crucial components that should be included in our LLMOps setup.\n",
      "Among the most effective prompt engineering practices are few-shot prompting and chain-of-thought reasoning. Let’s briefly review these techniques and discuss how they can be integrated into our LLMOps setup:\n",
      "\n",
      "Few-shot prompting involves providing the LLM with a small number of examples (shots) of the task at hand within the prompt itself. This helps the model to understand and perform the specific task more effectively.\n",
      "Chain of thought reasoning involves structuring prompts to lead the model through a step-by-step reasoning process. This technique is particularly useful for complex tasks that require logical reasoning or interfacing with external sources.\n",
      "\n",
      "Implementing techniques such as few-shot prompting and chain-of-thought reasoning within our LLMOps setup can be effectively achieved through the use of prompt templates.\n",
      "Prompt templates\n",
      "Prompt templates are predefined structures used to guide the reasoning of LLMs. They provide a consistent structure, ensuring that similar types of requests are made uniformly or helping to include prompt engineering techniques transparently to the user.\n",
      "Developing and managing a repository of well-crafted prompt templates for various use cases is crucial in an LLMOps setup. The selection of the appropriate prompt template for a specific use case is usually a preliminary step performed during the pre-processing phase before the query is sent to the model.\n",
      "For few-shot prompting, the templates should be designed to include several examples that demonstrate the task or response style expected from the model. Similarly, to incorporate chain-of-thought reasoning, the prompt templates should be meticulously designed to include a step-by-step thought process. In essence, they should guide the model on how to methodically approach and decompose any given problem into simpler and more manageable steps.\n",
      "Incorporating knowledge from external sources may be a step in the chain-of-thought reasoning, especially useful in frameworks like LangChain, where the model is instructed to retrieve information from the internet if its existing knowledge base is insufficient.\n",
      "In both few-shot prompting and chain-of-thought reasoning, employing A/B testing of prompts is highly beneficial. This involves conducting controlled experiments by exposing different subsets of users to different prompt versions, objectively measuring their performance, and selecting the most effective ones based on the results. Moreover, in both cases, it is important to rely on performance data to iteratively improve our prompt templates.\n",
      "Deployment and Monitoring\n",
      "Once the base model is trained or fine-tuned and we are happy with the result, it is time to deploy the model. Deployment of a model in the context of LLMOps refers to the process of making a language model available for use in a production environment. This involves taking the model out of the training environment and integrating it into the production infrastructure. This step is indicated in orange in our usual diagram:\n",
      "\n",
      "LLMOps workflow: Highlighting the deployment step (in orange) into our generic workflow. This step involves “moving” the model from the development environment to production.\n",
      "The deployment also includes setting the interface that we will use to communicate with the model once in production. Generally, the interface depends on our processing mode:\n",
      "\n",
      "Real-time processing: For applications requiring real-time interactions, such as chat applications, it is essential to deploy the model in a way that allows for immediate processing of data and generation of output. This is commonly achieved by creating an Application Programming Interface (API) that interfaces with the model. Nowadays, there are libraries such as Flask that allow us to create API interfaces in simple steps.\n",
      "\n",
      "The APIs can be deployed on web servers or cloud platforms, ensuring they are accessible to the users or the systems that need to interact with the model. Our LLMOps setup should ensure that the API can handle the expected load, with considerations for scaling, load balancing, and failover mechanisms.\n",
      "\n",
      "Batch prediction: In many use cases, real-time predictions are not necessary. For instance, if we have a batch of customer reviews and we need to classify them once a week, we can use our trained model to process these reviews in batches. This approach is efficient and resource-friendly for tasks that are not time-sensitive.\n",
      "\n",
      "For batch use cases, you can schedule batch jobs using tools like cron (in Unix-like systems) or cloud-based job scheduling services. These jobs will run the model on the new data at the specified intervals, process the data, and store the results.\n",
      "Finally, deploying the model to the production setup normally involves model packaging and versioning:\n",
      "\n",
      "Packaging involves wrapping your model and its dependencies into a format that can be easily deployed and used in the production environment. This might involve containerization technologies like Docker, which encapsulate the model and its environment to ensure consistency across different platforms.\n",
      "Model Versioning: Keeping track of different versions of your model is crucial, especially when you update or retrain your model. Versioning helps in maintaining a clear record of model iterations, training data, and prompt templates.\n",
      "\n",
      "CI/CD Pipelines\n",
      "Continuous Integration (CI) and Continuous Delivery (CD) pipelines automate the steps involved in bringing a model from the development environment to production, ensuring that the model is reliable, up-to-date, and deployed efficiently.\n",
      "In LLMOps, as new code or changes are introduced to the model (like hyperparameter tuning, model architecture changes, or new training data), CI ensures that these changes are automatically tested. This includes running unit tests, integration tests, and any other checks to validate that the changes do not break the model or degrade its performance. In this sense, CI is a way of continuously monitoring our model.\n",
      "Once the changes pass all tests in the CI phase, CD automates the model’s deployment to the production environment. This ensures that the most stable, tested version of the model is always running in the LLMOps production setup. CD also facilitates quick rollback to previous versions if an issue is detected in the production environment, minimizing downtime and ensuring service reliability.\n",
      "Orchestration\n",
      "Finally, there is one missing aspect that we haven’t commented on yet: how do we order our LLMOps components so that we form a reasonable chain of steps?\n",
      "Orchestration involves defining and managing the order of operations in LLMOps setup, forming the so-called workflow. For example, defining the order of the pre and post-processing steps or the different tests that a new model must undergo until it is ready to be deployed.\n",
      "In the LLM context, orchestration normally involves passing data between different components of the workflow. This is often managed by specifying data paths or workspaces where the output of one step is stored and then picked up by the next step.\n",
      "Orchestration is often managed through configuration files, typically written in YAML (Yet Another Markup Language). These files define the components, their order, and the parameters for each step in the workflow. Domain Specific Languages (DSLs) are often used in these configuration files to provide a more intuitive and specialized syntax for defining the workflows.\n",
      "Finally, workflows are normally automated. That ensures that once the workflow is initiated, all the related steps run smoothly without manual intervention, from one step to the next, thereby reducing the need for manual execution of tasks to avoid issues.\n",
      "Advanced Techniques in LLMOps\n",
      "In this article, we have discussed the key components of an LLMOps infrastructure and their reason for being. Nevertheless, other advanced techniques can enhance your LLMOps infrastructure performance:\n",
      "\n",
      "High-performance resources: Utilizing high-performance computing resources such as GPUs or TPUs can lead to faster inference, significantly reducing latency compared to using CPUs. When setting up an LLMOps infrastructure, the hardware must be selected wisely.\n",
      "Load-balancing: If we plan to offer a widely-used service across different countries, similar to ChatGPT, it is advisable to deploy multiple instances of the same model. This approach allows you to distribute incoming requests across various models. Our LLMOps setup should be aware of the number of models available and their computing capabilities at each moment.\n",
      "Geographical distribution: Additionally, if multiple models are available in different countries, a straightforward technique is to host the models — and the necessary parts of the LLMOps infrastructure — closer to the end-users. This strategy involves optimizing data serialization and transfer protocols to ensure fast and efficient data transfer between the user, the infrastructure, and the model.\n",
      "\n",
      "Addressing Security Concerns\n",
      "Ensuring data privacy and user protection in our LLMOps infrastructure is central to building a reliable service.\n",
      "For example, implementing robust data anonymization techniques is critical. Techniques such as differential privacy, k-anonymity, or data masking can be used to ensure that the training data does not reveal sensitive personal information gathered when forming the training datasets. In addition, if we plan to use real-world data to iteratively improve our models, the user should be aware of it, and their data must be anonymized before incorporating it into the fine-tuning loop.\n",
      "Another aspect concerning security is if we plan to store any private user information in our system, such as the conversation history in ChatGPT. In this case, we must ensure that all data is handled securely, and in compliance with data protection regulations like GDPR. This includes ensuring secure data storage and encrypted data transmission in our infrastructure.\n",
      "Finally, we must ensure robust access controls in the LLMOps infrastructure, ensuring that only authorized people have access to the model, the data, and the training environment and that there won’t be any leaks exposing users’ personal information.\n",
      "Conclusion\n",
      "LLM-powered applications have been on the rise since last year, driven by the enhanced capabilities of recent LLM iterations. These applications deliver LLMs as-a-service, thereby necessitating a robust framework for their management and optimization, known as the LLMOps infrastructure.\n",
      "In this article, we have explored the critical role of LLMOps as the backbone of a successful, efficient, and customer-centric LLM service. Firstly, we have reviewed the journey of a prompt from the moment a user sends it to the model until the response is received. LLMOps ensures that all these “behind the scenes” steps are streamlined and efficient.\n",
      "Secondly, we have examined how LLMOps encompasses model training, deployment, monitoring, and maintenance. LLMOps also includes scaling resources to efficiently handle varying loads, thus guaranteeing the scalability and reliability of our LLM-based applications. Additionally, we have touched upon advanced best practices to refine our LLMOps infrastructure.\n",
      "Finally, we have acknowledged how LLMOps is also involved in maintaining the integrity and security of LLMs as-a-service. As these models often process sensitive data, robust LLMOps practices are essential to enforce stringent security measures, ensuring data privacy and compliance with regulatory standards.\n",
      "In conclusion, after reading this article, I hope I have convinced you of one important aspect: LLMOps is not just an operational necessity but a strategic asset that enhances the value, reliability, and sustainability of LLMs as-a-service.\n",
      "Ready to put theory into practice? Dive deeper into the world of Large Language Models with our hands-on tutorial: \"How to Build LLM Applications with LangChain\". Start transforming your knowledge into actionable skills today!AuthorAndrea ValenzuelaLinkedInTwitter Andrea Valenzuela is currently working on the CMS experiment at the particle accelerator (CERN) in Geneva, Switzerland. With expertise in data engineering and analysis for the past six years, her duties include data analysis and software development. She is now working towards democratizing the learning of data-related technologies through the Medium publication ForCode'Sake.\n",
      "She holds a BS in Engineering Physics from the Polytechnic University of Catalonia, as well as an MS in Intelligent Interactive Systems from Pompeu Fabra University. Her research experience includes professional work with previous OpenAI algorithms for image generation, such as Normalizing Flows.\n",
      "TopicsArtificial IntelligenceAndrea ValenzuelaA data expert at CERN, democratizing tech learning. Skilled in data engineering and analysis.TopicsArtificial IntelligenceWhat is an LLM? A Guide on Large Language Models and How They WorkExploring BLOOM: A Comprehensive Guide to the Multilingual Large Language ModelUnderstanding and Mitigating Bias in Large Language Models (LLMs)MLOps Best Practices and How to Apply ThemQuantization for Large Language Models (LLMs): Reduce AI Model Sizes EfficientlyFine-Tuning LLMs: A Guide With ExamplesStart Your LLM Journey Today!TrackOpenAI Fundamentals0 minBegin creating AI systems using models from OpenAI. Learn how to use the OpenAI API to prompt OpenAI's GPT and Whisper models.See DetailsRight ArrowStart CourseCourseWorking with the OpenAI API3 hr66.1KStart your journey developing AI-powered applications with the OpenAI API. Learn about the functionality that underpins popular AI applications like ChatGPT.See DetailsRight ArrowStart CourseCourseIntroduction to Embeddings with the OpenAI API3 hr11KUnlock more advanced AI applications, like semantic search and recommendation engines, using OpenAI's embedding model!See DetailsRight ArrowStart CourseSee MoreRight ArrowRelatedblogWhat is an LLM? A Guide on Large Language Models and How They WorkRead this article to discover the basics of large language models, the key technology that is powering the current AI revolution\n",
      "Javier Canales Luna 12 minblogExploring BLOOM: A Comprehensive Guide to the Multilingual Large Language ModelDive into BLOOM, a multilingual large language model, exploring its creation, technical specs, usage, and ethical aspects for democratizing AI.\n",
      "Zoumana Keita  13 minblogUnderstanding and Mitigating Bias in Large Language Models (LLMs)Dive into a comprehensive walk-through on understanding bias in LLMs, the impact it causes, and how to mitigate it to ensure trust and fairness. \n",
      "Nisha Arya Ahmed 12 minblogMLOps Best Practices and How to Apply ThemLearn the key best practices of a successful MLOps practice and how it ensures reliable and scalable deployment of machine learning systemsAdel Nehme 12 minTutorialQuantization for Large Language Models (LLMs): Reduce AI Model Sizes EfficientlyA Comprehensive Guide to Reducing Model SizesAndrea Valenzuela TutorialFine-Tuning LLMs: A Guide With ExamplesLearn how fine-tuning large language models (LLMs) improves their performance in tasks like language translation, sentiment analysis, and text generation.Josep Ferrer See MoreSee MoreGrow your data skills with DataCamp for MobileMake progress on the go with our mobile courses and daily 5-minute coding challenges.LearnLearn PythonLearn AILearn Power BILearn Data EngineeringAssessmentsCareer TracksSkill TracksCoursesData Science RoadmapData CoursesPython CoursesR CoursesSQL CoursesPower BI CoursesTableau CoursesAlteryx CoursesAzure CoursesAWS CoursesGoogle Sheets CoursesExcel CoursesAI CoursesData Analysis CoursesData Visualization CoursesMachine Learning CoursesData Engineering CoursesProbability & Statistics CoursesDataLabGet StartedPricingSecurityDocumentationCertificationCertificationsData ScientistData AnalystData EngineerSQL AssociatePower BI Data AnalystTableau Certified Data AnalystAzure FundamentalsAI FundamentalsResourcesResource CenterUpcoming EventsBlogCode-AlongsTutorialsDocsOpen SourceRDocumentationBook a Demo with DataCamp for BusinessData PortfolioPlansPricingFor StudentsFor BusinessFor UniversitiesDiscounts, Promos & SalesExpense DataCampDataCamp DonatesFor BusinessBusiness PricingTeams PlanData & AI Unlimited PlanCustomer StoriesPartner ProgramAboutAbout UsLearner StoriesCareersBecome an InstructorPressLeadershipContact UsDataCamp EspañolDataCamp PortuguêsDataCamp DeutschDataCamp FrançaisSupportHelp CenterBecome an AffiliateFacebookTwitterLinkedInYouTubeInstagramPrivacy PolicyCookie NoticeDo Not Sell My Personal InformationAccessibilitySecurityTerms of Use© 2025 DataCamp, Inc. All Rights Reserved.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "What Is Data Management? | IBM\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        \n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "    What is data management?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "                    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                    Analytics\n",
      "                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                    \n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "    3 July 2024\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Link copied\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "    Authors\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jim Holdsworth\n",
      "\n",
      "Writer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        What is data management?\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "Data management is the practice of collecting, processing and using data securely and efficiently for better business outcomes.\n",
      "\n",
      "\n",
      "72% of top-performing CEOs agree that competitive advantage depends on who has the most advanced generative AI. However, in order to take advantage of artificial intelligence (AI), organizations must first organize their information architecture to make their data accessible and usable. Fundamental data management challenges include data volumes, and data silos across multiple locations and cloud providers. New data types and various formats such as documents, images and videos, also present challenges. Also, complexity and inconsistent datasets can limit an organization’s ability to use data for AI.\n",
      "As a result of these challenges, an effective data management strategy has become an increasing priority for organizations to address challenges presented by big data. A flexible, modern data management system integrates with existing technology within an organization to access high-quality, usable data for data scientists, AI and machine learning (ML) engineers, and the organization’s business users. \n",
      "A complete data management strategy accounts for various factors, including how to:\n",
      "Collect, integrate and store data from diverse sources—including structured and unstructured data—and from across hybrid and multiple clouds.\n",
      "\n",
      "Maintain high-availability, resiliency and disaster recovery of data stored across multiple locations.\n",
      "\n",
      "Build or obtain fit-for-purpose databases to meet a variety of workload and price-performance needs.\n",
      "\n",
      "Help ensure sharing of business data and metadata across organizations, to enable greater self-service, collaboration and access to data.\n",
      "\n",
      "Secure and govern data, while helping meet compliance requirements and meeting data privacy requirements.\n",
      "\n",
      "Manage the data lifecycle from creation to deletion with end-to-end integration, governance, lineage, observability and master data management (MDM).\n",
      "\n",
      "Automate data discovery and analysis with generative AI for data management.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Industry newsletter\n",
      "\n",
      "\n",
      "\n",
      "The latest tech news, backed by expert insights\n",
      "\n",
      "\n",
      "\n",
      "Stay up to date on the most important—and intriguing—industry trends on AI, automation, data and beyond with the Think newsletter. See the IBM Privacy Statement.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Thank you! You are subscribed.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe here. Refer to our IBM Privacy Statement for more information.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Why data management is important\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "While the data management tools for constructing generative AI applications are widely available, the data itself holds the value for both customers and businesses. High volumes of quality data must be properly organized and processed to successfully train models. This approach is a rapidly growing use case for modern data management.\n",
      "For example, a generative AI-driven commentary was offered during The Championships 2023 at Wimbledon, which accessed information from 130 million documents and 2.7 million pertinent contextual data points in real time. Visitors using the tournament app or website were able to access complete statistics, play-by-play narration and game commentary, as well as a precise prediction of the winner at any moment as matches progressed. Having the correct data management strategy can help ensure that valuable data is always available, integrated, governed, secure and accurate.\n",
      "\n",
      "\n",
      "Transforming data into a trusted asset\n",
      "\n",
      "\n",
      "Generative AI can give organizations a strong competitive advantage, with their AI strategy relying on the strength of the data that’s used. Many organizations still struggle with fundamental data challenges that are exacerbated by the demand for generative AI, which requires ever more data—leading to yet more data management headaches.\n",
      "Data might be stored in multiple locations, applications and clouds, often leading to isolated data silos. To add even more complexity, the uses of data have become more varied, with data in varying and complex forms—such as images, videos, documents and audio. More time is required for data cleaning, integration and preparation. These challenges can lead organizations to avoid using their full data estate for analytics and AI purposes.\n",
      "However, equipped with modern tools for data architecture, governance and security, data can be successfully used to gain new insights and make more precise predictions consistently. This capability can enable a deeper understanding of customer preferences and can enhance customer experiences (CX) by delivering insights derived from data analysis. Moreover, it facilitates the development of innovative data-driven business models, such as service offerings reliant on generative AI, which need a foundation of high-quality data for model training.\n",
      "\n",
      "\n",
      "Creating the right data foundation for digital transformation\n",
      "\n",
      "\n",
      "Data and analytics leaders face major challenges when transforming their organizations due to the increasing complexity of the data landscape across hybrid cloud deployments. Generative AI and AI assistants, machine learning (ML), advanced analytics, Internet of Things (IoT), and automation also all require huge volumes of data to work effectively. This data needs to be stored, integrated, governed, transformed and prepared for the right data foundation. And to build a strong data foundation for AI, organizations need to focus on building an open and trusted data foundation, which means creating a data management strategy that is centered on openness, trust and collaboration.\n",
      "\n",
      "The AI requirement was summed up by a Gartner® analyst1: “AI-ready data means that your data must be representative of the use case, including all patterns, errors, outliers and unexpected emergence that is needed to train or run the AI model for the specific use.”\n",
      "Data and analytics executives might feel that AI-prepared data equals high-quality data, but the standards of high-quality data for purposes other than AI do not necessarily meet the standard for AI readiness. In the realm of analytics, for instance, data is typically refined to eliminate outliers or conform to human expectations. However, when training an algorithm, it needs representative data.\n",
      "\n",
      "\n",
      "Ensuring governed, compliant and secure data\n",
      "\n",
      "\n",
      "Data governance is a subset of data management. This means that when a data governance team identifies commonalities across disparate datasets and wants to integrate them, they will need to partner with a database architecture or engineering team to define the data model and data architecture to facilitate linkages and data flows. Another example pertains to data access. A data governance team might set the policies around data access to specific types of data, such as personally identifiable information (PII). Meanwhile a data management team would either provide direct access or set a mechanism in place to provide access, such as adjusting internally defined user roles to approve access.\n",
      "Effective data management, including robust data governance practices, can help with adhering to regulatory compliance. This compliance encompasses both national and global data privacy regulations, such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA), along with industry-specific privacy and security standards. Establishing comprehensive data management policies and procedures becomes crucial for demonstrating or undergoing audits to validate these protections.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "  \n",
      "      AI Academy\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "    Is data management the secret to generative AI?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "Explore why high-quality data is essential for the successful use of generative AI.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Go to episode\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Key aspects of data management\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "Modern data management solutions provide an efficient way to manage data and metadata across diverse datasets. Modern systems are built with the latest data management software and reliable databases or data stores. This can include transactional data lakes, data warehouses or data lakehouses, combined with a data fabric architecture including data ingestion, governance, lineage, observability and master data management. Together, this trusted data foundation can feed quality data to data consumers as data products, business intelligence (BI) and dashboarding, and AI models—both traditional ML and generative AI.\n",
      "A strong data management strategy typically includes multiple components to streamline strategy and operations throughout an organization.\n",
      "\n",
      "\n",
      "The right databases and data lakehouse architecture\n",
      "\n",
      "\n",
      "While data can be stored before or after data processing, the type of data and purpose of it will usually dictate the storage repository that is used. While relational databases organize data into a tabular format, nonrelational databases do not have as rigid of a database schema.\n",
      "Relational databases are also typically associated with transactional databases, which run commands or transactions collectively. An example is a bank transfer. A defined amount is withdrawn from one account and then it is deposited within another. But for enterprises to support both structured and unstructured data types, they require purpose-built databases. These databases must also cater to various use cases across analytics, AI and applications. They must span both relational and nonrelational databases, such as key-value, document, wide-column, graph and in-memory. These multimodal databases provide native support for different types of data and the latest development models, and can run many kinds of workloads, including IoT, analytics, ML and AI.\n",
      "Data management best practices suggest that data warehousing be optimized for high-performance analytics on structured data. This requires a defined schema to meet specific data analytics requirements for specific use cases, such as dashboards, data visualization and other business intelligence tasks. These data requirements are usually directed and documented by business users in partnership with data engineers, who will ultimately run against the defined data model.\n",
      "The underlying structure of a data warehouse is typically organized as a relational system that uses a structured data format, sourcing data from transactional databases. However, for unstructured and semistructured data, data lakes incorporate data from both relational and nonrelational systems, and other business intelligence tasks. Data lakes are often preferred to the other storage options because they are normally a low-cost storage environment, which can house petabytes of raw data. \n",
      "Data lakes benefit data scientists in particular, as they enable them to incorporate both structured and unstructured data into their data science projects. However, data warehouses and data lakes have their own limitations. Proprietary data formats and high storage costs limit AI and ML model collaboration and deployments within a data warehouse environment.\n",
      "In contrast, data lakes are challenged with extracting insights directly in a governed and performant manner. An open data lakehouse addresses these limitations by handling multiple open formats over cloud object storage and combines data from multiple sources, including existing repositories, to ultimately enable analytics and AI at scale.\n",
      "\n",
      "\n",
      "Hybrid cloud database strategy\n",
      "\n",
      "\n",
      "Multicloud and hybrid strategies are steadily becoming more popular. AI technologies are powered by massive amounts of data that require modern data stores that reside on cloud-native architectures to provide scalability, cost optimization, enhanced performance and business continuity. According to Gartner2, by the end of 2026, \"90% of data management tools and platforms that fail to support multi-cloud and hybrid capabilities will be set for decommissioning.\"\n",
      "While existing tools aid database administrators (DBAs) in automating numerous conventional management duties, manual involvement remains necessary due to the typically large and intricate nature of database setups. Whenever manual intervention becomes necessary, the likelihood of errors rises. Minimizing the necessity for manual data management stands as a primary goal in operating databases as fully managed services.\n",
      "Fully managed cloud databases automate time-consuming tasks such as upgrades, backups, patching and maintenance. This approach helps free DBAs from time-consuming manual tasks to spend more time on valuable tasks such as schema optimization, new cloud-native apps and support for new AI use cases. Unlike on-premises deployments, cloud storage providers also enable users to spin up large clusters as needed, often requiring only payment for the storage specified. This means that if an organization needs more compute power to run a job in a few hours (versus a few days), it can do this on a cloud platform by purchasing more compute nodes.\n",
      "This shift to cloud data platforms is also facilitating the adoption of streaming data processing. Tools such as Apache Kafka enable more real-time data processing, so that consumers can subscribe to topics to receive data in a matter of seconds. However, batch processing still has its advantages as it’s more efficient at processing large volumes of data. While batch processing abides by a set schedule, such as daily, weekly or monthly, it is ideal for business performance dashboards, which typically do not require real-time data.\n",
      "\n",
      "\n",
      "Data fabric architecture\n",
      "\n",
      "\n",
      "More recently, data fabrics have emerged to assist with the complexity of managing these data systems. Data fabrics use intelligent and automated systems to facilitate end-to-end integration of data pipelines and cloud environments. A data fabric also simplifies delivery of quality data and provides a framework for enforcing data governance policies to help ensure that the data used is compliant. This facilitates self-service access to trustworthy data products by connecting to data residing across organizational silos, so that business leaders gain a more holistic view of business performance. The unification of data across HR, marketing, sales, supply chain and others give leaders a better understanding of their customer.\n",
      "\n",
      "A data mesh might also be useful. A data fabric is an architecture that facilitates the end-to-end integration. In contrast, a data mesh is a decentralized data architecture that organizes data by a specific business domain—for example, marketing, sales, customer service and more. This approach provides more ownership to the producers of a dataset.\n",
      "\n",
      "\n",
      "Data integration and processing\n",
      "\n",
      "\n",
      "Within this stage of the data management lifecycle, raw data is ingested from a range of data sources, such as web APIs, mobile apps, Internet of Things (IoT) devices, forms, surveys and more. After data collection, the data is usually processed or loaded by using data integration techniques, such as extract, transform, load (ETL) or extract, load, transform (ELT). While ETL has historically been the standard method to integrate and organize data across different datasets, ELT has been growing in popularity with the emergence of cloud data platforms and the increasing demand for real-time data.\n",
      "In addition to batch processing, data replication is an alternative method of integrating data and consists of synchronizing data from a source location to one or more target locations, helping ensure data availability, reliability and resilience. Technology such as change data capture (CDC) uses log-based replication to capture changes made to data at the source and propagate those changes to target systems, helping organizations make decisions based on current information.\n",
      "Independently of the data integration technique used, the data is usually filtered, merged or aggregated during the data processing stage to meet the requirements for its intended purpose. These applications can range from a business intelligence dashboard to a predictive machine learning algorithm.\n",
      "Using continuous integration and continuous deployment (CI/CD) for version control can enable data teams to track changes to their code and data assets. Version control enables data teams to collaborate more effectively, as they can work on different parts of a project simultaneously and merge their changes without conflicts.\n",
      "\n",
      "\n",
      "Data governance and metadata management\n",
      "\n",
      "\n",
      "Data governance promotes the availability and usage of data. To help ensure compliance, governance generally includes processes, policies and tools around data quality, data access, usability and data security. For instance, data governance councils tend to align taxonomies to help ensure that metadata is added consistently across various data sources. A taxonomy can also be further documented through a data catalog to make the data more accessible to users, facilitating data democratization across an organization.\n",
      "Enriching data with the right business context is critical for the automated enforcement of data governance policies and data quality. This is where service level agreement (SLA) rules come into effect, helping ensure that data is protected and of the required quality. It is also important to understand the provenance of the data and gain transparency into the journey of the data as it moves through pipelines. This calls for robust data lineage capabilities to drive visibility as organizational data makes it ways from data sources to the end users. Data governance teams also define roles and responsibilities to help ensure that data access is provided appropriately. This controlled access is particularly important to maintain data privacy.\n",
      "\n",
      "\n",
      "Data security\n",
      "\n",
      "\n",
      "Data security sets guardrails in place to protect digital information from unauthorized access, corruption or theft. As digital technology becomes an increasing part of our lives, more scrutiny is placed upon the security practices of modern businesses. This scrutiny is important to help protect customer data from cybercriminals or to help prevent incidents that need disaster recovery. While data loss can be devastating to any business, data breaches, in particular, can result in costly consequences from both a financial and brand standpoint. Data security teams can better secure their data by using encryption and data masking within their data security strategy. \n",
      "\n",
      "\n",
      "Data observability\n",
      "\n",
      "\n",
      "Data observability refers to the practice of monitoring, managing and maintaining data in a way that helps ensure its quality, availability and reliability across various processes, systems and pipelines within an organization. Data observability is about truly understanding the health of an organization’s data and its state across a data ecosystem. It includes various activities that go beyond traditional monitoring, which only describes a problem. Data observability can help identify, troubleshoot and resolve data issues in near-real time.\n",
      "\n",
      "\n",
      "Master data management\n",
      "\n",
      "\n",
      "Master data management (MDM) focuses on the creation of a single, high-quality view of core business entities including products, customers, employees and suppliers. By delivering accurate views of master data and their relationships, MDM enables faster insights, improved data quality and compliance readiness. With a single 360-degree view of master data across the enterprise, MDM enables businesses with the right data to drive business analytics, determine their most successful products and markets, and their highest valued customers.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Benefits of data management\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "Organizations experience multiple benefits when starting and maintaining data management initiatives. \n",
      "\n",
      "\n",
      "Reduced data silos\n",
      "\n",
      "\n",
      "Many companies inadvertently create data silos within their organization. Modern data management tools and frameworks, such as data fabrics and data lakes, help to eliminate data silos and dependencies on data owners. For instance, data fabrics assist in revealing potential integrations across disparate datasets across functions, such as human resources, marketing and sales. However, data lakes ingest raw data from those same functions, removing dependencies and eliminating single owners to a dataset.\n",
      "\n",
      "\n",
      "Improved compliance and security\n",
      "\n",
      "\n",
      "Governance councils assist in placing guardrails to protect businesses from fines and negative publicity that can occur due to noncompliance to government regulations and policies. Missteps here can be costly from both a brand and financial perspective.\n",
      "\n",
      "\n",
      "Enhanced customer experience\n",
      "\n",
      "\n",
      "While this benefit might not be immediately seen, successful proof of concepts can improve the overall user experience, enabling teams to better understand and personalize the customer journey through more holistic analyses.\n",
      "\n",
      "\n",
      "Scalability\n",
      "\n",
      "\n",
      "Data management can help businesses scale, but this largely depends on the technology and processes in place. For example, cloud platforms enable greater flexibility, so that data owners can scale up or scale down their compute power as needed.\n",
      "\n",
      "\n",
      "\n",
      "        New data management components\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "Over the last decade, developments within hybrid cloud, artificial intelligence, the Internet of Things (IoT) and edge computing have led to the exponential growth of big data, creating even more complexity for enterprises to manage. New components continue to improve data management capabilities. Here are some of the latest:\n",
      "\n",
      "\n",
      "Augmented data management\n",
      "\n",
      "\n",
      "To further boost data management capabilities, augmented data management is becoming increasingly popular. This is a branch of augmented intelligence, powered by cognitive technologies, which include AI, ML, data automation, data fabric and data mesh. The benefits of this automation include enabling data owners to create data products such as catalogs of data assets, with the ability to search and find data products, and query visuals and data products by using APIs. In addition, insights from data fabric metadata can help automate tasks by learning from patterns as part of the data product creation process or as part of the data management process of monitoring data products.\n",
      "\n",
      "\n",
      "Generative AI\n",
      "\n",
      "\n",
      "A data store for generative AI such as IBM® watsonx.data™ can help organizations unify, curate and prepare data efficiently for AI models and applications. Integrated and vectorized embedding capabilities enable retrieval-augmented generation (RAG) use cases at scale across large sets of trusted, governed data.\n",
      "\n",
      "\n",
      "Hybrid cloud deployments\n",
      "\n",
      "\n",
      "To simplify application connectivity and security across platforms, clusters and clouds, a hybrid cloud deployment can assist. Applications can be easily deployed and moved between environments because containers and object storage have made computing and data portable.\n",
      "\n",
      "\n",
      "Semantic layer\n",
      "\n",
      "\n",
      "To accelerate data access and unlock new data insights without SQL, organizations are creating an embeddable, AI-powered semantic layer. This is a metadata and abstraction layer that is built onto the organization’s source data, such as a data lake or warehouse. The metadata can enrich the data model being used and also be sufficiently clear for business users to understand.\n",
      "\n",
      "\n",
      "Shared metadata layer\n",
      "\n",
      "\n",
      "Organizations can access data across a hybrid cloud by connecting storage and analytics environments. This access can be through a single point of entry with a shared metadata layer across clouds and on-premises environments. Multiple query engines can be used to optimize analytics and AI workloads.\n",
      "Creating a shared metadata layer in a data lakehouse to catalog and share data is a best practice. This speeds discovery and enrichment, the analysis of data across multiple sources, the running of multiple workloads and use cases.\n",
      "In addition, a shared metadata management tool speeds the management of objects in a shared repository. It can be used to add a new host system, add a new database or data file, or add a new schema, in addition to deleting items from a shared repository.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Report\n",
      "            \n",
      "\n",
      "                Data management for AI and analytics\n",
      "            \n",
      "Explore the value of data architectures and learn how IBM’s database portfolio can help simplify data for all your applications, analytics and AI workflows.\n",
      "\n",
      "Read the report\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "    Resources\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Report\n",
      "            \n",
      "\n",
      "                Managing data for AI and analytics at scale\n",
      "            \n",
      "Learn how an open data lakehouse approach can provide trustworthy data and faster analytics and AI projects execution.\n",
      "\n",
      "Read the report\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Report\n",
      "            \n",
      "\n",
      "                2024 Gartner® Magic Quadrant™ for Data Integration Tools\n",
      "            \n",
      "IBM named a Leader for the 19th year in a row in the 2024 Gartner® Magic Quadrant™ for Data Integration Tools.\n",
      "\n",
      "Read the report\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Report\n",
      "            \n",
      "\n",
      "                Increase AI adoption with AI-ready data\n",
      "            \n",
      "Discover why AI-powered data intelligence and data integration are critical to drive structured and unstructured data preparedness and accelerate AI outcomes.\n",
      "\n",
      "Read the report\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Ebook\n",
      "            \n",
      "\n",
      "                The hybrid, open data lakehouse for AI\n",
      "            \n",
      "Simplify data access and automate data governance. Discover the power of integrating a data lakehouse strategy into your data architecture, including cost-optimizing your workloads and scaling AI and analytics, with all your data, anywhere.\n",
      "\n",
      "Read the ebook\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Insights\n",
      "            \n",
      "\n",
      "                IBM Research® data management publications\n",
      "            \n",
      "Explore how IBM Research is regularly integrated into new features for IBM Cloud Pak® for data.\n",
      "\n",
      "Explore articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Report\n",
      "            \n",
      "\n",
      "                Gartner® predicts 2024: How AI will impact analytics users\n",
      "            \n",
      "Gain unique insights into the evolving landscape of ABI solutions, highlighting key findings, assumptions and recommendations for data and analytics leaders.\n",
      "\n",
      "Read the report\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        \n",
      "        \n",
      "\n",
      "     \n",
      "    Related solutions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "    Data management software and solutions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "Design a data strategy that eliminates data silos, reduces complexity and improves data quality for exceptional customer and employee experiences.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Discover data management solutions\n",
      "                \n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "    IBM® watsonx.data™\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "Watsonx.data enables you to scale analytics and AI with all your data, wherever it resides, through an open, hybrid and governed data store.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Discover watsonx.data\n",
      "                \n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "    Data and analytics consulting services\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "Unlock the value of enterprise data with IBM Consulting®, building an insight-driven organization that delivers business advantage.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Discover analytics services\n",
      "                \n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Take the next step\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Unify all your data for AI and analytics with IBM® watsonx.data™. Put your data to work, wherever it resides, with the hybrid, open data lakehouse for AI and analytics.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Discover watsonx.data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Explore data management solutions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "What Is Data Management? | Oracle\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accessibility Policy\n",
      "Skip to content\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "About\n",
      "Services\n",
      "Solutions\n",
      "Pricing\n",
      "Partners\n",
      "Resources\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Close Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search Oracle.com\n",
      "\n",
      "\n",
      "QUICK LINKS\n",
      "Oracle Cloud Infrastructure\n",
      "Oracle Fusion Cloud Applications\n",
      "Oracle Database\n",
      "Download Java\n",
      "Careers at Oracle\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Menu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Menu\n",
      "\n",
      "\n",
      "Contact Sales\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sign in to Oracle Cloud\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Database\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "What Is Business Intelligence?\n",
      "May 14, 2021\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In This Article\n",
      "\n",
      "Data Management, Defined\n",
      "Data Management Systems Today\n",
      "Big Data Management Systems\n",
      "Data Management Challenges\n",
      "Data Management Best Practices\n",
      "Data Management Evolves\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data Management, Defined\n",
      "Data management is the practice of collecting, keeping, and using data securely, efficiently, and cost-effectively. The goal of data management is to help people, organizations, and connected things optimize the use of data within the bounds of policy and regulation so that they can make decisions and take actions that maximize the benefit to the organization. A robust data management strategy is becoming more important than ever as organizations increasingly rely on intangible assets to create value.\n",
      "  \n",
      "\n",
      "Managing digital data in an organization involves a broad range of tasks, policies, procedures, and practices. The work of data management has a wide scope, covering factors such as how to:\n",
      "\n",
      "Create, access, and update data across a diverse data tier\n",
      "Store data across multiple clouds and on premises\n",
      "Provide high availability and disaster recovery\n",
      "Use data in a growing variety of apps, analytics, and algorithms\n",
      "Ensure data privacy and security\n",
      "Archive and destroy data in accordance with retention schedules and compliance requirements\n",
      "\n",
      "A formal data management strategy addresses the activity of users and administrators, the capabilities of data management technologies, the demands of regulatory requirements, and the needs of the organization to obtain value from its data.\n",
      "Data Capital Is Business Capital\n",
      "In today’s digital economy, data is a kind of capital, an economic factor of production in digital goods and services. Just as an automaker can’t manufacture a new model if it lacks the necessary financial capital, it can’t make its cars autonomous if it lacks the data to feed the onboard algorithms. This new role for data has implications for competitive strategy as well as for the future of computing.\n",
      "Given this central and mission-critical role of data, strong management practices and a robust management system are essential for every organization, regardless of size or type.\n",
      "\n",
      "Learn more about The Rise of Data Capital (PDF)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data Management Systems Today\n",
      "Today’s organizations need a data management solution that provides an efficient way to manage data across a diverse but unified data tier. Data management systems are built on data management platforms and can include databases, data lakes and data warehouses, big data management systems, data analytics, and more. \n",
      "All these components work together as a “data utility” to deliver the data management capabilities an organization needs for its apps, and the analytics and algorithms that use the data originated by those apps. Although current tools help database administrators (DBAs) automate many of the traditional management tasks, manual intervention is still often required because of the size and complexity of most database deployments. Whenever manual intervention is required, the chance for errors increases. Reducing the need for manual data management is a key objective of a new data management technology, the autonomous database.\n",
      "  \n",
      "\n",
      "Data Management Platforms\n",
      "The most critical step for continuous delivery of software is continuous integration (CI). CI is a development practice where developers commit their code changes (usually small and incremental) to a centralized source repository, which kicks off a set of automated builds and tests. This repository allows developers to capture the bugs early and automatically before passing them on to production. Continuous Integration pipeline usually involves a series of steps, starting from code commit to performing basic automated linting/static analysis, capturing dependencies, and finally building the software and performing some basic unit tests before creating a build artifact. Source code management systems like Github, Gitlab, etc., offer webhooks integration to which CI tools like Jenkins can subscribe to start running automated builds and tests after each code check-in.\n",
      "A data management platform is the foundational system for collecting and analyzing large volumes of data across an organization. Commercial data platforms typically include software tools for management, developed by the database vendor or by third-party vendors. These data management solutions help IT teams and DBAs perform typical tasks such as:\n",
      "\n",
      "Identifying, alerting, diagnosing, and resolving faults in the database system or underlying infrastructure\n",
      "Allocating database memory and storage resources\n",
      "Making changes in the database design\n",
      "Optimizing responses to database queries for faster application performance\n",
      "\n",
      "The increasingly popular cloud database platforms allow businesses to scale up or down quickly and cost-effectively. Some are available as a service, allowing organizations to save even more.\n",
      "\n",
      "Learn more about agile, flexible, and secure data management \n",
      "Learn more about data management platforms in the cloud (PDF)\n",
      "\n",
      "\n",
      "What is an Autonomous Database\n",
      "Based in the cloud, an autonomous database uses artificial intelligence (AI) and machine learning to automate many data management tasks performed by DBAs, including managing database backups, security, and performance tuning.\n",
      "Also called a self-driving database, an autonomous database offers significant benefits for data management, including:\n",
      "\n",
      "Reduced complexity\n",
      "Decreased potential for human error\n",
      "Higher database reliability and security\n",
      "Improved operational efficiency\n",
      "Lower costs\n",
      "\n",
      "The increasingly popular cloud data platforms allow businesses to scale up or down quickly and cost-effectively. Some are available as a service, allowing organizations to save even more. \n",
      "\n",
      "Learn more about autonomous databases\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Big Data Management Systems\n",
      "In some ways, big data is just what it sounds like—lots and lots of data. But big data also comes in a wider variety of forms than traditional data, and it’s collected at a high rate of speed. Think of all the data that comes in every day, or every minute, from a social media source such as Facebook. The amount, variety, and speed of that data are what make it so valuable to businesses, but they also make it very complex to manage.\n",
      "As more and more data is collected from sources as disparate as video cameras, social media, audio recordings, and Internet of Things (IoT) devices, big data management systems have emerged. These systems specialize in three general areas.\n",
      "\n",
      "Big data integration brings in different types of data—from batch to streaming—and transforms it so that it can be consumed.\n",
      "Big data management stores and processes data in a data lake or data warehouse efficiently, securely, and reliably, often by using object storage.\n",
      "Big data analysis uncovers new insights with analytics, including graph analytics, and uses machine learning and AI visualization to build models.\n",
      "\n",
      "Companies are using big data to improve and accelerate product development, predictive maintenance, the customer experience, security, operational efficiency, and much more. As big data gets bigger, so will the opportunities.\n",
      "\n",
      "Learn more about big data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data Management Challenges\n",
      "Most of the challenges in data management today stem from the faster pace of business and the increasing proliferation of data. The ever-expanding variety, velocity, and volume of data available to organizations is pushing them to seek more-effective management tools to keep up. Some of the top challenges organizations face include the following:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lack of data insight\n",
      "Data from an increasing number and variety of sources such as sensors, smart devices, social media, and video cameras is being collected and stored. But none of that data is useful if the organization doesn’t know what data it has, where it is, and how to use it. Data management solutions need scale and performance to   deliver meaningful insights in a timely manner.\n",
      "\n",
      "\n",
      "Difficulty maintaining data-management performance levels \n",
      "Organizations are capturing, storing, and using more data all the time. To maintain peak response times across this expanding tier, organizations need to continuously monitor the type of questions the database is answering and change the indexes as the queries change—without affecting performance.\n",
      "\n",
      "\n",
      "Challenges complying with changing data requirements\n",
      "Compliance regulations are complex and multijurisdictional, and they change constantly. Organizations need to be able to easily review their data and identify anything that falls under new or modified requirements. In particular, personally identifiable information (PII) must be detected, tracked, and monitored for compliance with increasingly strict global privacy regulations.\n",
      "\n",
      "\n",
      "Need to easily process and convert data\n",
      "Collecting and identifying the data itself doesn’t provide any value—the organization needs to process it. If it takes a lot of time and effort to convert the data into what they need for analysis, that analysis won’t happen. As a result, the potential value of that data is lost.\n",
      "\n",
      "\n",
      "Constant need to store data effectively\n",
      "In the new world of data management, organizations store data in multiple systems, including data warehouses and unstructured data lakes that store any data in any format in a single repository. An organization’s data scientists need a way to quickly and easily transform data from its original format into the shape, format, or model they need it to be in for a wide array of analyses.\n",
      "\n",
      "\n",
      "Demand to continually optimize IT agility and costs\n",
      "With the availability of cloud data management systems, organizations can now choose whether keep and analyze data in on-premises environments, in the cloud, or in a hybrid mixture of the two. IT organizations need to evaluate the level of identicality between on-premises and cloud environments in order to maintain maximum IT agility and lower costs.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data Management Principles and Data Privacy\n",
      "The General Data Protection Regulation (GDPR) enacted by the European Union and implemented in May 2018 includes seven key principles for the management and processing of personal data. These principles include lawfulness, fairness, and transparency; purpose limitation; accuracy; storage limitation; integrity and confidentiality; and more.\n",
      "The GDPR and other laws that follow in its footsteps, such as the California Consumer Privacy Act (CCPA), are changing the face of data management. These requirements provide standardized data protection laws that give individuals control over their personal data and how it is used. In effect, it turns consumers into data stakeholders with real legal recourse when organizations fail to obtain informed consent at data capture, exercise poor control over data use or locality, or fail to comply with data erasure or portability requirements.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data Management Best Practices\n",
      "Addressing data management challenges requires a comprehensive, well-thought-out set of best practices. Although specific best practices vary depending on the type of data involved and the industry, the following best practices address the major data management challenges organizations face today:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Create a discovery layer to identify your data\n",
      "A discovery layer on top of your organization’s data tier allows analysts and data scientists to search and browse for datasets to make your data useable.\n",
      "\n",
      "\n",
      "Develop a data science environment to efficiently repurpose your data\n",
      "A data science environment automates as much of the data transformation work as possible, streamlining the creation and evaluation of data models. A set of tools that eliminates the need for the manual transformation of data can expedite the hypothesizing and testing of new models.\n",
      "\n",
      "\n",
      "Use autonomous technology to maintain performance levels across your expanding data tier\n",
      "Autonomous data capabilities use AI and machine learning to continuously monitor database queries and optimize indexes as the queries change. This allows the database to maintain rapid response times and frees DBAs and data scientists from time-consuming manual tasks.\n",
      "\n",
      "\n",
      "Use discovery to stay on top of compliance requirements\n",
      "New tools use data discovery to review data and identify the chains of connection that need to be detected, tracked, and monitored for multijurisdictional compliance. As compliance demands increase globally, this capability is going to be increasingly important to risk and security officers.\n",
      "\n",
      "\n",
      "Ensure you’re using a converged database\n",
      "A converged database is a database that has native support for all modern data types and the latest development models built into one product. The best converged databases can run many kinds of workloads, including graph, IoT, blockchain, and machine learning.\n",
      "\n",
      "\n",
      "Ensure your database platform has the performance, scale, and availability to support your business\n",
      "The goal of bringing data together is to be able to analyze it to make better, more timely decisions. A scalable, high-performance database platform allows enterprises to rapidly analyze data from multiple sources using advanced analytics and machine learning so they can make better business decisions. \n",
      "\n",
      "\n",
      "Use a common query layer to manage multiple and diverse forms of data storage\n",
      "New technologies are enabling data management repositories to work together, making the differences between them disappear. A common query layer that spans the many kinds of data storage enables data scientists, analysts, and applications to access data without needing to know where it is stored and without needing to manually transform it into a usable format.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Value of a Data Science Environment\n",
      "Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract value from data. Data scientists combine a range of skills—including statistics, computer science, and business knowledge—to analyze data collected from the web, smartphones, customers, sensors, and other sources.\n",
      "\n",
      "Learn more about data science\n",
      "Learn how to make a bigger impact with a data science platform\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data Management Evolves\n",
      "With data’s new role as business capital, organizations are discovering what digital startups and disruptors already know: Data is a valuable asset for identifying trends, making decisions, and taking action before competitors. The new position of data in the value chain is leading organizations to actively seek better ways to derive value from this new capital.\n",
      "Learn more about what the best data management can do for you, including the benefits of an autonomous strategy in the cloud and scalable, high performance database cloud capabilities.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data management related products\n",
      "\n",
      "Oracle Autonomous Database\n",
      "World’s first self-driving database\n",
      "Oracle Database\n",
      "World's leading converged, multi-model database management system\n",
      "Oracle Exadata\n",
      "Unmatched Oracle Database performance, scale, and availability\n",
      "Oracle Autonomous Data Warehouse\n",
      "Data warehousing without complexity\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Resources for\n",
      "\n",
      "Careers\n",
      "Developers\n",
      "Investors\n",
      "Partners\n",
      "Startups\n",
      "Students and Educators\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Why Oracle\n",
      "\n",
      "Analyst Reports\n",
      "Cloud Economics\n",
      "with Microsoft Azure\n",
      "vs. AWS\n",
      "vs. Google Cloud\n",
      "vs. MongoDB\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Learn\n",
      "\n",
      "What is AI?\n",
      "What is Cloud Computing?\n",
      "What is Cloud Storage?\n",
      "What is HPC?\n",
      "What is IaaS?\n",
      "What is PaaS?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "What’s new\n",
      "\n",
      "Oracle Supports Ukraine\n",
      "Oracle Cloud Free Tier\n",
      "Cloud Architecture Center\n",
      "Cloud Lift\n",
      "Oracle Support Rewards\n",
      "Oracle Red Bull Racing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contact us\n",
      "\n",
      "US Sales: +1.800.633.0738\n",
      "How can we help?\n",
      "Subscribe to emails\n",
      "Events\n",
      "News\n",
      "OCI Blog\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Country/Region \n",
      "\n",
      "\n",
      "\n",
      "© 2025 Oracle\n",
      "Privacy/Do Not Sell My Info\n",
      "\n",
      "\n",
      "\n",
      "Ad Choices\n",
      "Careers\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Facebook\n",
      "Twitter\n",
      "LinkedIn\n",
      "YouTube\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Quickstart - Pinecone DocsPinecone Docs home pageSearch...⌘KStatusSupportLog InSign up freeSign up freeSearch...NavigationGet startedQuickstartGuidesReferenceExamplesModelsIntegrationsTroubleshootingReleasesPinecone DatabaseGet startedOverviewQuickstartConceptsArchitectureIndex dataOverviewCreate an indexData modelingData ingestionImplement multitenancyDedicated read capacitySearchOverviewSemantic searchLexical searchHybrid searchFilter by metadataRerank resultsOptimizeIncrease relevanceIncrease throughputDecrease latencyManage dataTarget an indexManage indexesManage namespacesManage backupsUpdate recordsDelete recordsFetch recordsList record IDsManage costUnderstanding costManage costMonitor usage and costsMove to productionOverviewEnforce securityMonitor performanceCI/CDAdminManage billingManage organizationsManage projectsOperationsIntegrate with cloud storageIntegrate with AI agentsLocal developmentUsing podsOverviewMigrate a pod-based index to serverlessChoose a pod typeCreate a pod-based indexManage pod-based indexesScale pod-based indexesBack up and restoreOn this page1. Sign up2. Install an SDK3. Create an index4. Upsert text5. Semantic search6. Rerank results7. Improve results8. Clean upNext stepsGet startedQuickstartCopy pageCopy pageThis guide shows you how to set up and use Pinecone Database for high-performance semantic search.\n",
      "To get started in your browser, use the Quickstart colab notebook.\n",
      "​1. Sign up\n",
      "If you’re new to Pinecone, sign up at app.pinecone.io and choose a free plan:\n",
      "\n",
      "\n",
      "Starter plan: If you’re building a small or personal project, this is likely the right choice. You get free access to most features, but you’re limited to one cloud region and need to stay under Starter plan limits.\n",
      "\n",
      "\n",
      "Standard plan trial: If you’re building for scale or commercial use, choose this option. You get 21 days and $300 in credits with access to Standard plan features and higher limits that let you test Pinecone at scale.\n",
      "\n",
      "\n",
      "After signing up, you’ll receive an API key in the console. Save this key. You’ll need it to authenticate your requests to Pinecone.\n",
      "​2. Install an SDK\n",
      "Pinecone SDKs provide convenient programmatic access to the Pinecone APIs.\n",
      "Install the SDK for your preferred language:\n",
      "PythonJavaScriptJavaGoC#Copypip install pinecone\n",
      "\n",
      "​3. Create an index\n",
      "In Pinecone, there are two types of indexes for storing vector data: Dense indexes store dense vectors for semantic search, and sparse indexes store sparse vectors for lexical/keyword search.\n",
      "For this quickstart, create a dense index that is integrated with an embedding model hosted by Pinecone. With integrated models, you upsert and search with text and have Pinecone generate vectors automatically.\n",
      "If you prefer to use external embedding models, see Bring your own vectors.\n",
      "PythonJavaScriptJavaGoC#Copy# Import the Pinecone library\n",
      "from pinecone import Pinecone\n",
      "\n",
      "# Initialize a Pinecone client with your API key\n",
      "pc = Pinecone(api_key=\"YOUR_API_KEY\")\n",
      "\n",
      "# Create a dense index with integrated embedding\n",
      "index_name = \"quickstart-py\"\n",
      "if not pc.has_index(index_name):\n",
      "    pc.create_index_for_model(\n",
      "        name=index_name,\n",
      "        cloud=\"aws\",\n",
      "        region=\"us-east-1\",\n",
      "        embed={\n",
      "            \"model\":\"llama-text-embed-v2\",\n",
      "            \"field_map\":{\"text\": \"chunk_text\"}\n",
      "        }\n",
      "    )\n",
      "\n",
      "​4. Upsert text\n",
      "Prepare a sample dataset of factual statements from different domains like history, physics, technology, and music. Model the data as as records with an ID, text, and category.\n",
      "PythonJavaScriptJavaGoC#Copyrecords = [\n",
      "    { \"_id\": \"rec1\", \"chunk_text\": \"The Eiffel Tower was completed in 1889 and stands in Paris, France.\", \"category\": \"history\" },\n",
      "    { \"_id\": \"rec2\", \"chunk_text\": \"Photosynthesis allows plants to convert sunlight into energy.\", \"category\": \"science\" },\n",
      "    { \"_id\": \"rec3\", \"chunk_text\": \"Albert Einstein developed the theory of relativity.\", \"category\": \"science\" },\n",
      "    { \"_id\": \"rec4\", \"chunk_text\": \"The mitochondrion is often called the powerhouse of the cell.\", \"category\": \"biology\" },\n",
      "    { \"_id\": \"rec5\", \"chunk_text\": \"Shakespeare wrote many famous plays, including Hamlet and Macbeth.\", \"category\": \"literature\" },\n",
      "    { \"_id\": \"rec6\", \"chunk_text\": \"Water boils at 100°C under standard atmospheric pressure.\", \"category\": \"physics\" },\n",
      "    { \"_id\": \"rec7\", \"chunk_text\": \"The Great Wall of China was built to protect against invasions.\", \"category\": \"history\" },\n",
      "    { \"_id\": \"rec8\", \"chunk_text\": \"Honey never spoils due to its low moisture content and acidity.\", \"category\": \"food science\" },\n",
      "    { \"_id\": \"rec9\", \"chunk_text\": \"The speed of light in a vacuum is approximately 299,792 km/s.\", \"category\": \"physics\" },\n",
      "    { \"_id\": \"rec10\", \"chunk_text\": \"Newton's laws describe the motion of objects.\", \"category\": \"physics\" },\n",
      "    { \"_id\": \"rec11\", \"chunk_text\": \"The human brain has approximately 86 billion neurons.\", \"category\": \"biology\" },\n",
      "    { \"_id\": \"rec12\", \"chunk_text\": \"The Amazon Rainforest is one of the most biodiverse places on Earth.\", \"category\": \"geography\" },\n",
      "    { \"_id\": \"rec13\", \"chunk_text\": \"Black holes have gravitational fields so strong that not even light can escape.\", \"category\": \"astronomy\" },\n",
      "    { \"_id\": \"rec14\", \"chunk_text\": \"The periodic table organizes elements based on their atomic number.\", \"category\": \"chemistry\" },\n",
      "    { \"_id\": \"rec15\", \"chunk_text\": \"Leonardo da Vinci painted the Mona Lisa.\", \"category\": \"art\" },\n",
      "    { \"_id\": \"rec16\", \"chunk_text\": \"The internet revolutionized communication and information sharing.\", \"category\": \"technology\" },\n",
      "    { \"_id\": \"rec17\", \"chunk_text\": \"The Pyramids of Giza are among the Seven Wonders of the Ancient World.\", \"category\": \"history\" },\n",
      "    { \"_id\": \"rec18\", \"chunk_text\": \"Dogs have an incredible sense of smell, much stronger than humans.\", \"category\": \"biology\" },\n",
      "    { \"_id\": \"rec19\", \"chunk_text\": \"The Pacific Ocean is the largest and deepest ocean on Earth.\", \"category\": \"geography\" },\n",
      "    { \"_id\": \"rec20\", \"chunk_text\": \"Chess is a strategic game that originated in India.\", \"category\": \"games\" },\n",
      "    { \"_id\": \"rec21\", \"chunk_text\": \"The Statue of Liberty was a gift from France to the United States.\", \"category\": \"history\" },\n",
      "    { \"_id\": \"rec22\", \"chunk_text\": \"Coffee contains caffeine, a natural stimulant.\", \"category\": \"food science\" },\n",
      "    { \"_id\": \"rec23\", \"chunk_text\": \"Thomas Edison invented the practical electric light bulb.\", \"category\": \"inventions\" },\n",
      "    { \"_id\": \"rec24\", \"chunk_text\": \"The moon influences ocean tides due to gravitational pull.\", \"category\": \"astronomy\" },\n",
      "    { \"_id\": \"rec25\", \"chunk_text\": \"DNA carries genetic information for all living organisms.\", \"category\": \"biology\" },\n",
      "    { \"_id\": \"rec26\", \"chunk_text\": \"Rome was once the center of a vast empire.\", \"category\": \"history\" },\n",
      "    { \"_id\": \"rec27\", \"chunk_text\": \"The Wright brothers pioneered human flight in 1903.\", \"category\": \"inventions\" },\n",
      "    { \"_id\": \"rec28\", \"chunk_text\": \"Bananas are a good source of potassium.\", \"category\": \"nutrition\" },\n",
      "    { \"_id\": \"rec29\", \"chunk_text\": \"The stock market fluctuates based on supply and demand.\", \"category\": \"economics\" },\n",
      "    { \"_id\": \"rec30\", \"chunk_text\": \"A compass needle points toward the magnetic north pole.\", \"category\": \"navigation\" },\n",
      "    { \"_id\": \"rec31\", \"chunk_text\": \"The universe is expanding, according to the Big Bang theory.\", \"category\": \"astronomy\" },\n",
      "    { \"_id\": \"rec32\", \"chunk_text\": \"Elephants have excellent memory and strong social bonds.\", \"category\": \"biology\" },\n",
      "    { \"_id\": \"rec33\", \"chunk_text\": \"The violin is a string instrument commonly used in orchestras.\", \"category\": \"music\" },\n",
      "    { \"_id\": \"rec34\", \"chunk_text\": \"The heart pumps blood throughout the human body.\", \"category\": \"biology\" },\n",
      "    { \"_id\": \"rec35\", \"chunk_text\": \"Ice cream melts when exposed to heat.\", \"category\": \"food science\" },\n",
      "    { \"_id\": \"rec36\", \"chunk_text\": \"Solar panels convert sunlight into electricity.\", \"category\": \"technology\" },\n",
      "    { \"_id\": \"rec37\", \"chunk_text\": \"The French Revolution began in 1789.\", \"category\": \"history\" },\n",
      "    { \"_id\": \"rec38\", \"chunk_text\": \"The Taj Mahal is a mausoleum built by Emperor Shah Jahan.\", \"category\": \"history\" },\n",
      "    { \"_id\": \"rec39\", \"chunk_text\": \"Rainbows are caused by light refracting through water droplets.\", \"category\": \"physics\" },\n",
      "    { \"_id\": \"rec40\", \"chunk_text\": \"Mount Everest is the tallest mountain in the world.\", \"category\": \"geography\" },\n",
      "    { \"_id\": \"rec41\", \"chunk_text\": \"Octopuses are highly intelligent marine creatures.\", \"category\": \"biology\" },\n",
      "    { \"_id\": \"rec42\", \"chunk_text\": \"The speed of sound is around 343 meters per second in air.\", \"category\": \"physics\" },\n",
      "    { \"_id\": \"rec43\", \"chunk_text\": \"Gravity keeps planets in orbit around the sun.\", \"category\": \"astronomy\" },\n",
      "    { \"_id\": \"rec44\", \"chunk_text\": \"The Mediterranean diet is considered one of the healthiest in the world.\", \"category\": \"nutrition\" },\n",
      "    { \"_id\": \"rec45\", \"chunk_text\": \"A haiku is a traditional Japanese poem with a 5-7-5 syllable structure.\", \"category\": \"literature\" },\n",
      "    { \"_id\": \"rec46\", \"chunk_text\": \"The human body is made up of about 60% water.\", \"category\": \"biology\" },\n",
      "    { \"_id\": \"rec47\", \"chunk_text\": \"The Industrial Revolution transformed manufacturing and transportation.\", \"category\": \"history\" },\n",
      "    { \"_id\": \"rec48\", \"chunk_text\": \"Vincent van Gogh painted Starry Night.\", \"category\": \"art\" },\n",
      "    { \"_id\": \"rec49\", \"chunk_text\": \"Airplanes fly due to the principles of lift and aerodynamics.\", \"category\": \"physics\" },\n",
      "    { \"_id\": \"rec50\", \"chunk_text\": \"Renewable energy sources include wind, solar, and hydroelectric power.\", \"category\": \"energy\" }\n",
      "]\n",
      "See all 52 lines\n",
      "Upsert the sample dataset into a new namespace in your index.\n",
      "Because your index is integrated with an embedding model, you provide the textual statements and Pinecone converts them to dense vectors automatically.\n",
      "PythonJavaScriptJavaGoC#Copy# Target the index\n",
      "dense_index = pc.Index(index_name)\n",
      "\n",
      "# Upsert the records into a namespace\n",
      "dense_index.upsert_records(\"example-namespace\", records)\n",
      "\n",
      "To control costs when ingesting large datasets (10,000,000+ records), use import instead of upsert.\n",
      "Pinecone is eventually consistent, so there can be a slight delay before new or changed records are visible to queries. You can view index stats to check if the current vector count matches the number of vectors you upserted (50):\n",
      "PythonJavaScriptJavaGoC#Copy# Wait for the upserted vectors to be indexed\n",
      "import time\n",
      "time.sleep(10)\n",
      "\n",
      "# View stats for the index\n",
      "stats = dense_index.describe_index_stats()\n",
      "print(stats)\n",
      "\n",
      "The response looks like this:\n",
      "PythonJavaScriptJavaGoC#Copy{'dimension': 1024,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'example-namespace': {'vector_count': 50}},\n",
      " 'total_vector_count': 50,\n",
      " 'vector_type': 'dense'}\n",
      "\n",
      "​5. Semantic search\n",
      "Search the dense index for ten records that are most semantically similar to the query, “Famous historical structures and monuments”.\n",
      "Again, because your index is integrated with an embedding model, you provide the query as text and Pinecone converts the text to a dense vector automatically.\n",
      "PythonJavaScriptJavaGoC#Copy# Define the query\n",
      "query = \"Famous historical structures and monuments\"\n",
      "\n",
      "# Search the dense index\n",
      "results = dense_index.search(\n",
      "    namespace=\"example-namespace\",\n",
      "    query={\n",
      "        \"top_k\": 10,\n",
      "        \"inputs\": {\n",
      "            'text': query\n",
      "        }\n",
      "    }\n",
      ")\n",
      "\n",
      "# Print the results\n",
      "for hit in results['result']['hits']:\n",
      "        print(f\"id: {hit['_id']:<5} | score: {round(hit['_score'], 2):<5} | category: {hit['fields']['category']:<10} | text: {hit['fields']['chunk_text']:<50}\")\n",
      "\n",
      "Notice that most of the results are about historical structures and monuments. However, a few unrelated statements are included as well and are ranked high in the list, for example, a statement about Shakespeare.\n",
      "PythonJavaScriptJavaGoC#Copyid: rec17 | score: 0.24  | category: history    | text: The Pyramids of Giza are among the Seven Wonders of the Ancient World.\n",
      "id: rec38 | score: 0.19  | category: history    | text: The Taj Mahal is a mausoleum built by Emperor Shah Jahan.\n",
      "id: rec5  | score: 0.19  | category: literature | text: Shakespeare wrote many famous plays, including Hamlet and Macbeth.\n",
      "id: rec15 | score: 0.11  | category: art        | text: Leonardo da Vinci painted the Mona Lisa.          \n",
      "id: rec50 | score: 0.1   | category: energy     | text: Renewable energy sources include wind, solar, and hydroelectric power.\n",
      "id: rec26 | score: 0.09  | category: history    | text: Rome was once the center of a vast empire.        \n",
      "id: rec47 | score: 0.08  | category: history    | text: The Industrial Revolution transformed manufacturing and transportation.\n",
      "id: rec7  | score: 0.07  | category: history    | text: The Great Wall of China was built to protect against invasions.\n",
      "id: rec1  | score: 0.07  | category: history    | text: The Eiffel Tower was completed in 1889 and stands in Paris, France.\n",
      "id: rec3  | score: 0.07  | category: science    | text: Albert Einstein developed the theory of relativity.\n",
      "\n",
      "​6. Rerank results\n",
      "To get a more accurate ranking, search again but this time rerank the initial results based on their relevance to the query.\n",
      "PythonJavaScriptJavaGoC#Copy# Search the dense index and rerank results\n",
      "reranked_results = dense_index.search(\n",
      "    namespace=\"example-namespace\",\n",
      "    query={\n",
      "        \"top_k\": 10,\n",
      "        \"inputs\": {\n",
      "            'text': query\n",
      "        }\n",
      "    },\n",
      "    rerank={\n",
      "        \"model\": \"bge-reranker-v2-m3\",\n",
      "        \"top_n\": 10,\n",
      "        \"rank_fields\": [\"chunk_text\"]\n",
      "    }   \n",
      ")\n",
      "\n",
      "# Print the reranked results\n",
      "for hit in reranked_results['result']['hits']:\n",
      "    print(f\"id: {hit['_id']}, score: {round(hit['_score'], 2)}, text: {hit['fields']['chunk_text']}, category: {hit['fields']['category']}\")\n",
      "\n",
      "Notice that all of the most relevant results about historical structures and monuments are now ranked highest.\n",
      "PythonJavaScriptJavaGoC#Copyid: rec1  | score: 0.11  | category: history    | text: The Eiffel Tower was completed in 1889 and stands in Paris, France.\n",
      "id: rec38 | score: 0.06  | category: history    | text: The Taj Mahal is a mausoleum built by Emperor Shah Jahan.\n",
      "id: rec7  | score: 0.06  | category: history    | text: The Great Wall of China was built to protect against invasions.\n",
      "id: rec17 | score: 0.02  | category: history    | text: The Pyramids of Giza are among the Seven Wonders of the Ancient World.\n",
      "id: rec26 | score: 0.01  | category: history    | text: Rome was once the center of a vast empire.        \n",
      "id: rec15 | score: 0.01  | category: art        | text: Leonardo da Vinci painted the Mona Lisa.          \n",
      "id: rec5  | score: 0.0   | category: literature | text: Shakespeare wrote many famous plays, including Hamlet and Macbeth.\n",
      "id: rec47 | score: 0.0   | category: history    | text: The Industrial Revolution transformed manufacturing and transportation.\n",
      "id: rec50 | score: 0.0   | category: energy     | text: Renewable energy sources include wind, solar, and hydroelectric power.\n",
      "id: rec3  | score: 0.0   | category: science    | text: Albert Einstein developed the theory of relativity.\n",
      "\n",
      "​7. Improve results\n",
      "Reranking results is one of the most effective ways to improve search accuracy and relevance, but there are many other techniques to consider. For example:\n",
      "\n",
      "\n",
      "Filtering by metadata: When records contain additional metadata, you can limit the search to records matching a filter expression.\n",
      "\n",
      "\n",
      "Hybrid search: You can add lexical search to capture precise keyword matches (e.g., product SKUs, email addresses, domain-specific terms) in addition to semantic matches.\n",
      "\n",
      "\n",
      "Chunking strategies: You can chunk your content in different ways to get better results. Consider factors like the length of the content, the complexity of queries, and how results will be used in your application.\n",
      "\n",
      "\n",
      "​8. Clean up\n",
      "When you no longer need your example index, delete it as follows:\n",
      "PythonJavaScriptJavaGoC#Copy# Delete the index\n",
      "pc.delete_index(index_name)\n",
      "\n",
      "For production indexes, consider enabling deletion protection.\n",
      "​Next steps\n",
      "Index dataLearn more about storing data in PineconeSearchExplore different forms of vector search.OptimizeFind out how to improve performanceWas this page helpful?YesNoOverviewConceptstwitterlinkedinAssistantResponses are generated using AI and may contain mistakes.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Quickstart (with cloud resources) | Weaviate Documentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to main contentProduct Update: Meet Weaviate Agents — Read the blogGitHubWeaviate CloudAsk AI or Search⌘KGet StartedHow-to manuals & GuidesModel IntegrationsReference & APIsConceptsRecipesOtherGo to documentation:⌘U✕Weaviate DatabaseDevelop AI applications using Weaviate's APIs and toolsDeployDeploy, configure, and maintain Weaviate DatabaseWeaviate AgentsBuild and deploy intelligent agents with WeaviateWeaviate CloudManage and scale Weaviate in the cloudAdditional resourcesAcademyIntegrationsContributor guideEvents & WorkshopsNeed help?Ask AI Assistant⌘KCommunity ForumIntroductionQuickstartLocally hostedInstallationConnect to WeaviateStarter guidesBest practicesAI-based code generationQuickstartQuickstart (with cloud resources)Expected time: 30 minutes\n",
      "Prerequisites: None\n",
      "\n",
      "\n",
      "What you will learnThis quickstart shows you how to combine Weaviate Cloud and Cohere to:\n",
      "Set up a Weaviate instance. (10 minutes)\n",
      "Add and vectorize your data. (10 minutes)\n",
      "Perform a semantic search and retrieval augmented generation (RAG). (10 minutes)\n",
      "Notes:\n",
      "The code examples here are self-contained. You can copy and paste them into your own environment to try them out.\n",
      "\n",
      "If you prefer to use locally hosted resources, see Quickstart: locally hosted.\n",
      "\n",
      "Requirements​\n",
      "In order to perform Retrieval Augmented Generation (RAG) in the last step, you will need a Cohere account. You can use a free Cohere trial API key.\n",
      "If you have another preferred model provider, you can use that instead of Cohere.\n",
      "\n",
      "Step 1: Set up Weaviate​\n",
      "1.1 Create a Weaviate database​\n",
      "Go to the Weaviate Cloud console and create a free Sandbox instance.\n",
      "\n",
      "\n",
      "note\n",
      "Cluster provisioning typically takes 1-3 minutes.\n",
      "When the cluster is ready, Weaviate Cloud displays a checkmark (✔️) next to the cluster name.\n",
      "Note that Weaviate Cloud adds a random suffix to sandbox cluster names to ensure uniqueness.\n",
      "\n",
      "\n",
      "TIP: Use the latest Weaviate version!When possible, try to use the latest Weaviate version.\n",
      "New releases include cutting-edge features, performance enhancements, and critical security updates to keep your application safe and up-to-date.\n",
      "1.2 Install a client library​\n",
      "We recommend using a client library to work with Weaviate. Follow the instructions below to install one of the official client libraries, available in Python, JavaScript/TypeScript, Go, and Java.\n",
      "\n",
      "PythonJS/TSGoJavaInstall the latest, Python client v4, by adding weaviate-client to your Python environment with pip:pip install -U weaviate-clientInstall the latest, JS/TS client v3, by adding weaviate-client to your project with npm:npm install weaviate-clientAdd weaviate-go-client to your project with go get:go get github.com/weaviate/weaviate-go-client/v5Add this dependency to your project:<dependency>  <groupId>io.weaviate</groupId>  <artifactId>client</artifactId>  <version>4.8.3</version>  <!-- Check latest version: https://github.com/weaviate/java-client --></dependency>\n",
      "1.3 Connect to Weaviate​\n",
      "Now you can connect to your Weaviate instance. You will need the:\n",
      "\n",
      "REST Endpoint URL and the\n",
      "Administrator API Key.\n",
      "\n",
      "You can retrieve them both from the WCD console as shown in the interactive example below.\n",
      "noteNew clusters with Weaviate version v1.30 (or later) have RBAC (Role-Based Access Control) enabled by default. These clusters don't come with API keys, you will need to create an API key yourself and assign it a role (admin, viewer or a custom role).\n",
      "\n",
      "\n",
      "REST vs gRPC endpointsWeaviate supports both REST and gRPC protocols. For Weaviate Cloud deployments, you only need to provide the REST endpoint URL - the client will automatically configure gRPC.\n",
      "Once you have the REST Endpoint URL and the admin API key, you can connect to the Sandbox instance, and work with Weaviate.\n",
      "The example below shows how to connect to Weaviate and perform a basic operation, like checking the cluster status.\n",
      "\n",
      "PythonJS/TSGoJavaCurlquickstart_check_readiness.pyimport weaviatefrom weaviate.classes.init import Authimport os# Best practice: store your credentials in environment variablesweaviate_url = os.environ[\"WEAVIATE_URL\"]weaviate_api_key = os.environ[\"WEAVIATE_API_KEY\"]client = weaviate.connect_to_weaviate_cloud(    cluster_url=weaviate_url,    auth_credentials=Auth.api_key(weaviate_api_key),)print(client.is_ready())  # Should print: `True`client.close()  # Free up resources  API docsquickstart_check_readiness.tsimport weaviate, { WeaviateClient } from 'weaviate-client';// Best practice: store your credentials in environment variablesconst weaviateUrl = process.env.WEAVIATE_URL as string;const weaviateApiKey = process.env.WEAVIATE_API_KEY as string;const client: WeaviateClient = await weaviate.connectToWeaviateCloud(  weaviateUrl, // Replace with your Weaviate Cloud URL  {    authCredentials: new weaviate.ApiKey(weaviateApiKey), // Replace with your Weaviate Cloud API key  });var clientReadiness = await client.isReady();console.log(clientReadiness); // Should return `true`client.close(); // Close the client connection  API docsquickstart/1_check_readiness/main.go// Set these environment variables// WEAVIATE_HOSTNAME            your Weaviate instance hostname// WEAVIATE_API_KEY          your Weaviate instance API keypackage mainimport (    \"context\"    \"fmt\"    \"os\"    \"github.com/weaviate/weaviate-go-client/v5/weaviate\"    \"github.com/weaviate/weaviate-go-client/v5/weaviate/auth\")func main() {    cfg := weaviate.Config{        Host:       os.Getenv(\"WEAVIATE_HOSTNAME\"),        Scheme:     \"https\",        AuthConfig: auth.ApiKey{Value: os.Getenv(\"WEAVIATE_API_KEY\")},    }    client, err := weaviate.NewClient(cfg)    if err != nil {        fmt.Println(err)    }    // Check the connection    ready, err := client.Misc().ReadyChecker().Do(context.Background())    if err != nil {        panic(err)    }    fmt.Printf(\"%v\", ready)}  API docscautionThis client uses the hostname parameter (without the https scheme) instead of a complete URL.quickstart/IsReady.javaimport io.weaviate.client.Config;import io.weaviate.client.WeaviateClient;import io.weaviate.client.WeaviateAuthClient;import io.weaviate.client.base.Result;// Set these environment variables// WEAVIATE_HOSTNAME     Your Weaviate instance hostname// WEAVIATE_API_KEY      Your Weaviate instance API keypublic class IsReady {  public static void main(String[] args) throws Exception {    String host = System.getenv(\"WEAVIATE_HOSTNAME\");    String apiKey = System.getenv(\"WEAVIATE_API_KEY\");    Config config = new Config(\"https\", host);    WeaviateClient client = WeaviateAuthClient.apiKey(config, apiKey);    // check the result    Result<Boolean> result = client.misc().readyChecker().run();    System.out.println(result.getResult());  }}  API docscautionThis client uses the hostname parameter (without the https scheme) instead of a complete URL.# Best practice: store your credentials in environment variables# export WEAVIATE_URL=\"YOUR_INSTANCE_URL\"  # Your Weaviate instance URL# export WEAVIATE_API_KEY=\"YOUR_API_KEY\"   # Your Weaviate instance API keycurl -w \"\\nResponse code: %{http_code}\\n\" \\  -H \"Authorization: Bearer $WEAVIATE_API_KEY\" \\  $WEAVIATE_URL/v1/.well-known/ready# You should see \"Response code: 200\" if the instance is ready\n",
      "If you did not see any errors, you are ready to proceed. We will replace the simple cluster status check with more meaningful operations in the next steps.\n",
      "\n",
      "Step 2: Populate the database​\n",
      "Now, we can populate our database by first defining a collection and then adding data.\n",
      "2.1 Define a collection​\n",
      "What is a collection?A collection is a set of objects that share the same data structure, like a table in relational databases or a collection in NoSQL databases. A collection also includes additional configurations that define how the data objects are stored and indexed.\n",
      "The following example creates a collection called Question with:\n",
      "\n",
      "The Weaviate Embeddings service for creating vectors during ingestion & queries.\n",
      "Cohere generative AI integrations for retrieval augmented generation (RAG).\n",
      "\n",
      "\n",
      "PythonJS/TSGoJavaCurl.Vectors.text2vec_xxx with AutoSchemaDefining a collection with Configure.Vectors.text2vec_xxx() with Python client library 4.16.0-4.16.3 will throw an error if no properties are defined and vectorize_collection_name is not set to True.This is addressed in 4.16.4 of the Weaviate Python client. See this FAQ entry for more details: Invalid properties error in Python client versions 4.16.0 to 4.16.3.Python and JS/TS client - Vectorizer Configuration API ChangesStarting with Weaviate Python client v4.16.0, the vectorizer configuration API has been updated.\n",
      "Starting with Weaviate JS/TS client v3.8.0, the vectorizer configuration API has been updated.Action required: Update to the latest client version and migrate your code to use the new vectorizer configuration API.quickstart_create_collection.pyimport weaviatefrom weaviate.classes.init import Authfrom weaviate.classes.config import Configureimport os# Best practice: store your credentials in environment variablesweaviate_url = os.environ[\"WEAVIATE_URL\"]weaviate_api_key = os.environ[\"WEAVIATE_API_KEY\"]client = weaviate.connect_to_weaviate_cloud(    cluster_url=weaviate_url,                                    # Replace with your Weaviate Cloud URL    auth_credentials=Auth.api_key(weaviate_api_key),             # Replace with your Weaviate Cloud key)questions = client.collections.create(    name=\"Question\",    vector_config=Configure.Vectors.text2vec_weaviate(), # Configure the Weaviate Embeddings integration    generative_config=Configure.Generative.cohere()             # Configure the Cohere generative AI integration)client.close()  # Free up resources  API docsquickstart_create_collection.tsimport weaviate, { WeaviateClient, vectors, generative } from 'weaviate-client';// Best practice: store your credentials in environment variablesconst weaviateUrl = process.env.WEAVIATE_URL as string;const weaviateApiKey = process.env.WEAVIATE_API_KEY as string;const client: WeaviateClient = await weaviate.connectToWeaviateCloud(  weaviateUrl, // Replace with your Weaviate Cloud URL  {    authCredentials: new weaviate.ApiKey(weaviateApiKey), // Replace with your Weaviate Cloud API key  });await client.collections.create({  name: 'Question',  vectorizers: vectors.text2VecWeaviate(),  generative: generative.cohere(),});client.close(); // Close the client connection  API docsquickstart/2_1_create_collection/main.go// Set these environment variables// WEAVIATE_HOSTNAME            your Weaviate instance hostname// WEAVIATE_API_KEY          your Weaviate instance API key// COHERE_APIKEY           your Cohere API keypackage mainimport (    \"context\"    \"fmt\"    \"os\"    \"github.com/weaviate/weaviate-go-client/v5/weaviate\"    \"github.com/weaviate/weaviate-go-client/v5/weaviate/auth\"    \"github.com/weaviate/weaviate/entities/models\")func main() {    cfg := weaviate.Config{        Host:       os.Getenv(\"WEAVIATE_HOSTNAME\"),        Scheme:     \"https\",        AuthConfig: auth.ApiKey{Value: os.Getenv(\"WEAVIATE_API_KEY\")},    }    client, err := weaviate.NewClient(cfg)    if err != nil {        fmt.Println(err)    }    // Define the collection    classObj := &models.Class{        Class:      \"Question\",        Vectorizer: \"text2vec-weaviate\",        ModuleConfig: map[string]interface{}{            \"text2vec-weaviate\": map[string]interface{}{},            \"generative-cohere\": map[string]interface{}{},        },    }    // add the collection    err = client.Schema().ClassCreator().WithClass(classObj).Do(context.Background())    if err != nil {        panic(err)    }}  API docsquickstart/CreateCollection.javaimport io.weaviate.client.Config;import io.weaviate.client.WeaviateAuthClient;import io.weaviate.client.WeaviateClient;import io.weaviate.client.base.Result;import io.weaviate.client.v1.schema.model.WeaviateClass;import java.util.HashMap;import java.util.Map;// Set these environment variables// WEAVIATE_HOSTNAME     Your Weaviate instance hostname// WEAVIATE_API_KEY      Your Weaviate instance API keypublic class CreateCollection {  public static void main(String[] args) throws Exception {    String host = System.getenv(\"WEAVIATE_HOSTNAME\");    String apiKey = System.getenv(\"WEAVIATE_API_KEY\");    Config config = new Config(\"https\", host);    WeaviateClient client = WeaviateAuthClient.apiKey(config, apiKey);    Map<String, Object> text2vecWeaviateSettings = new HashMap<>();    Map<String, Object> generativeCohereSettings = new HashMap<>();    Map<String, Object> moduleConfig = new HashMap<>();    moduleConfig.put(\"text2vec-weaviate\", text2vecWeaviateSettings);    moduleConfig.put(\"generative-cohere\", generativeCohereSettings);    // Create the collection \"Question\"    WeaviateClass clazz = WeaviateClass.builder()      .className(\"Question\")      .vectorizer(\"text2vec-weaviate\")      .moduleConfig(moduleConfig)      .build();    // Review the response    Result<Boolean> result = client.schema().classCreator().withClass(clazz).run();    System.out.println(result);    Result<WeaviateClass> collectionDefinition = client.schema().classGetter().withClassName(\"Question\").run();    System.out.println(collectionDefinition.getResult());  }}  API docs# Best practice: store your credentials in environment variables# export WEAVIATE_URL=\"YOUR_INSTANCE_URL\"  # Your Weaviate instance URL# export WEAVIATE_API_KEY=\"YOUR_API_KEY\"   # Your Weaviate instance API keycurl -X POST \\-H \"Content-Type: application/json\" \\-H \"Authorization: Bearer $WEAVIATE_API_KEY\" \\-d '{  \"class\": \"Question\",  \"vectorizer\": \"text2vec-weaviate\",  \"moduleConfig\": {    \"text2vec-weaviate\": {},    \"generative-cohere\": {}  }}' \\\"$WEAVIATE_URL/v1/schema\"\n",
      "Run this code to create the collection to which you can add data.\n",
      "What models are being used?You can optionally specify the model in the collection definition. As we did not specify models in the collection definition above, these integrations will use the Weaviate-defined default models.See the model providers integration section for more information.\n",
      "Do you prefer a different setup?Weaviate is very flexible. If you prefer a different model provider integration, or prefer to import your own vectors, see one of the following guides:Prefer a different model provider?See this section for information on how to user another provider, such as AWS, Cohere, Google, and many more.Want to specify object vectors?If you prefer to add vectors yourself along with the object data, see Starter Guide: Bring Your Own Vectors.\n",
      "2.2 Add objects​\n",
      "We can now add data to our collection.\n",
      "The following example:\n",
      "\n",
      "Loads objects, and\n",
      "Adds objects to the target collection (Question) using a batch process.\n",
      "\n",
      "Batch imports(Batch imports) are the most efficient way to add large amounts of data, as it sends multiple objects in a single request. See the How-to: Batch import guide for more information.\n",
      "\n",
      "PythonJS/TSGoJavaCurlquickstart_import.pyimport weaviatefrom weaviate.classes.init import Authimport requests, json, os# Best practice: store your credentials in environment variablesweaviate_url = os.environ[\"WEAVIATE_URL\"]weaviate_api_key = os.environ[\"WEAVIATE_API_KEY\"]client = weaviate.connect_to_weaviate_cloud(    cluster_url=weaviate_url,                                    # Replace with your Weaviate Cloud URL    auth_credentials=Auth.api_key(weaviate_api_key),             # Replace with your Weaviate Cloud key)resp = requests.get(    \"https://raw.githubusercontent.com/weaviate-tutorials/quickstart/main/data/jeopardy_tiny.json\")data = json.loads(resp.text)questions = client.collections.get(\"Question\")with questions.batch.fixed_size(batch_size=200) as batch:    for d in data:        batch.add_object(            {                \"answer\": d[\"Answer\"],                \"question\": d[\"Question\"],                \"category\": d[\"Category\"],            }        )        if batch.number_errors > 10:            print(\"Batch import stopped due to excessive errors.\")            breakfailed_objects = questions.batch.failed_objectsif failed_objects:    print(f\"Number of failed imports: {len(failed_objects)}\")    print(f\"First failed object: {failed_objects[0]}\")client.close()  # Free up resources  API docsDuring a batch import, any failed objects can be obtained through batch.failed_objects. Additionally, a running count of failed objects is maintained and can be accessed through batch.number_errors within the context manager. This counter can be used to stop the import process in order to investigate the failed objects or references. Find out more about error handling on the Python client reference page.quickstart_import.tsimport weaviate, { WeaviateClient } from 'weaviate-client';// Best practice: store your credentials in environment variablesconst weaviateUrl = process.env.WEAVIATE_URL as string;const weaviateApiKey = process.env.WEAVIATE_API_KEY as string;const client: WeaviateClient = await weaviate.connectToWeaviateCloud(  weaviateUrl, // Replace with your Weaviate Cloud URL  {    authCredentials: new weaviate.ApiKey(weaviateApiKey), // Replace with your Weaviate Cloud API key  });// Load dataasync function getJsonData() {  const file = await fetch(    'https://raw.githubusercontent.com/weaviate-tutorials/quickstart/main/data/jeopardy_tiny.json'  );  return file.json();}// Note: The TS client does not have a `batch` method yet// We use `insertMany` instead, which sends all of the data in one requestasync function importQuestions() {  const questions = client.collections.use('Question');  const data = await getJsonData();  const result = await questions.data.insertMany(data);  console.log('Insertion response: ', result);}await importQuestions();client.close(); // Close the client connection  API docsquickstart/2_2_import/main.go// Set these environment variables// WEAVIATE_HOSTNAME            your Weaviate instance hostname// WEAVIATE_API_KEY          your Weaviate instance API keypackage mainimport (    \"context\"    \"encoding/json\"    \"fmt\"    \"net/http\"    \"os\"    \"github.com/weaviate/weaviate-go-client/v5/weaviate\"    \"github.com/weaviate/weaviate-go-client/v5/weaviate/auth\"    \"github.com/weaviate/weaviate/entities/models\")func main() {    cfg := weaviate.Config{        Host:       os.Getenv(\"WEAVIATE_HOSTNAME\"),        Scheme:     \"https\",        AuthConfig: auth.ApiKey{Value: os.Getenv(\"WEAVIATE_API_KEY\")},    }    client, err := weaviate.NewClient(cfg)    if err != nil {        fmt.Println(err)    }    // Retrieve the data    data, err := http.DefaultClient.Get(\"https://raw.githubusercontent.com/weaviate-tutorials/quickstart/main/data/jeopardy_tiny.json\")    if err != nil {        panic(err)    }    defer data.Body.Close()    // Decode the data    var items []map[string]string    if err := json.NewDecoder(data.Body).Decode(&items); err != nil {        panic(err)    }    // convert items into a slice of models.Object    objects := make([]*models.Object, len(items))    for i := range items {        objects[i] = &models.Object{            Class: \"Question\",            Properties: map[string]any{                \"category\": items[i][\"Category\"],                \"question\": items[i][\"Question\"],                \"answer\":   items[i][\"Answer\"],            },        }    }    // batch write items    batchRes, err := client.Batch().ObjectsBatcher().WithObjects(objects...).Do(context.Background())    if err != nil {        panic(err)    }    for _, res := range batchRes {        if res.Result.Errors != nil {            panic(res.Result.Errors.Error)        }    }}  API docsquickstart/Import.javaimport io.weaviate.client.Config;import io.weaviate.client.WeaviateAuthClient;import io.weaviate.client.WeaviateClient;import io.weaviate.client.v1.batch.api.ObjectsBatcher;import io.weaviate.client.v1.data.model.WeaviateObject;import org.json.JSONArray;import org.json.JSONObject;import java.io.BufferedReader;import java.io.InputStreamReader;import java.net.HttpURLConnection;import java.net.URL;import java.util.HashMap;import java.util.Map;// Set these environment variables// WEAVIATE_HOSTNAME     Your Weaviate instance hostname// WEAVIATE_API_KEY      Your Weaviate instance API keypublic class Import {  public static void main(String[] args) throws Exception {    String host = System.getenv(\"WEAVIATE_HOSTNAME\");    String apiKey = System.getenv(\"WEAVIATE_API_KEY\");    Config config = new Config(\"https\", host);    WeaviateClient client = WeaviateAuthClient.apiKey(config, apiKey);    // Get JSON data    URL url = new URL(\"https://raw.githubusercontent.com/weaviate-tutorials/quickstart/main/data/jeopardy_tiny.json\");    String jsonData = new BufferedReader(new InputStreamReader(((HttpURLConnection) url.openConnection()).getInputStream()))      .lines().reduce(\"\", String::concat);    // Create and execute batch    ObjectsBatcher batcher = client.batch().objectsBatcher();    new JSONArray(jsonData).forEach(item -> {      JSONObject json = (JSONObject) item;      HashMap<String, Object> properties = new HashMap<>();      properties.put(\"category\", json.getString(\"Category\"));      properties.put(\"question\", json.getString(\"Question\"));      properties.put(\"answer\", json.getString(\"Answer\"));      batcher.withObject(WeaviateObject.builder()        .className(\"Question\")        .properties(properties)        .build());    });    // Flush    batcher.run();  }}  API docsnote\n",
      "Download the jeopardy_tiny.json file from here before running the following script.\n",
      "This assumes you have jq installed.\n",
      "# Best practice: store your credentials in environment variables# export WEAVIATE_URL=\"YOUR_INSTANCE_URL\"  # Your Weaviate instance URL# export WEAVIATE_API_KEY=\"YOUR_API_KEY\"   # Your Weaviate instance API key# Set batch sizeBATCH_ENDPOINT=\"$WEAVIATE_URL/v1/batch/objects\"BATCH_SIZE=100# Read the JSON file and loop through its entrieslines_processed=0batch_data=\"{\\\"objects\\\": [\"cat jeopardy_tiny.json | jq -c '.[]' | while read line; do  # Concatenate lines  line=$(echo \"$line\" | jq \"{class: \\\"Question\\\", properties: {answer: .Answer, question: .Question, category: .Category}}\")  if [ $lines_processed -eq 0 ]; then    batch_data+=$line  else    batch_data+=\",$line\"  fi  lines_processed=$((lines_processed + 1))  # If the batch is full, send it to the API using curl  if [ $lines_processed -eq $BATCH_SIZE ]; then    batch_data+=\"]}\"    curl -X POST \"$BATCH_ENDPOINT\" \\         -H \"Content-Type: application/json\" \\         -H \"Authorization: Bearer $WEAVIATE_API_KEY\" \\         -d \"$batch_data\"    echo \"\" # Print a newline for better output formatting    # Reset the batch data and counter    lines_processed=0    batch_data=\"{\\\"objects\\\": [\"  fidone# Send the remaining data (if any) to the API using curlif [ $lines_processed -ne 0 ]; then  batch_data+=\"]}\"  curl -X POST \"$BATCH_ENDPOINT\" \\       -H \"Content-Type: application/json\" \\       -H \"Authorization: Bearer $WEAVIATE_API_KEY\" \\       -d \"$batch_data\"  echo \"\" # Print a newline for better output formattingfi\n",
      "Run this code to add the demo data.\n",
      "\n",
      "Step 3: Queries​\n",
      "Weaviate provides a wide range of query tools to help you find the right data. We will try a few searches here.\n",
      "3.1 Semantic search​\n",
      "Semantic search finds results based on meaning. This is called nearText in Weaviate.\n",
      "The following example searches for 2 objects whose meaning is most similar to that of biology.\n",
      "\n",
      "PythonJS/TSGoJavaCurlquickstart_neartext_query.pyimport weaviatefrom weaviate.classes.init import Authimport os, json# Best practice: store your credentials in environment variablesweaviate_url = os.environ[\"WEAVIATE_URL\"]weaviate_api_key = os.environ[\"WEAVIATE_API_KEY\"]client = weaviate.connect_to_weaviate_cloud(    cluster_url=weaviate_url,                                    # Replace with your Weaviate Cloud URL    auth_credentials=Auth.api_key(weaviate_api_key),             # Replace with your Weaviate Cloud key)questions = client.collections.get(\"Question\")response = questions.query.near_text(    query=\"biology\",    limit=2)for obj in response.objects:    print(json.dumps(obj.properties, indent=2))client.close()  # Free up resources  API docsquickstart_neartext_query.tsimport weaviate, { WeaviateClient } from 'weaviate-client';// Best practice: store your credentials in environment variablesconst weaviateUrl = process.env.WEAVIATE_URL as string;const weaviateApiKey = process.env.WEAVIATE_API_KEY as string;const client: WeaviateClient = await weaviate.connectToWeaviateCloud(  weaviateUrl, // Replace with your Weaviate Cloud URL  {    authCredentials: new weaviate.ApiKey(weaviateApiKey), // Replace with your Weaviate Cloud API key  });const questions = client.collections.use('Question');const result = await questions.query.nearText('biology', {  limit: 2,});result.objects.forEach((item) => {  console.log(JSON.stringify(item.properties, null, 2));});client.close(); // Close the client connection  API docsquickstart/3_1_neartext/main.go// Set these environment variables// WEAVIATE_HOSTNAME            your Weaviate instance hostname// WEAVIATE_API_KEY          your Weaviate instance API keypackage mainimport (    \"context\"    \"fmt\"    \"os\"    \"github.com/weaviate/weaviate-go-client/v5/weaviate\"    \"github.com/weaviate/weaviate-go-client/v5/weaviate/auth\"    \"github.com/weaviate/weaviate-go-client/v5/weaviate/graphql\")func main() {    cfg := weaviate.Config{        Host:       os.Getenv(\"WEAVIATE_HOSTNAME\"),        Scheme:     \"https\",        AuthConfig: auth.ApiKey{Value: os.Getenv(\"WEAVIATE_API_KEY\")},    }    client, err := weaviate.NewClient(cfg)    if err != nil {        fmt.Println(err)    }    ctx := context.Background()    response, err := client.GraphQL().Get().        WithClassName(\"Question\").        WithFields(            graphql.Field{Name: \"question\"},            graphql.Field{Name: \"answer\"},            graphql.Field{Name: \"category\"},        ).        WithNearText(client.GraphQL().NearTextArgBuilder().            WithConcepts([]string{\"biology\"})).        WithLimit(2).        Do(ctx)    if err != nil {        panic(err)    }    fmt.Printf(\"%v\", response)}  API docsquickstart/NearText.javaimport io.weaviate.client.Config;import io.weaviate.client.WeaviateAuthClient;import io.weaviate.client.WeaviateClient;import io.weaviate.client.base.Result;import io.weaviate.client.v1.graphql.model.GraphQLResponse;import io.weaviate.client.v1.graphql.query.argument.NearTextArgument;import io.weaviate.client.v1.graphql.query.builder.GetBuilder;import io.weaviate.client.v1.graphql.query.fields.Field;import io.weaviate.client.v1.graphql.query.fields.Fields;import java.util.HashMap;import java.util.Map;// Set these environment variables// WEAVIATE_HOSTNAME     Your Weaviate instance hostname// WEAVIATE_API_KEY      Your Weaviate instance API keypublic class NearText {  public static void main(String[] args) throws Exception {    String host = System.getenv(\"WEAVIATE_HOSTNAME\");    String apiKey = System.getenv(\"WEAVIATE_API_KEY\");    Config config = new Config(\"https\", host);    WeaviateClient client = WeaviateAuthClient.apiKey(config, apiKey);    NearTextArgument nearText = NearTextArgument.builder()      .concepts(new String[]{\"biology\"})      .build();    Fields fields = Fields.builder()      .fields(new Field[]{        Field.builder().name(\"question\").build(),        Field.builder().name(\"answer\").build(),      })      .build();    String query = GetBuilder.builder()      .className(\"Question\")      .fields(fields)      .withNearTextFilter(nearText)      .limit(2)      .build()      .buildQuery();    Result<GraphQLResponse> result = client.graphQL().raw().withQuery(query).run();    System.out.println(result.getResult());  }}  API docs# Best practice: store your credentials in environment variables# export WEAVIATE_URL=\"YOUR_INSTANCE_URL\"    # Your Weaviate instance URL# export WEAVIATE_API_KEY=\"YOUR_API_KEY\"     # Your Weaviate instance API keyecho '{  \"query\": \"{    Get {      Question (        limit: 2        nearText: {          concepts: [\\\"biology\\\"],        }      ) {        question        answer        category      }    }  }\"}' | tr -d \"\\n\" | curl \\    -X POST \\    -H 'Content-Type: application/json' \\    -H \"Authorization: Bearer $WEAVIATE_API_KEY\" \\    -d @- \\    $WEAVIATE_URL/v1/graphql\n",
      "Run this code to perform the query. Our query found entries for DNA and species.\n",
      "Example full response in JSON format{  {    \"answer\": \"DNA\",    \"question\": \"In 1953 Watson & Crick built a model of the molecular structure of this, the gene-carrying substance\",    \"category\": \"SCIENCE\"  },  {    \"answer\": \"species\",    \"question\": \"2000 news: the Gunnison sage grouse isn't just another northern sage grouse, but a new one of this classification\",    \"category\": \"SCIENCE\"  }}\n",
      "If you inspect the full response, you will see that the word biology does not appear anywhere.\n",
      "Even so, Weaviate was able to return biology-related entries. This is made possible by vector embeddings that capture meaning. Under the hood, semantic search is powered by vectors, or vector embeddings.\n",
      "Here is a diagram showing the workflow in Weaviate.\n",
      "\n",
      "Where did the vectors come from?Weaviate used the Weaviate Embeddings service to generate a vector embedding for each object during import. During the query, Weaviate similarly converted the query (biology) into a vector.As we mentioned above, this is optional. See Starter Guide: Bring Your Own Vectors if you would prefer to provide your own vectors.\n",
      "More search types availableWeaviate is capable of many types of searches. See, for example, our how-to guides on similarity searches, keyword searches, hybrid searches, and filtered searches.\n",
      "3.2 Retrieval augmented generation​\n",
      "Retrieval augmented generation (RAG), also called generative search, combines the power of generative AI models such as large language models (LLMs) with the up-to-date truthfulness of a database.\n",
      "RAG works by prompting a large language model (LLM) with a combination of a user query and data retrieved from a database.\n",
      "This diagram shows the RAG workflow in Weaviate.\n",
      "\n",
      "The following example combines the same search (for biology) with a prompt to generate a tweet.\n",
      "\n",
      "PythonJS/TSGoJavaCurlquickstart_rag.pyimport weaviatefrom weaviate.classes.init import Authimport os# Best practice: store your credentials in environment variablesweaviate_url = os.environ[\"WEAVIATE_URL\"]weaviate_api_key = os.environ[\"WEAVIATE_API_KEY\"]cohere_api_key = os.environ[\"COHERE_APIKEY\"]client = weaviate.connect_to_weaviate_cloud(    cluster_url=weaviate_url,                                    # Replace with your Weaviate Cloud URL    auth_credentials=Auth.api_key(weaviate_api_key),             # Replace with your Weaviate Cloud key    headers={\"X-Cohere-Api-Key\": cohere_api_key},           # Replace with your Cohere API key)questions = client.collections.get(\"Question\")response = questions.generate.near_text(    query=\"biology\",    limit=2,    grouped_task=\"Write a tweet with emojis about these facts.\")print(response.generative.text)  # Inspect the generated textclient.close()  # Free up resources  API docsquickstart_rag.tsimport weaviate, { WeaviateClient } from 'weaviate-client';// Best practice: store your credentials in environment variablesconst weaviateUrl = process.env.WEAVIATE_URL as string;const weaviateApiKey = process.env.WEAVIATE_API_KEY as string;const cohereKey = process.env.COHERE_APIKEY as string;const client: WeaviateClient = await weaviate.connectToWeaviateCloud(  weaviateUrl, // Replace with your Weaviate Cloud URL  {    authCredentials: new weaviate.ApiKey(weaviateApiKey), // Replace with your Weaviate Cloud API key    headers: {      'X-Cohere-Api-Key': cohereKey, // Replace with your Cohere API key    },  });const questions = client.collections.use('Question');const result = await questions.generate.nearText(  'biology',  {    groupedTask: 'Write a tweet with emojis about these facts.',  },  {    limit: 2,  });console.log(result.generated);client.close(); // Close the client connection  API docsquickstart/3_2_rag/main.go// Set these environment variables// WEAVIATE_HOSTNAME            your Weaviate instance hostname// WEAVIATE_API_KEY          your Weaviate instance API key// COHERE_APIKEY           your Cohere API keypackage mainimport (    \"context\"    \"fmt\"    \"os\"    \"github.com/weaviate/weaviate-go-client/v5/weaviate\"    \"github.com/weaviate/weaviate-go-client/v5/weaviate/auth\"    \"github.com/weaviate/weaviate-go-client/v5/weaviate/graphql\")func main() {    cfg := weaviate.Config{        Host:       os.Getenv(\"WEAVIATE_HOSTNAME\"),        Scheme:     \"https\",        AuthConfig: auth.ApiKey{Value: os.Getenv(\"WEAVIATE_API_KEY\")},        Headers: map[string]string{            \"X-Cohere-Api-Key\": os.Getenv(\"COHERE_APIKEY\"),        },    }    client, err := weaviate.NewClient(cfg)    if err != nil {        fmt.Println(err)    }    ctx := context.Background()    generatePrompt := \"Write a tweet with emojis about these facts.\"    gs := graphql.NewGenerativeSearch().GroupedResult(generatePrompt)    response, err := client.GraphQL().Get().        WithClassName(\"Question\").        WithFields(            graphql.Field{Name: \"question\"},            graphql.Field{Name: \"answer\"},            graphql.Field{Name: \"category\"},        ).        WithGenerativeSearch(gs).        WithNearText(client.GraphQL().NearTextArgBuilder().            WithConcepts([]string{\"biology\"})).        WithLimit(2).        Do(ctx)    if err != nil {        panic(err)    }    fmt.Printf(\"%v\", response)}  API docsquickstart/RAG.javaimport io.weaviate.client.Config;import io.weaviate.client.WeaviateAuthClient;import io.weaviate.client.WeaviateClient;import io.weaviate.client.base.Result;import io.weaviate.client.v1.graphql.model.GraphQLResponse;import io.weaviate.client.v1.graphql.query.argument.NearTextArgument;import io.weaviate.client.v1.graphql.query.builder.GetBuilder;import io.weaviate.client.v1.graphql.query.fields.Field;import io.weaviate.client.v1.graphql.query.fields.Fields;import io.weaviate.client.v1.graphql.query.fields.GenerativeSearchBuilder;import java.util.HashMap;import java.util.Map;// Set these environment variables// WEAVIATE_HOSTNAME     Your Weaviate instance hostname// WEAVIATE_API_KEY      Your Weaviate instance API key// COHERE_APIKEY    Your Cohere API keypublic class RAG {  public static void main(String[] args) throws Exception {    String host = System.getenv(\"WEAVIATE_HOSTNAME\");    String apiKey = System.getenv(\"WEAVIATE_API_KEY\");    String cohereKey = System.getenv(\"COHERE_APIKEY\");    Map<String, String> headers = new HashMap<String, String>() { {      put(\"X-Cohere-Api-Key\", cohereKey);    } };    Config config = new Config(\"https\", host, headers);    WeaviateClient client = WeaviateAuthClient.apiKey(config, apiKey);    NearTextArgument nearText = NearTextArgument.builder()      .concepts(new String[]{\"biology\"})      .build();    GenerativeSearchBuilder ragQuery = GenerativeSearchBuilder.builder()      .groupedResultTask(\"Write a tweet with emojis about these facts.\")      .build();    Fields fields = Fields.builder()      .fields(new Field[]{        Field.builder().name(\"question\").build(),        Field.builder().name(\"answer\").build(),      })      .build();    String query = GetBuilder.builder()      .className(\"Question\")      .fields(fields)      .withNearTextFilter(nearText)      .withGenerativeSearch(ragQuery)      .limit(2)      .build()      .buildQuery();    Result<GraphQLResponse> result = client.graphQL().raw().withQuery(query).run();    System.out.println(result.getResult());  }}  API docs# Best practice: store your credentials in environment variables# export WEAVIATE_URL=\"YOUR_INSTANCE_URL\"    # Your Weaviate instance URL# export WEAVIATE_API_KEY=\"YOUR_API_KEY\"     # Your Weaviate instance API key# export COHERE_APIKEY=\"YOUR_API_KEY\"   # Your Cohere API keyecho '{  \"query\": \"{    Get {      Question (        limit: 2        nearText: {          concepts: [\\\"biology\\\"],        }      ) {        question        answer        category        _additional {          generate(            groupedResult: {              task: \\\"\\\"\\\"                Write a tweet with emojis about these facts.              \\\"\\\"\\\"            }          ) {            groupedResult            error          }        }      }    }  }\"}' | tr -d \"\\n\" | curl \\    -X POST \\    -H 'Content-Type: application/json' \\    -H \"Authorization: Bearer $WEAVIATE_API_KEY\" \\    -H \"X-Cohere-Api-Key: $COHERE_APIKEY\" \\    -d @- \\    $WEAVIATE_URL/v1/graphql\n",
      "Cohere API key in the headerNote that this code includes an additional header for the Cohere API key. Weaviate uses this key to access the Cohere generative AI model and perform retrieval augmented generation (RAG).\n",
      "Run this code to perform the query. Here is one possible response (your response will likely be different).\n",
      "🧬 In 1953 Watson & Crick built a model of the molecular structure of DNA, the gene-carrying substance! 🧬🔬🦢 2000 news: the Gunnison sage grouse isn't just another northern sage grouse, but a new species! 🦢🌿 #ScienceFacts #DNA #SpeciesClassification\n",
      "The response should be new, yet familiar. This is because you have seen the entries above for DNA and species in the semantic search section.\n",
      "The power of RAG comes from the ability to transform your own data. Weaviate helps you in this journey by making it easy to perform a combined search & generation in just a few lines of code.\n",
      "\n",
      "Recap​\n",
      "In this quickstart guide, you:\n",
      "\n",
      "Created a Serverless Weaviate sandbox instance on Weaviate Cloud.\n",
      "Defined a collection and added data.\n",
      "Performed queries, including:\n",
      "\n",
      "Semantic search, and\n",
      "Retrieval augmented generation.\n",
      "\n",
      "\n",
      "\n",
      "Where to go next is up to you. We include some suggested steps and resources below.\n",
      "\n",
      "Next​\n",
      "Try these additional resources to learn more about Weaviate:\n",
      "More on searchSee how to perform searches, such as keyword, similarity, hybrid, image, filtered and reranked searches.Manage dataSee how to manage data, such as manage collections, create objects, batch import data and use multi-tenancy.RAGCheck out the Starter guide: retrieval augmented generation, and the Weaviate Academy unit on chunking.Workshops and office hoursWe hold in-person and online workshops, office hours and events for different experience levels. Join us!\n",
      "\n",
      "FAQs & Troubleshooting​\n",
      "We provide answers to some common questions, or potential issues below.\n",
      "Questions​\n",
      "Can I use different integrations?​\n",
      "See answerIn this example, we use the Weaviate Embeddings and Cohere inference API. But you can use others.If you do want to change the embeddings, or the generative AI integrations, you can. You will need to:\n",
      "Ensure that the Weaviate module is available in the Weaviate instance you are using,\n",
      "Modify your collection definition to use your preferred integration, and\n",
      "Make sure to use the right API key(s) (if necessary) for your integration.\n",
      "See the model providers integration section for more information.\n",
      "Troubleshooting​\n",
      "If you see Error: Name 'Question' already used as a name for an Object class​\n",
      "See answerYou may see this error if you try to create a collection that already exists in your instance of Weaviate. In this case, you can follow these instructions to delete the collection.You can delete any unwanted collection(s), along with the data that they contain.\n",
      "Deleting a collection also deletes its objectsWhen you delete a collection, you delete all associated objects!Be very careful with deletes on a production database and anywhere else that you have important data.\n",
      "This code deletes a collection and its objects.\n",
      "Python Client v4Python Client v3JS/TS Client v3JS/TS Client v2GoJavaCurl# collection_name can be a string (\"Article\") or a list of strings ([\"Article\", \"Category\"])client.collections.delete(    collection_name)  # THIS WILL DELETE THE SPECIFIED COLLECTION(S) AND THEIR OBJECTS# Note: you can also delete all collections in the Weaviate instance with:# client.collections.delete_all()  API docs# delete class \"Article\" - THIS WILL DELETE ALL DATA IN THIS CLASSclient.schema.delete_class(\"Article\")  # Replace with your class name// delete collection \"Article\" - THIS WILL DELETE THE COLLECTION AND ALL ITS DATAawait client.collections.delete('Article')// you can also delete all collections of a cluster// await client.collections.deleteAll()// delete collection \"Article\" - THIS WILL DELETE THE COLLECTION AND ALL ITS DATAawait client.schema  .classDeleter()  .withClassName('Article')  .do();className := \"YourClassName\"// delete the classif err := client.Schema().ClassDeleter().WithClassName(className).Do(context.Background()); err != nil {  // Weaviate will return a 400 if the class does not exist, so this is allowed, only return an error if it's not a 400  if status, ok := err.(*fault.WeaviateClientError); ok && status.StatusCode != http.StatusBadRequest {    panic(err)  }}Result<Boolean> result = client.schema().classDeleter()    .withClassName(collectionName)    .run();  API docscurl \\  -X DELETE \\  https://WEAVIATE_INSTANCE_URL/v1/schema/YourClassName  # Replace WEAVIATE_INSTANCE_URL with your instance URL\n",
      "How to confirm collection creation​\n",
      "See answerIf you are not sure whether the collection has been created, check the schema endpoint.Replace WEAVIATE_INSTANCE_URL with your instance's REST Endpoint URL.:https://WEAVIATE_INSTANCE_URL/v1/schemaYou should see:{    \"classes\": [        {            \"class\": \"Question\",            ...  // truncated additional information here            \"vectorizer\": \"text2vec-weaviate\"        }    ]}Where the schema should indicate that the Question collection has been added.REST & GraphQL in WeaviateWeaviate uses a combination of RESTful and GraphQL APIs. In Weaviate, RESTful API endpoints can be used to add data or obtain information about the Weaviate instance, and the GraphQL interface to retrieve data.\n",
      "How to confirm data import​\n",
      "See answerTo confirm successful data import, check the objects endpoint to verify that all objects are imported.Replace WEAVIATE_INSTANCE_URL with your instance REST Endpoint URL:https://WEAVIATE_INSTANCE_URL/v1/objectsYou should see:{    \"deprecations\": null,    \"objects\": [        ...  // Details of each object    ],    \"totalResults\": 10  // You should see 10 results here}Where you should be able to confirm that you have imported all 10 objects.\n",
      "If the nearText search is not working​\n",
      "See answerTo perform text-based (nearText) similarity searches, you need to have a vectorizer enabled, and configured in your collection.Make sure the vectorizer is configured like this.If the search still doesn't work, contact us!\n",
      "Questions and feedback​\n",
      "\n",
      "If you have any questions or feedback, let us know in the user forum.\n",
      "\n",
      "\n",
      "Technical questionsIf you have questions feel free to post on our Community forum.Documentation feedbackLeave feedback by opening a GitHub issue.Edit this pageDocumentationWeaviate DatabaseDeployment documentationWeaviate CloudWeaviate AgentsSupportForumSlack\n",
      "\n",
      "\n",
      "Fine-tuning with gpt-oss and Hugging Face TransformersTopicsAboutAPI docsSourceToggle themeToggle themeAug 5, 2025Fine-tuning with gpt-oss and Hugging Face TransformersEdward Beeching, Quentin Gallouédec, Lewis TunstallOpen in GitHubView as MarkdownSetupPrepare the datasetPrepare the modelFine-tuningSave the model and push to the Hugging Face HubInferenceConclusionAuthored by: Edward Beeching, Quentin Gallouédec, and Lewis Tunstall\n",
      "Large reasoning models like OpenAI o3 generate a chain-of-thought to improve the accuracy and quality of their responses. However, most of these models reason in English, even when a question is asked in another language.\n",
      "In this notebook, we show how OpenAI's open-weight reasoning model OpenAI gpt-oss-20b can be fine-tuned to reason effectively in multiple languages. We'll do this by adding a new \"reasoning language\" option to the model's system prompt, and applying supervised fine-tuning with Hugging Face's TRL library on a multilingual reasoning dataset.\n",
      "We'll cover the following steps:\n",
      "\n",
      "Setup: Install the required libraries.\n",
      "Prepare the dataset:  Download and format the dataset for fine-tuning.\n",
      "Prepare the model: Loading the base model and configure it for fine-tuning LoRA, a memory efficient technique.\n",
      "Fine-tuning: Train the model with our multilingual reasoning data.\n",
      "Inference: Generate reasoning responses in different languages using the fine-tuned model.\n",
      "\n",
      "The end result is a multilingual reasoning model that can generate a chain-of-thought in English, Spanish, French, Italian, or German. You can even mix languages—for example, ask a question in Spanish, request reasoning in German, and receive the final response in Spanish:\n",
      "User:\n",
      "    ¿Cuál es el capital de Australia?\n",
      "Assistant reasoning:\n",
      "    Okay, der Benutzer fragt nach der Hauptstadt Australiens. Ich erinnere mich, dass Canberra die Hauptstadt ist. Ich\n",
      "    sollte das bestätigen. Lass mich sehen, ob es irgendwelche potenziellen Verwirrungen gibt. Der Benutzer könnte auch\n",
      "    an der größten Stadt interessiert sein. Die größte Stadt ist Sydney, aber die Hauptstadt ist Canberra. Ich sollte\n",
      "    das klarstellen. Vielleicht auch erwähnen, dass Canberra eine geplante Stadt ist und nicht die größte. Der Benutzer\n",
      "    könnte auch nach der Geografie fragen. Vielleicht erwähne ich, dass Canberra im südwestlichen Teil der Australian\n",
      "    Capital Territory liegt. Ich sollte die Antwort präzise und freundlich halten. Vielleicht auch erwähnen, dass\n",
      "    Canberra oft mit Sydney verwechselt wird. Ich sollte sicherstellen, dass die Antwort klar und korrekt ist.\n",
      "Assistant response:\n",
      "    La capital de Australia es **Canberra**. Aunque es la ciudad más pequeña de las principales capitales del país, fue\n",
      "    elegida en 1908 como la sede del gobierno federal para equilibrar la influencia entre las ciudades de Sydney y\n",
      "    Melbourne. Canberra está ubicada en el Territorio de la Capital Australiana (ACT), en el este de Australia.\n",
      "We hope this tutorial will enable AI developers working with under-represented languages to improve the interpretability of openai/gpt-oss-20b in their native languages.\n",
      "\n",
      "Note: This notebook is designed to be run on a single H100 GPU with 80GB of memory. If you have access to a smaller GPU, you can reduce the batch size and sequence length in the hyperparameters below.\n",
      "Setup\n",
      "To get started, let’s install all the necessary libraries. First install PyTorch:%pip install torch --index-url https://download.pytorch.org/whl/cu128Next, install the remaining dependencies:%pip install \"trl>=0.20.0\" \"peft>=0.17.0\" \"transformers>=4.55.0\" trackioFinally, log into your Hugging Face account as follows:from huggingface_hub import notebook_login\n",
      "\n",
      "notebook_login()Now that we've installed the required libraries, let's take a look at the dataset that we will use for fine-tuning.Prepare the dataset\n",
      "We will be using Multilingual-Thinking, which is a reasoning dataset where the chain-of-thought has been translated into several languages such as French, Spanish, and German. By fine-tuning openai/gpt-oss-20b on this dataset, it will learn to generate reasoning steps in these languages, and thus its reasoning process can be interpreted by users who speak those languages.Let's download this dataset from the Hugging Face Hub:from datasets import load_dataset\n",
      "\n",
      "dataset = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\n",
      "datasetThis is a small dataset of 1,000 examples, but this is usually more than sufficient for models like openai/gpt-oss-20b which have undergone extensive post-training. Let's take a look at one of the training examples:dataset[0]The gpt-oss models were trained on the Harmony response format for defining conversation structures, generating reasoning output and structuring function calls. The format is designed to mimic the OpenAI Responses API, and the table below summarizes the different message types used in the dataset:\n",
      "developerThe developer message is used to provide custom instructions for the model (what we usually call the system role).userThe user message is used to provide the input to the model.assistantOutput by the model which can either be a tool call or a message output. The output might also be associated with a particular “channel” identifying what the intent of the message is.analysisThese are messages that are being used by the model for its chain-of-thoughtfinalMessages tagged in the final channel are messages intended to be shown to the end-user and represent the responses from the model.messagesThe list of messages that combine the content of the above to produce a full conversation. This is the input to the model.If you're familiar with OpenAI's messages format, you will recognise this as being quite similar, but with an important difference:\n",
      "\n",
      "The assistant turn contains two special fields: a thinking one which contains the model's reasoning process, and a content one which contains the final response to the user.\n",
      "\n",
      "In order to fine-tune the model, we need to convert these messages into a format that the model can understand. In practice this is done by formatting each message with the model's chat template and then tokenizing the resulting text. The TRL library does this automatically, but let's walk through it step by step to understand how it works.\n",
      "To do so, let's first load the tokenizer:from transformers import AutoTokenizer\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")Then we can use the tokenizer's apply_chat_template() method to format the messages:messages = dataset[0][\"messages\"]\n",
      "conversation = tokenizer.apply_chat_template(messages, tokenize=False)\n",
      "print(conversation)This chat template is quite sophisticated, so let's take a closer look at it! First, we can see there are special tokens <|start|> and <|end|> that indicate the start and end of each message. There is also a <|return|> token that marks the end of the conversation. These tokens help the model understand the structure of the conversation.\n",
      "We can also see there are two types of system message:\n",
      "\n",
      "A default system one that is used for all messages. In the example above, this refers to the text \"You are ChatGPT, a large language model trained by OpenAI...\"\n",
      "A special developer one that contains custom instructions (defined by the system role in our messages object). This allows us to provide additional context to the model about how it should behave for a given conversation. In the example above, this refers to the text \"You are an AI chatbot with a lively and energetic personality.\"\n",
      "\n",
      "Finally, we can see that the assistant response is contained in a series of channels:\n",
      "\n",
      "The analysis channel is used for the model's reasoning process, where it can think step by step about the user's question. In the example above, this refers to the French text \"D'accord, l'utilisateur demande les tendances Twitter...\"\n",
      "The final channel is used for the model's final response to the user. In the example above, this refers to the text \"Hey there!  While I can't check Twitter...\"\n",
      "Now that we understand how the dataset will be prepared, let's move on to preparing the model for training.Prepare the model\n",
      "To prepare the model for training, let's first download the weights from the Hugging Face Hub. We will use the AutoModelForCausalLM class from 🤗 Transformers to load the model:import torch\n",
      "from transformers import AutoModelForCausalLM, Mxfp4Config\n",
      "\n",
      "quantization_config = Mxfp4Config(dequantize=True)\n",
      "model_kwargs = dict(\n",
      "    attn_implementation=\"eager\",\n",
      "    torch_dtype=torch.bfloat16,\n",
      "    quantization_config=quantization_config,\n",
      "    use_cache=False,\n",
      "    device_map=\"auto\",\n",
      ")\n",
      "\n",
      "model = AutoModelForCausalLM.from_pretrained(\"openai/gpt-oss-20b\", **model_kwargs)This will load the model with the necessary configurations for training. The attn_implementation is set to eager for better performance, and use_cache is set to False since we will fine-tune the model with gradient checkpointing.\n",
      "If you're familiar with 🤗 Transformers, you might notice that we are using the Mxfp4Config for quantization. This is a specific configuration for the OpenAI models that allows us to use mixed precision training with a special 4-bit floating point format called MXFP4 that is optimized for AI workloads.\n",
      "Before we train the model, let's generate a sample response to see how the model behaves with the default settings. To do so, we need to tokenize a sample prompt and then use the model to generate a response:messages = [\n",
      "    {\"role\": \"user\", \"content\": \"¿Cuál es el capital de Australia?\"},\n",
      "]\n",
      "\n",
      "input_ids = tokenizer.apply_chat_template(\n",
      "    messages,\n",
      "    add_generation_prompt=True,\n",
      "    return_tensors=\"pt\",\n",
      ").to(model.device)\n",
      "\n",
      "output_ids = model.generate(input_ids, max_new_tokens=512)\n",
      "response = tokenizer.batch_decode(output_ids)[0]\n",
      "print(response)In this example, we can see that the model first reasons about the question in English, and then provides a final response in Spanish. This is the default behavior of the model, but let's see if we can change it with a bit of fine-tuning.\n",
      "To do so, we will use a technique called LoRA (Low-Rank Adaptation) to fine-tune the model. This technique allows us to tune a few specific layers of the model, which is particularly useful for large models like openai/gpt-oss-20b.\n",
      "First we need to wrap the model as a PeftModel and define the LoRA configuration. We will use the LoraConfig class from the PEFT library to do this:from peft import LoraConfig, get_peft_model\n",
      "\n",
      "peft_config = LoraConfig(\n",
      "    r=8,\n",
      "    lora_alpha=16,\n",
      "    target_modules=\"all-linear\",\n",
      "    target_parameters=[\n",
      "        \"7.mlp.experts.gate_up_proj\",\n",
      "        \"7.mlp.experts.down_proj\",\n",
      "        \"15.mlp.experts.gate_up_proj\",\n",
      "        \"15.mlp.experts.down_proj\",\n",
      "        \"23.mlp.experts.gate_up_proj\",\n",
      "        \"23.mlp.experts.down_proj\",\n",
      "    ],\n",
      ")\n",
      "peft_model = get_peft_model(model, peft_config)\n",
      "peft_model.print_trainable_parameters()Here we've used some basic hyperparameters for LoRA, but you can experiment with different values to see how they affect the model's performance. For instance, if you increase r you will enable more trainable parameters, which may produce a better model at the expense of requiring more VRAM and time to train.\n",
      "Note: The openai/gpt-oss-20b model is a Mixture-of-Experts (MoE) architecture. In addition to targeting the attention layers (target_modules=\"all-linear\"), it’s also important to include the projection layers within the expert modules. PEFT facilitates this via the target_parameters argument, which allows you to specify expert-specific layers such as mlp.experts.down_proj and mlp.experts.gate_up_proj. In this example, we target a subset of these projection layers, but you are encouraged to experiment with different configurations.Now that we have the model and dataset ready, we can define the hyperparameters for training.Fine-tuningTRL provides a convenient way to define hyperparameters for training using the SFTConfig class. We will set the learning rate, batch size, number of epochs, and other parameters as follows:from trl import SFTConfig\n",
      "\n",
      "training_args = SFTConfig(\n",
      "    learning_rate=2e-4,\n",
      "    gradient_checkpointing=True,\n",
      "    num_train_epochs=1,\n",
      "    logging_steps=1,\n",
      "    per_device_train_batch_size=4,\n",
      "    gradient_accumulation_steps=4,\n",
      "    max_length=2048,\n",
      "    warmup_ratio=0.03,\n",
      "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
      "    lr_scheduler_kwargs={\"min_lr_rate\": 0.1},\n",
      "    output_dir=\"gpt-oss-20b-multilingual-reasoner\",\n",
      "    report_to=\"trackio\",\n",
      "    push_to_hub=True,\n",
      ")Note that the per_device_train_batch_size is set to 4, and the gradient_accumulation_steps is set to 4. This means that we will effectively have a batch size of 4 x 4 = 16 across 1 GPU. You may need to adjust these values based on your hardware setup. We also use Trackio to log the training progress and metrics, but you can use any other logging library of your choice.We now have all the pieces needed to train the model. We will use the SFTTrainer class from TRL to handle the training process. The trainer will take care of formatting the dataset, applying the chat template, and training the model:from trl import SFTTrainer\n",
      "\n",
      "trainer = SFTTrainer(\n",
      "    model=peft_model,\n",
      "    args=training_args,\n",
      "    train_dataset=dataset,\n",
      "    processing_class=tokenizer,\n",
      ")\n",
      "trainer.train()On a H100 GPU, this takes about 18 minutes to train, but may take longer depending on your hardware.Save the model and push to the Hugging Face HubFinally, you can push the fine-tuned model to your Hub repository to share with the community:trainer.save_model(training_args.output_dir)\n",
      "trainer.push_to_hub(dataset_name=\"HuggingFaceH4/Multilingual-Thinking\")Note: To avoid out-of-memory (OOM) errors, we recommend restarting the kernel at this point. The trained model is still occupying GPU memory, but it's no longer needed.Inference\n",
      "Once the model is uploaded to Hub, we can use it for inference. To do so we first initialize the original base model and its tokenizer. Next, we need to merge the fine-tuned weights with the base model for fast inference:from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "from peft import PeftModel\n",
      "\n",
      "# Load the tokenizer\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")\n",
      "\n",
      "# Load the original model first\n",
      "model_kwargs = dict(attn_implementation=\"eager\", torch_dtype=\"auto\", use_cache=True, device_map=\"auto\")\n",
      "base_model = AutoModelForCausalLM.from_pretrained(\"openai/gpt-oss-20b\", **model_kwargs).cuda()\n",
      "\n",
      "# Merge fine-tuned weights with the base model\n",
      "peft_model_id = \"gpt-oss-20b-multilingual-reasoner\"\n",
      "model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
      "model = model.merge_and_unload()Now that the model is loaded, the final step is to generate some tokens from it! Here we use the model's generate method to produce output based on the input prompt. Let's first define the prompt:Now we can tokenize the prompt and generate the output. Finally, we can decode the output tokens to get the final response:REASONING_LANGUAGE = \"German\"\n",
      "SYSTEM_PROMPT = f\"reasoning language: {REASONING_LANGUAGE}\"\n",
      "USER_PROMPT = \"¿Cuál es el capital de Australia?\"  # Spanish for \"What is the capital of Australia?\"\n",
      "\n",
      "messages = [\n",
      "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
      "    {\"role\": \"user\", \"content\": USER_PROMPT},\n",
      "]\n",
      "\n",
      "input_ids = tokenizer.apply_chat_template(\n",
      "    messages,\n",
      "    add_generation_prompt=True,\n",
      "    return_tensors=\"pt\",\n",
      ").to(model.device)\n",
      "\n",
      "gen_kwargs = {\"max_new_tokens\": 512, \"do_sample\": True, \"temperature\": 0.6, \"top_p\": None, \"top_k\": None}\n",
      "\n",
      "output_ids = model.generate(input_ids, **gen_kwargs)\n",
      "response = tokenizer.batch_decode(output_ids)[0]\n",
      "print(response)Let's also try with languages that the model has not been explicitly fine-tuned on, such as Chinese and Hindi:REASONING_LANGUAGE = \"Chinese\"  # or Hindi, or any other language...\n",
      "SYSTEM_PROMPT = f\"reasoning language: {REASONING_LANGUAGE}\"\n",
      "USER_PROMPT = \"What is the national symbol of Canada?\"\n",
      "\n",
      "messages = [\n",
      "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
      "    {\"role\": \"user\", \"content\": USER_PROMPT},\n",
      "]\n",
      "\n",
      "input_ids = tokenizer.apply_chat_template(\n",
      "    messages,\n",
      "    add_generation_prompt=True,\n",
      "    return_tensors=\"pt\",\n",
      ").to(model.device)\n",
      "\n",
      "output_ids = model.generate(input_ids, **gen_kwargs)\n",
      "response = tokenizer.batch_decode(output_ids)[0]\n",
      "print(response)Great, it works - we've now fine-tuned openai/gpt-oss-20b to reason in multiple languages!ConclusionCongratulations! You have successfully fine-tuned a multilingual reasoning model using the TRL library and LoRA. The steps in this notebook can be adapted to fine-tune openai/gpt-oss-20b on many other datasets on the Hugging Face Hub - we are excited to see what you'll build!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Fine-Tuning Your First Large Language Model (LLM) with PyTorch and Hugging Face\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hugging Face\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tModels\n",
      "\n",
      "\t\t\t\t\t\tDatasets\n",
      "\n",
      "\t\t\t\t\t\tSpaces\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tCommunity\n",
      "\t\t\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tDocs\n",
      "\n",
      "\t\t\t\t\t\tEnterprise\n",
      "\n",
      "Pricing\n",
      "\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log In\n",
      "\t\t\t\t\n",
      "Sign Up\n",
      "\t\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\tBack to Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tFine-Tuning Your First Large Language Model (LLM) with PyTorch and Hugging Face\n",
      "\t\n",
      "\n",
      "\n",
      "Community Article\n",
      "Published\n",
      "\t\t\t\tFebruary 11, 2025\n",
      "\n",
      "\n",
      "\n",
      "\t\tUpvote\n",
      "\n",
      "\t\t58\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "+52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Daniel Voigt Godoy\n",
      "\n",
      "dvgodoy\n",
      "\n",
      "\n",
      "Follow\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Spoilers\n",
      "\n",
      "Jupyter Notebook\n",
      "Setup\n",
      "\n",
      "Imports\n",
      "\n",
      "\n",
      "Loading a Quantized Base Model\n",
      "\n",
      "Setting Up Low-Rank Adapters (LoRA)\n",
      "\n",
      "Formatting Your Dataset\n",
      "Tokenizer\n",
      "\n",
      "\n",
      "Fine-Tuning with SFTTrainer\n",
      "SFTConfig\n",
      "\n",
      "SFTTrainer\n",
      "\n",
      "\n",
      "Querying the Model\n",
      "\n",
      "Saving the Adapter\n",
      "\n",
      "Subscribe Follow Connect\n",
      "\n",
      "\n",
      "    This blog post contains \"Chapter 0: TL;DR\" of my latest book A Hands-On Guide to Fine-Tuning Large Language Models with PyTorch and Hugging Face.\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tSpoilers\n",
      "\t\n",
      "\n",
      "In this blog post, we'll get right to it and fine-tune a small language model, Microsoft's Phi-3 Mini 4K Instruct, to translate English into Yoda-speak. You can think of this initial chapter as a recipe you can just follow. It's a \"shoot first, ask questions later\" kind of post.\n",
      "You'll learn how to:\n",
      "\n",
      "Load a quantized model using BitsAndBytes \n",
      "Configure low-rank adapters (LoRA) using Hugging Face's peft \n",
      "Load and format a dataset \n",
      "Fine-tune the model using the supervised fine-tuning trainer (SFTTrainer) from Hugging Face's trl \n",
      "Use the fine-tuned model to generate a sentence\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tJupyter Notebook\n",
      "\t\n",
      "\n",
      "The Jupyter notebook corresponding to this post is part of the official Fine-Tuning LLMs repository on GitHub. You can also run it directly in Google Colab \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tSetup\n",
      "\t\n",
      "\n",
      "If you're running it on Colab, you'll need to pip install a few libraries: datasets, bitsandbytes, and trl. \n",
      "For better reproducibility during training, however, use the pinned versions instead:\n",
      "#!pip install datasets bitsandbytes trl\n",
      "# bitsandbytes had to be bumped to 0.45.2 to avoid errors in Colab env\n",
      "!pip install transformers==4.46.2 peft==0.13.2 accelerate==1.1.1 trl==0.12.1 bitsandbytes==0.45.2 datasets==3.1.0 huggingface-hub==0.26.2 safetensors==0.4.5 pandas==2.2.2 matplotlib==3.8.0 numpy==1.26.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tImports\n",
      "\t\n",
      "\n",
      "For the sake of organization, all libraries needed throughout the code used are imported at its very start. For this post, we'll need the following imports:\n",
      "import os\n",
      "import torch\n",
      "from datasets import load_dataset\n",
      "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
      "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
      "from trl import SFTConfig, SFTTrainer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tLoading a Quantized Base Model\n",
      "\t\n",
      "\n",
      "We start by loading a quantized model, so it takes up less space in the GPU's RAM. A quantized model replaces the original weights with approximate values that are represented by fewer bits. The simplest and most straightforward way to quantize a model is to turn its weights from 32-bit floating-point (FP32) numbers into 4-bit floating-point numbers (NF4). This simple yet powerful change already reduces the model's memory footprint by roughly a factor of eight.\n",
      "We can use an instance of BitsAndBytesConfig as the quantization_config argument while loading a model using the from_pretrained() method. To keep it flexible, so you can try it out with any other model of your choice, we're using Hugging Face's\n",
      "AutoModelForCausalLM. The repo you choose to use determines the model being loaded.\n",
      "Without further ado, here's our quantized model being loaded:\n",
      "bnb_config = BitsAndBytesConfig(\n",
      "   load_in_4bit=True,\n",
      "   bnb_4bit_quant_type=\"nf4\",\n",
      "   bnb_4bit_use_double_quant=True,\n",
      "   bnb_4bit_compute_dtype=torch.float32\n",
      ")\n",
      "repo_id = 'microsoft/Phi-3-mini-4k-instruct'\n",
      "model = AutoModelForCausalLM.from_pretrained(\n",
      "   repo_id, device_map=\"cuda:0\", quantization_config=bnb_config\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "\"The Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. The model belongs to the Phi-3 family with the Mini version in two variants 4K and 128K which is the context length (in tokens) that it can support.\"\n",
      "\n",
      "    Source: Hugging Face Hub\n",
      "\n",
      "\n",
      "Once the model is loaded, you can see how much space it occupies in memory using the get_memory_footprint() method.\n",
      "print(model.get_memory_footprint()/1e6)\n",
      "\n",
      "2206.347264\n",
      "\n",
      "Even though it's been quantized, the model still takes up a bit more than 2 gigabytes of RAM. The quantization procedure focuses on the linear layers within the Transformer decoder blocks (also referred to as \"layers\" in some cases):\n",
      "model\n",
      "\n",
      "Phi3ForCausalLM(\n",
      "  (model): Phi3Model(\n",
      "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
      "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x Phi3DecoderLayer(\n",
      "        (self_attn): Phi3Attention(\n",
      "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)        <1>\n",
      "          (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)      <1>\n",
      "          (rotary_emb): Phi3RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Phi3MLP(\n",
      "          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False) <1>\n",
      "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)     <1>\n",
      "          (activation_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
      ")\n",
      "\n",
      "<1> Quantized layers\n",
      "\n",
      "A quantized model can be used directly for inference, but it cannot be trained any further. Those pesky Linear4bit layers take up much less space, which is the whole point of quantization; however, we cannot update them.\n",
      "We need to add something else to our mix, a sprinkle of adapters.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tSetting Up Low-Rank Adapters (LoRA)\n",
      "\t\n",
      "\n",
      "Low-rank adapters can be attached to each and every one of the quantized layers. The adapters are mostly regular Linear  layers that can be easily updated as usual. The clever trick in this case is that these adapters are significantly smaller than the layers that have been quantized.\n",
      "Since the quantized layers are frozen (they cannot be updated), setting up LoRA adapters on a quantized model drastically reduces the total number of trainable parameters to just 1% (or less) of its original size.\n",
      "We can set up LoRA adapters in three easy steps:\n",
      "\n",
      "Call prepare_model_for_kbit_training() to improve numerical stability during training.\n",
      "Create an instance of LoraConfig.\n",
      "Apply the configuration to the quantized base model using the get_peft_model() method.\n",
      "\n",
      "Let's try it out with our model:\n",
      "model = prepare_model_for_kbit_training(model)\n",
      "\n",
      "config = LoraConfig(\n",
      "    # the rank of the adapter, the lower the fewer parameters you'll need to train\n",
      "    r=8,                   \n",
      "    lora_alpha=16, # multiplier, usually 2*r\n",
      "    bias=\"none\",           \n",
      "    lora_dropout=0.05,\n",
      "    task_type=\"CAUSAL_LM\",\n",
      "    # Newer models, such as Phi-3 at time of writing, may require \n",
      "    # manually setting target modules\n",
      "    target_modules=['o_proj', 'qkv_proj', 'gate_up_proj', 'down_proj'],\n",
      ")\n",
      "model = get_peft_model(model, config)\n",
      "model\n",
      "\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Phi3ForCausalLM(\n",
      "      (model): Phi3Model(\n",
      "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
      "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x Phi3DecoderLayer(\n",
      "            (self_attn): Phi3Attention(\n",
      "              (o_proj): lora.Linear4bit(                      <1>\n",
      "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "                (lora_dropout): ModuleDict((default): Dropout(p=0.05, inplace=False))\n",
      "                (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (qkv_proj): lora.Linear4bit(...)                <1>\n",
      "              (rotary_emb): Phi3RotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Phi3MLP(\n",
      "              (gate_up_proj): lora.Linear4bit(...)            <1>\n",
      "              (down_proj): lora.Linear4bit(...)               <1>\n",
      "              (activation_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "<1> LoRA adapters\n",
      "\n",
      "The output of the other three LoRA layers (qkv_proj, gate_up_proj, and down_proj) was suppressed to shorten the output.\n",
      "\n",
      "\n",
      "    Did you get the following error?\n",
      "    \n",
      "\n",
      "ValueError: Please specify `target_modules` in `peft_config`\n",
      "\n",
      "\n",
      "    Most likely, you don't need to specify the target_modules if you're using one of the well-known models. The peft library takes care of it by automatically choosing the appropriate targets. However, there may be a gap between the time a popular model is released and the time the library gets updated. So, if you get the error above, look for the quantized layers in your model and list their names in the target_modules argument.\n",
      "  \n",
      "\n",
      "The quantized layers (Linear4bit) have turned into lora.Linear4bit modules where the quantized layer itself became the base_layer with some regular Linear layers (lora_A and lora_B) added to the mix.\n",
      "These extra layers would make the model only slightly larger. However, the model preparation function (prepare_model_for_kbit_training()) turned every non-quantized layer to full precision (FP32), thus resulting in a 30% larger model:\n",
      "print(model.get_memory_footprint()/1e6)\n",
      "\n",
      "2651.080704\n",
      "\n",
      "Since most parameters are frozen, only a tiny fraction of the total number of parameters are currently trainable, thanks to LoRA!\n",
      "train_p, tot_p = model.get_nb_trainable_parameters()\n",
      "print(f'Trainable parameters:      {train_p/1e6:.2f}M')\n",
      "print(f'Total parameters:          {tot_p/1e6:.2f}M')\n",
      "print(f'% of trainable parameters: {100*train_p/tot_p:.2f}%')\n",
      "\n",
      "Trainable parameters:      12.58M\n",
      "Total parameters:          3833.66M\n",
      "% of trainable parameters: 0.33%\n",
      "\n",
      "The model is ready to be fine-tuned, but we are still missing one key component: our dataset.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tFormatting Your Dataset\n",
      "\t\n",
      "\n",
      "\n",
      "\n",
      "\"Like Yoda, speak, you must. Hrmmm.\"\n",
      "\n",
      "\n",
      "    Master Yoda\n",
      "  \n",
      "\n",
      "The dataset yoda_sentences consists of 720 sentences translated from English to Yoda-speak. The dataset is hosted on the Hugging Face Hub and we can easily load it using the load_dataset() method from the Hugging Face datasets library:\n",
      "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
      "dataset\n",
      "\n",
      "Dataset({\n",
      "features: ['sentence', 'translation', 'translation_extra'],\n",
      "num_rows: 720\n",
      "})\n",
      "\n",
      "The dataset has three columns:\n",
      "\n",
      "original English sentence (sentence)\n",
      "basic translation to Yoda-speak (translation)\n",
      "enhanced translation including typical Yesss and Hrrmm interjections (translation_extra)\n",
      "\n",
      "dataset[0]\n",
      "\n",
      "{'sentence': 'The birch canoe slid on the smooth planks.',\n",
      "'translation': 'On the smooth planks, the birch canoe slid.',\n",
      "'translation_extra': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.'}\n",
      "\n",
      "The SFTTrainer we'll be using to fine-tune the model can automatically handle datasets either in conversational or instruction formats.\n",
      "\n",
      "conversational format\n",
      "\n",
      "{\"messages\":[\n",
      "  {\"role\": \"system\", \"content\": \"<general directives>\"},\n",
      "  {\"role\": \"user\", \"content\": \"<prompt text>\"},\n",
      "  {\"role\": \"assistant\", \"content\": \"<ideal generated text>\"}\n",
      "]}\n",
      "\n",
      "\n",
      "instruction format:  [unfortunately, recent versions of trl do not support this format properly anymore]\n",
      "\n",
      "{\"prompt\": \"<prompt text>\",\n",
      "\"completion\": \"<ideal generated text>\"}\n",
      "\n",
      "\n",
      "IMPORTANT UPDATE: unfortunately, in more recent versions of the trl library, the \"instruction\" format is not properly supported anymore, thus leading to the chat template not being applied to the dataset. In order to avoid this issue, we can convert the dataset to the \"conversational\" format.\n",
      "\n",
      "First, we'll simply rename and keep the relevant columns from our dataset:\n",
      "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
      "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
      "dataset = dataset.remove_columns([\"translation\"])\n",
      "dataset\n",
      "\n",
      "Dataset({\n",
      "    features: ['prompt', 'completion'],\n",
      "    num_rows: 720\n",
      "})\n",
      "\n",
      "dataset[0]\n",
      "\n",
      "{'prompt': 'The birch canoe slid on the smooth planks.',\n",
      "'completion': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.'}\n",
      "\n",
      "Next, we'll convert the dataset to the conversational format using the format_dataset() function below:\n",
      "# Adapted from trl.extras.dataset_formatting.instructions_formatting_function\n",
      "# Converts dataset from prompt/completion format (not supported anymore)\n",
      "# to the conversational format\n",
      "def format_dataset(examples):\n",
      "    if isinstance(examples[\"prompt\"], list):\n",
      "        output_texts = []\n",
      "        for i in range(len(examples[\"prompt\"])):\n",
      "            converted_sample = [\n",
      "                {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\n",
      "                {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\n",
      "            ]\n",
      "            output_texts.append(converted_sample)\n",
      "        return {'messages': output_texts}\n",
      "    else:\n",
      "        converted_sample = [\n",
      "            {\"role\": \"user\", \"content\": examples[\"prompt\"]},\n",
      "            {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\n",
      "        ]\n",
      "        return {'messages': converted_sample}\n",
      "\n",
      "dataset = dataset.map(format_dataset).remove_columns(['prompt', 'completion'])\n",
      "dataset[0]['messages']\n",
      "\n",
      "[{'role': 'user', \n",
      "  'content': 'The birch canoe slid on the smooth planks.'},\n",
      " {'role': 'assistant',\n",
      "  'content': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.'}]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tTokenizer\n",
      "\t\n",
      "\n",
      "Before moving into the actual training, we still need to load the tokenizer that corresponds to our model. The tokenizer is an important part of this process, determining how to convert text into tokens in the same way used to train the model.\n",
      "For instruction/chat models, the tokenizer also contains its corresponding chat template that specifies:\n",
      "\n",
      "Which special tokens should be used, and where they should be placed.\n",
      "Where the system directives, user prompt, and model response should be placed.\n",
      "What is the generation prompt, that is, the special token that triggers the model's response (more on that in the \"Querying the Model\" section)\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
      "tokenizer.chat_template\n",
      "\n",
      "\"{% for message in messages %}\n",
      "    {% if message['role'] ## 'system' %}\n",
      "      {{'<|system|>\\n' + message['content'] + '<|end|>\\n'}}\n",
      "    {% elif message['role'] ## 'user' %}\n",
      "      {{'<|user|>\\n' + message['content'] + '<|end|>\\n'}}\n",
      "    {% elif message['role'] ## 'assistant' %}\n",
      "      {{'<|assistant|>\\n' + message['content'] + '<|end|>\\n'}}\n",
      "    {% endif %}\n",
      "{% endfor %}\n",
      "{% if add_generation_prompt %}\n",
      "  {{ '<|assistant|>\\n' }}{% else %}{{ eos_token }}\n",
      "{% endif %}\"\n",
      "\n",
      "Never mind the seemingly overcomplicated template (I have added line breaks and indentation to it so it's easier to read). It simply organizes the messages into a coherent block with the appropriate tags, as shown below (tokenize=False ensures we get readable text back instead of a numeric sequence of token IDs):\n",
      "print(tokenizer.apply_chat_template(messages, tokenize=False))\n",
      "\n",
      "<|user|>\n",
      "The birch canoe slid on the smooth planks.<|end|>\n",
      "<|assistant|>\n",
      "On the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|>\n",
      "<|endoftext|>\n",
      "\n",
      "Notice that each interaction is wrapped in either <|user|> or <|assistant|> tokens at the beginning and <|end|> at the end. Moreover, the <|endoftext|> token indicates the end of the whole block.\n",
      "Different models will have different templates and tokens to indicate the beginning and end of sentences and blocks.\n",
      "\n",
      "IMPORTANT UPDATE: due to changes in the default collator used by the SFTTrainer class while building the dataset, the EOS token (which is, in Phi-3, the same as the PAD token) was masked in the labels too thus leading to the model not being able to properly stop token generation.\n",
      "In order to address this change, we can assign the UNK token to the PAD token, so the EOS token becomes unique and therefore not masked as part of the labels.\n",
      "\n",
      "tokenizer.pad_token = tokenizer.unk_token\n",
      "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
      "\n",
      "We're now ready to tackle the actual fine-tuning!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tFine-Tuning with SFTTrainer\n",
      "\t\n",
      "\n",
      "Fine-tuning a model, whether large or otherwise, follows exactly the same training procedure as training a model from scratch. We could write our own training loop in pure PyTorch, or we could use Hugging Face's Trainer to fine-tune our model.\n",
      "It is much easier, however, to use SFTTrainer instead (which uses Trainer underneath, by the way), since it takes care of most of the nitty-gritty details for us, as long as we provide it with the following four arguments:\n",
      "\n",
      "a model \n",
      "a tokenizer \n",
      "a dataset \n",
      "a configuration object\n",
      "\n",
      "We've already got the first three elements; let's work on the last one.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tSFTConfig\n",
      "\t\n",
      "\n",
      "There are many parameters that we can set in the configuration object. We have divided them into four groups:\n",
      "\n",
      "Memory usage optimization parameters related to gradient accumulation and checkpointing\n",
      "Dataset-related arguments, such as the max_seq_length required by your data, and whether you are packing or not the sequences\n",
      "Typical training parameters such as the learning_rate and the num_train_epochs\n",
      "Environment and logging parameters such as output_dir (this will be the name of the model if you choose to push it to the Hugging Face Hub once it's trained), logging_dir, and logging_steps.\n",
      "\n",
      "While the learning rate is a very important parameter (as a starting point, you can try the learning rate used to train the base model in the first place), it's actually the maximum sequence length that's more likely to cause out-of-memory issues.\n",
      "Make sure to always pick the shortest possible max_seq_length that makes sense for your use case. In ours, the sentences—both in English and Yoda-speak—are quite short, and a sequence of 64 tokens is more than enough to cover the prompt, the completion, and the added special tokens.\n",
      "\n",
      "\n",
      "    Flash attention (which, unfortunately, isn't supported in Colab), allows for more flexibility in working with longer sequences, avoiding the potential issue of OOM errors.\n",
      "  \n",
      "\n",
      "sft_config = SFTConfig(\n",
      "    ## GROUP 1: Memory usage\n",
      "    # These arguments will squeeze the most out of your GPU's RAM\n",
      "    # Checkpointing\n",
      "    gradient_checkpointing=True,    # this saves a LOT of memory\n",
      "    # Set this to avoid exceptions in newer versions of PyTorch\n",
      "    gradient_checkpointing_kwargs={'use_reentrant': False}, \n",
      "    # Gradient Accumulation / Batch size\n",
      "    # Actual batch (for updating) is same (1x) as micro-batch size\n",
      "    gradient_accumulation_steps=1,  \n",
      "    # The initial (micro) batch size to start off with\n",
      "    per_device_train_batch_size=16, \n",
      "    # If batch size would cause OOM, halves its size until it works\n",
      "    auto_find_batch_size=True,\n",
      "\n",
      "    ## GROUP 2: Dataset-related\n",
      "    max_seq_length=64,\n",
      "    # Dataset\n",
      "    # packing a dataset means no padding is needed\n",
      "    packing=True,\n",
      "\n",
      "    ## GROUP 3: These are typical training parameters\n",
      "    num_train_epochs=10,\n",
      "    learning_rate=3e-4,\n",
      "    # Optimizer\n",
      "    # 8-bit Adam optimizer - doesn't help much if you're using LoRA!\n",
      "    optim='paged_adamw_8bit',       \n",
      "    \n",
      "    ## GROUP 4: Logging parameters\n",
      "    logging_steps=10,\n",
      "    logging_dir='./logs',\n",
      "    output_dir='./phi3-mini-yoda-adapter',\n",
      "    report_to='none'\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tSFTTrainer\n",
      "\t\n",
      "\n",
      "\n",
      "\n",
      "\"It is training time!\"\n",
      "\n",
      "\n",
      "    The Hulk\n",
      "  \n",
      "\n",
      "We can now finally create an instance of the supervised fine-tuning trainer:\n",
      "trainer = SFTTrainer(\n",
      "    model=model,\n",
      "    processing_class=tokenizer,\n",
      "    args=sft_config,\n",
      "    train_dataset=dataset,\n",
      ")\n",
      "\n",
      "The SFTTrainer had already preprocessed our dataset, so we can take a look inside and see how each mini-batch was assembled:\n",
      "dl = trainer.get_train_dataloader()\n",
      "batch = next(iter(dl))\n",
      "\n",
      "Let's check the labels; after all, we didn't provide any, did we?\n",
      "batch['input_ids'][0], batch['labels'][0]\n",
      "\n",
      "(tensor([ 1746, 29892,   278, 10435,  3147,   698,   287, 29889,  32007, 32000, 32000, \n",
      "  32010, 10987,   278,  3252,   262,  1058,   380,  1772,   278,  282,   799,   29880,\n",
      "  18873,  1265, 29889, 32007, 32001, 11644,   380,  1772,   278,  282,   799,   29880,\n",
      "  18873,  1265, 29892,  1284,   278,  3252,   262, 29892,   366,  1818, 29889,   3869,\n",
      "  29892,   298, 21478,  1758, 29889, 32007, 32000, 32000, 32010,   315,   329,    278,\n",
      "  13793,   393,  7868, 29879,   278], device='cuda:0'),\n",
      " tensor([ 1746, 29892,   278, 10435,  3147,   698,   287, 29889,  32007, 32000, 32000, \n",
      "  32010, 10987,   278,  3252,   262,  1058,   380,  1772,   278,  282,   799,   29880,\n",
      "  18873,  1265, 29889, 32007, 32001, 11644,   380,  1772,   278,  282,   799,   29880,\n",
      "  18873,  1265, 29892,  1284,   278,  3252,   262, 29892,   366,  1818, 29889,   3869,\n",
      "  29892,   298, 21478,  1758, 29889, 32007, 32000, 32000, 32010,   315,   329,    278,\n",
      "  13793,   393,  7868, 29879,   278], device='cuda:0'))\n",
      "\n",
      "The labels were added automatically, and they're exactly the same as the inputs. Thus, this is a case of self-supervised fine-tuning.\n",
      "The shifting of the labels will be handled automatically as well; there's no need to be concerned about it.\n",
      "\n",
      "\n",
      "    Although this is a 3.8 billion-parameter model, the configuration above allows us to squeeze training, using a mini-batch of eight, into an old setup with a consumer-grade GPU such as a GTX 1060 with only 6 GB RAM. True story!\n",
      "    \n",
      "    It takes about 35 minutes to complete the training process.\n",
      "  \n",
      "\n",
      "Next, we call the train() method and wait:\n",
      "trainer.train()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Step\n",
      "Training Loss\n",
      "\n",
      "\n",
      "10\n",
      "2.990700\n",
      "\n",
      "\n",
      "20\n",
      "1.789500\n",
      "\n",
      "\n",
      "30\n",
      "1.581700\n",
      "\n",
      "\n",
      "40\n",
      "1.458300\n",
      "\n",
      "\n",
      "50\n",
      "1.362300\n",
      "\n",
      "\n",
      "100\n",
      "0.607900\n",
      "\n",
      "\n",
      "150\n",
      "0.353600\n",
      "\n",
      "\n",
      "200\n",
      "0.277500\n",
      "\n",
      "\n",
      "220\n",
      "0.252400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tQuerying the Model\n",
      "\t\n",
      "\n",
      "Now, our model should be able to produce a Yoda-like sentence as a response to any short sentence we give it.\n",
      "So, the model requires its inputs to be properly formatted. We need to build a list of \"messages\"—ours, from the user, in this case—and prompt the model to answer by indicating it's its turn to write.\n",
      "This is the purpose of the add_generation_prompt argument: it adds <|assistant|> to the end of the conversation, so the model can predict the next word—and continue doing so until it predicts an <|endoftext|> token.\n",
      "The helper function below assembles a message (in the conversational format) and applies the chat template to it, appending the generation prompt to its end.\n",
      "def gen_prompt(tokenizer, sentence):\n",
      "    converted_sample = [{\"role\": \"user\", \"content\": sentence}]\n",
      "    prompt = tokenizer.apply_chat_template(\n",
      "        converted_sample, tokenize=False, add_generation_prompt=True\n",
      "    )\n",
      "    return prompt\n",
      "\n",
      "Let's try generating a prompt for an example sentence:\n",
      "sentence = 'The Force is strong in you!'\n",
      "prompt = gen_prompt(tokenizer, sentence)\n",
      "print(prompt)\n",
      "\n",
      "<|user|>\n",
      "The Force is strong in you!<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "The prompt seems about right; let's use it to generate a completion. The helper function below does the following:\n",
      "\n",
      "It tokenizes the prompt into a tensor of token IDs (add_special_tokens is set to False because the tokens were already added by the chat template).\n",
      "It sets the model to evaluation mode.\n",
      "It calls the model's generate() method to produce the output (generated token IDs).\n",
      "It decodes the generated token IDs back into readable text.\n",
      "\n",
      "def generate(model, tokenizer, prompt, max_new_tokens=64, skip_special_tokens=False):\n",
      "    tokenized_input = tokenizer(\n",
      "        prompt, add_special_tokens=False, return_tensors=\"pt\"\n",
      "    ).to(model.device)\n",
      "\n",
      "    model.eval()\n",
      "    gen_output = model.generate(**tokenized_input,\n",
      "                                eos_token_id=tokenizer.eos_token_id,\n",
      "                                max_new_tokens=max_new_tokens)\n",
      "    \n",
      "    output = tokenizer.batch_decode(gen_output, skip_special_tokens=skip_special_tokens)\n",
      "    return output[0]\n",
      "\n",
      "Now, we can finally try out our model and see if it's indeed capable of generating Yoda-speak.\n",
      "print(generate(model, tokenizer, prompt))\n",
      "\n",
      "<|user|> The Force is strong in you!<|end|><|assistant|> Strong in you, the Force is. Yes, hrrmmm.<|end|>\n",
      "\n",
      "Awesome! It works! Like Yoda, the model speaks. Hrrrmm.\n",
      "Congratulations, you've fine-tuned your first LLM!\n",
      "Now, you've got a small adapter that can be loaded into an instance of the Phi-3 Mini 4K Instruct model to turn it into a Yoda translator! How cool is that?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tSaving the Adapter\n",
      "\t\n",
      "\n",
      "Once the training is completed, you can save the adapter (and the tokenizer) to disk by calling the trainer's save_model() method. It will save everything to the specified folder:\n",
      "trainer.save_model('local-phi3-mini-yoda-adapter')\n",
      "\n",
      "The files that were saved include:\n",
      "\n",
      "the  adapter configuration (adapter_config.json) and weights (adapter_model.safetensors)—the adapter itself is just 50 MB in size\n",
      "the training arguments (training_args.bin)\n",
      "the tokenizer (tokenizer.json and tokenizer.model), its configuration (tokenizer_config.json), and its special tokens (added_tokens.json and speciak_tokens_map.json) \n",
      "a README file\n",
      "\n",
      "If you'd like to share your adapter with everyone, you can also push it to the Hugging Face Hub. First, log in using a token that has permission to write:\n",
      "from huggingface_hub import login\n",
      "login()\n",
      "\n",
      "The code above will ask you to enter an access token:\n",
      "\n",
      "A successful login should look like this (pay attention to the permissions): \n",
      "\n",
      "Then, you can use the trainer's push_to_hub() method to upload everything to your account in the Hub. The model will be named after the output_dir argument of the training arguments:\n",
      "trainer.push_to_hub()\n",
      "\n",
      "There you go! Our model is out there in the world, and anyone can use it to translate English into Yoda speak. \n",
      "That's a wrap!\n",
      "Did you like this post? You can learn much more about fine-tuning in my latest book: A Hands-On Guide to Fine-Tuning Large Language Models with PyTorch and Hugging Face.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tSubscribe Follow Connect\n",
      "\t\n",
      "\n",
      "\n",
      "\n",
      "dvgodoy.com\n",
      "X\n",
      "GitHub\n",
      "LinkedIn\n",
      "\n",
      "\n",
      "Community\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "shoveling42\n",
      "\n",
      "Feb 19\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Thank you for sharing your tutorial!!\n",
      "\n",
      "\n",
      "\n",
      "👍\n",
      "3\n",
      "3\n",
      "\t\t\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "+\n",
      "\n",
      "Reply\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ghostoverflow\n",
      "\n",
      "Mar 13\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Heree's a requirements.txt that worked for me, slightly updated:\n",
      "flash-attn==2.7.4.post1\n",
      "scipy==1.15.2\n",
      "torch==2.6.0\n",
      "cffi==1.17.1\n",
      "transformers==4.49.0\n",
      "peft==0.14.0\n",
      "accelerate==1.5.1\n",
      "trl==0.15.2\n",
      "bitsandbytes==0.45.3\n",
      "datasets==3.3.2\n",
      "huggingface-hub==0.29.3\n",
      "safetensors==0.5.3\n",
      "pandas==2.2.3\n",
      "matplotlib==3.10.1\n",
      "numpy==1.26.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reply\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "smolgabs\n",
      "\n",
      "Jul 2\n",
      "•\n",
      "edited Jul 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tysm, can't wait to play around with this!\n",
      "you explained everything amazingly!\n",
      "it works it works it works!!! <3 <3 <3\n",
      "for anyone having issues with triton, do 'pip install triton-windows'\n",
      "also for bitsandbytes issues on windows saying it can't find the cuda, version 0.46 should work.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reply\n",
      "\n",
      "\n",
      "EditPreview\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload images, audio, and videos by dragging in the text input, pasting, or clicking here.\n",
      "\t\t\t\n",
      "Tap or paste here to upload images\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tComment\n",
      "\t\t\t\n",
      "·\n",
      "Sign up or\n",
      "\t\t\t\t\tlog in to comment\n",
      "\t\t\t\t\n",
      "\n",
      "\n",
      "\t\tUpvote\n",
      "\n",
      "\t\t58\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "+46\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tSystem theme\n",
      "\t\t\n",
      "\n",
      "Company\n",
      "TOS\n",
      "Privacy\n",
      "About\n",
      "Jobs\n",
      "\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Pricing\n",
      "Docs\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Introduction - Hugging Face LLM Course\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hugging Face\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tModels\n",
      "\n",
      "\t\t\t\t\t\tDatasets\n",
      "\n",
      "\t\t\t\t\t\tSpaces\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tCommunity\n",
      "\t\t\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tDocs\n",
      "\n",
      "\t\t\t\t\t\tEnterprise\n",
      "\n",
      "Pricing\n",
      "\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log In\n",
      "\t\t\t\t\n",
      "Sign Up\n",
      "\t\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "LLM Course documentation\n",
      "\t\t\t\n",
      "Introduction\n",
      "\n",
      "\n",
      "\n",
      "LLM Course\n",
      "\n",
      "🏡 View all resourcesAgents CourseAudio CourseCommunity Computer Vision CourseDeep RL CourseDiffusion CourseLLM CourseMCP CourseML for 3D CourseML for Games CourseOpen-Source AI Cookbook\n",
      "\n",
      "Search documentation\n",
      "\n",
      "\n",
      "ARBNDEENESFAFRGJHEHIIDITJAKONEPLPTRURUMTETHTRVIZH-CNZH-TW\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0. Setup\n",
      "\n",
      "\n",
      "1. Transformer models\n",
      "\n",
      "\n",
      "2. Using 🤗 Transformers\n",
      "\n",
      "\n",
      "3. Fine-tuning a pretrained model\n",
      "\n",
      "\n",
      "Introduction\n",
      "Processing the data\n",
      "Fine-tuning a model with the Trainer API\n",
      "A full training loop\n",
      "Understanding Learning Curves\n",
      "Fine-tuning, Check!\n",
      "End-of-chapter quiz\n",
      "\n",
      "\n",
      "4. Sharing models and tokenizers\n",
      "\n",
      "\n",
      "5. The 🤗 Datasets library\n",
      "\n",
      "\n",
      "6. The 🤗 Tokenizers library\n",
      "\n",
      "\n",
      "7. Classical NLP tasks\n",
      "\n",
      "\n",
      "8. How to ask for help\n",
      "\n",
      "\n",
      "9. Building and sharing demos\n",
      "\n",
      "\n",
      "10. Curate high-quality datasets\n",
      "\n",
      "\n",
      "11. Fine-tune Large Language Models\n",
      "\n",
      "\n",
      "12. Build Reasoning Models\n",
      "new\n",
      "\n",
      "\n",
      "Course Events\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Join the Hugging Face community\n",
      "and get access to the augmented documentation experience\n",
      "\t\t\n",
      "\n",
      "Collaborate on models, datasets and Spaces\n",
      "\t\t\t\t\n",
      "\n",
      "Faster examples with accelerated inference\n",
      "\t\t\t\t\n",
      "\n",
      "Switch between documentation themes\n",
      "\t\t\t\t\n",
      "Sign Up\n",
      "to get started\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   Pytorch  TensorFlow   Introduction   In Chapter 2 we explored how to use tokenizers and pretrained models to make predictions. But what if you want to fine-tune a pretrained model to solve a specific task? That’s the topic of this chapter! You will learn: How to prepare a large dataset from the Hub using the latest 🤗 Datasets features How to use the high-level Trainer API to fine-tune a model with modern best practices How to implement a custom training loop with optimization techniques How to leverage the 🤗 Accelerate library to easily run distributed training on any setup How to apply current fine-tuning best practices for maximum performance 📚 Essential Resources: Before starting, you might want to review the 🤗 Datasets documentation for data processing. This chapter will also serve as an introduction to some Hugging Face libraries beyond the 🤗 Transformers library! We’ll see how libraries like 🤗 Datasets, 🤗 Tokenizers, 🤗 Accelerate, and 🤗 Evaluate can help you train models more efficiently and effectively. Each of the main sections in this chapter will teach you something different: Section 2: Learn modern data preprocessing techniques and efficient dataset handling Section 3: Master the powerful Trainer API with all its latest features Section 4: Implement training loops from scratch and understand distributed training with Accelerate By the end of this chapter, you’ll be able to fine-tune models on your own datasets using both high-level APIs and custom training loops, applying the latest best practices in the field. 🎯 What You’ll Build: By the end of this chapter, you’ll have fine-tuned a BERT model for text classification and understand how to adapt the techniques to your own datasets and tasks. This chapter focuses exclusively on PyTorch, as it has become the standard framework for modern deep learning research and production. We’ll use the latest APIs and best practices from the Hugging Face ecosystem. To upload your trained models to the Hugging Face Hub, you will need a Hugging Face account: create an account < > Update on GitHub \n",
      "\n",
      "\n",
      "←End-of-chapter quiz\n",
      "Processing the data→\n",
      "\n",
      "\n",
      "Introduction\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for page in web_data:\n",
    "    print(page.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0036c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db849cd4",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aee62aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5835943f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyDCILMWAY0MDlBVtB4GXGjIWYb_pon9iBA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "print(GOOGLE_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2a8900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",\n",
    "                                          google_api_key=GOOGLE_API_KEY)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=20)\n",
    "pdf_chunks = text_splitter.split_documents(pdf_data)\n",
    "web_chunks = text_splitter.split_documents(web_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc3d0e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7b4264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66659/93803870.py:5: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  ).persist()\n"
     ]
    }
   ],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=pdf_chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cee30d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c81d1336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66659/277266192.py:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(persist_directory=persist_directory,\n"
     ]
    }
   ],
   "source": [
    "vectordb = Chroma(persist_directory=persist_directory,\n",
    "                  embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23a1fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d4c8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = retriever.invoke(\"OOP in Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15bf2924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a3b7442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0.7,\n",
    "    reasoning_effort=\"medium\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "fc9d28a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       retriever = retriever,\n",
    "                                       return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "4c8bdbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Quick Introduction to FastAPI**\\n\\nFastAPI is a modern, fast (high‑performance) web framework for building APIs with Python 3.7+ based on standard Python type hints. It was created by Sebastián Ramírez and is heavily inspired by Flask, Starlette, and Pydantic.\\n\\n| Feature | What It Means |\\n|---------|---------------|\\n| **Fast** | Uses Starlette for the web parts and Pydantic for data validation → built on async IO, so it can handle many requests concurrently. |\\n| **Easy** | Leverages Python type hints to automatically validate request bodies, query parameters, headers, etc. |\\n| **Documentation** | Auto‑generates interactive API docs (Swagger UI & ReDoc) from your code. |\\n| **Production‑ready** | Works with ASGI servers like Uvicorn or Hypercorn; integrates with ORMs, authentication, background tasks, etc. |\\n\\n---\\n\\n### 1. Basic Project Structure\\n\\n```\\napp/\\n├─ main.py          # FastAPI app definition\\n├─ models.py        # Pydantic schemas\\n├─ crud.py          # Database logic (optional)\\n└─ db.py            # Database connection\\n```\\n\\n---\\n\\n### 2. Core Concepts\\n\\n#### a. **Endpoints**\\n\\n```python\\nfrom fastapi import FastAPI\\n\\napp = FastAPI()\\n\\n@app.get(\"/items/{item_id}\")\\ndef read_item(item_id: int):\\n    return {\"item_id\": item_id}\\n```\\n\\n- `@app.get`, `@app.post`, `@app.put`, `@app.delete` declare HTTP methods.\\n- Path parameters are typed (`item_id: int`) → FastAPI auto‑converts and validates.\\n\\n#### b. **Request & Response Models**\\n\\n```python\\nfrom pydantic import BaseModel\\n\\nclass MovieReview(BaseModel):\\n    title: str\\n    rating: float\\n    comment: str\\n\\nclass DbReview(MovieReview):\\n    id: int\\n```\\n\\n- `BaseModel` classes define the JSON schema for requests and responses.\\n- FastAPI uses these models for automatic validation and documentation.\\n\\n#### c. **POST Example (Create a Review)**\\n\\n```python\\nfrom fastapi import FastAPI\\nfrom .models import MovieReview, DbReview\\nimport crud  # your database helper\\n\\napp = FastAPI()\\n\\n@app.post(\"/reviews\", response_model=DbReview)\\ndef create_review(review: MovieReview):\\n    db_review = crud.create_review(review)   # Persist to DB\\n    return db_review\\n```\\n\\n- `review: MovieReview` tells FastAPI to parse the incoming JSON into a `MovieReview` instance.\\n- `response_model=DbReview` tells FastAPI to serialize the returned object as `DbReview` (adds the `id`).\\n\\n#### d. **CRUD Utilities**\\n\\nFastAPI’s docs show a simple `crud.py` pattern:\\n\\n```python\\ndef create_review(review: MovieReview) -> DbReview:\\n    # Insert into database, generate ID, return full object\\n    ...\\n```\\n\\n---\\n\\n### 3. Running the App\\n\\n```bash\\nuvicorn app.main:app --reload\\n```\\n\\n- `--reload` watches for code changes (useful in dev).\\n\\n---\\n\\n### 4. Auto‑Generated Docs\\n\\nOnce running, visit:\\n\\n- Swagger UI: `http://localhost:8000/docs`\\n- ReDoc: `http://localhost:8000/redoc`\\n\\nThese pages let you interactively test your endpoints and see the exact request/response schemas.\\n\\n---\\n\\n### 5. Common Use‑Cases\\n\\n| Use‑Case | FastAPI Feature |\\n|----------|-----------------|\\n| **Database Integration** | Use `SQLAlchemy` or `Tortoise‑ORM` with async support. |\\n| **Dependency Injection** | `Depends` for reusable components (auth, DB session). |\\n| **Background Tasks** | `BackgroundTasks` to run jobs after a response is sent. |\\n| **Authentication** | OAuth2, JWT, or custom middleware. |\\n| **Testing** | `TestClient` from `fastapi.testclient`. |\\n\\n---\\n\\n### 6. Quick Starter Code\\n\\n```python\\n# app/main.py\\nfrom fastapi import FastAPI\\nfrom .models import MovieReview, DbReview\\nimport crud\\n\\napp = FastAPI()\\n\\n@app.post(\"/reviews\", response_model=DbReview)\\ndef create_review(review: MovieReview):\\n    return crud.create_review(review)\\n\\n@app.get(\"/reviews/{review_id}\")\\ndef read_review(review_id: int):\\n    return crud.get_review(review_id)\\n```\\n\\n```python\\n# app/models.py\\nfrom pydantic import BaseModel\\n\\nclass MovieReview(BaseModel):\\n    title: str\\n    rating: float\\n    comment: str\\n\\nclass DbReview(MovieReview):\\n    id: int\\n```\\n\\n```python\\n# app/crud.py\\nfrom .models import MovieReview, DbReview\\nfrom typing import Dict\\n\\n# Fake in‑memory store for demo purposes\\n_store: Dict[int, DbReview] = {}\\n_next_id = 1\\n\\ndef create_review(review: MovieReview) -> DbReview:\\n    global _next_id\\n    db_review = DbReview(id=_next_id, **review.dict())\\n    _store[_next_id] = db_review\\n    _next_id += 1\\n    return db_review\\n\\ndef get_review(review_id: int) -> DbReview:\\n    return _store.get(review_id)\\n```\\n\\nRun `uvicorn app.main:app --reload` and test via `curl` or the interactive docs.\\n\\n---\\n\\n### 7. Resources\\n\\n- **Official docs**: https://fastapi.tiangolo.com/\\n- **Tutorials**: “FastAPI tutorial: CRUD with SQLAlchemy” (tutorial on the docs site)\\n- **Community**: FastAPI Discord, GitHub Discussions\\n\\n---\\n\\nThat’s the essence of FastAPI: type‑safe, auto‑documented, async‑friendly, and easy to grow from a simple “Hello, world” to a full‑blown production API. Happy coding!'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.invoke(\"quick into\")[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f396fd",
   "metadata": {},
   "source": [
    "# Langchain Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "23c9c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"DirectEd-VA\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "0f1a799a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Core langchain imports\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# Memory\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory\n",
    "\n",
    "# Retrivers\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Langsmith\n",
    "from langsmith import traceable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "8a7bdb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "c8f6d255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "3ae71231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EducationalRetriever:\n",
    "    def __init__(self, retriever_instance):\n",
    "        self.retriever = retriever_instance\n",
    "\n",
    "    def get_documents(self,query: str) ->str:\n",
    "        docs = self.retriever.invoke(query)\n",
    "        return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    def __call__(self, query: str) -> str:\n",
    "        \"\"\"Makes this class directly callable as a Runnable.\"\"\"\n",
    "        docs = self.retriever.invoke(query)\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "dcbe214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdaptiveConversationChain:\n",
    "    def __init__(self, llm_model):\n",
    "        self.llm = llm_model\n",
    "        # Use ConversationalSummaryBufferMemory to manage long-term context\n",
    "        self.memory = ConversationSummaryBufferMemory(llm=self.llm, max_token_limit=1000)\n",
    "\n",
    "    def get_chain(self):\n",
    "        \"\"\"\n",
    "        Creates and returns the conversational chain. The prompt is designed to\n",
    "        receive history, context, and the user's question as direct inputs.\n",
    "        \"\"\"\n",
    "        prompt_template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \n",
    "             \"\"\"You are DirectEd, a friendly and knowledgeable AI tutor. \n",
    "             Use the following educational context to answer the student's question. \n",
    "             If you don't know the answer, politely say so. Keep your response concise and encouraging.\n",
    "             \n",
    "             Context: {context}\n",
    "             \n",
    "             Conversation History:\n",
    "             {history}\n",
    "             \"\"\"),\n",
    "            (\"user\", \"{question}\")\n",
    "        ])\n",
    "        \n",
    "        # The chain is a simple sequence that formats the prompt and gets a response.\n",
    "        # It relies on the caller to manage the inputs.\n",
    "        chain = prompt_template | self.llm | StrOutputParser()\n",
    "        return chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "86b2e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentGenerator:\n",
    "    def __init__(self, llm_model, retriever: EducationalRetriever):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm_model\n",
    "        self.quiz_chain = self._quiz_generation_chain()\n",
    "        self.answer_chain = self._answer_generation_chain()\n",
    "\n",
    "    @traceable(run_type=\"chain\")\n",
    "    def _answer_generation_chain(self):\n",
    "        prompt_template = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            You are an AI tutor. \n",
    "            Use ONLY the provided 'Content' to form your answer. \n",
    "            If the content does not contain the answer, say so.\n",
    "\n",
    "            Question: {question}\n",
    "            Content: {content}\n",
    "\n",
    "            Guidelines:\n",
    "            - Start with a beginner-friendly explanation.  \n",
    "            - Add deeper insights only if needed.  \n",
    "            - Use bullet points or step-by-step lists.  \n",
    "            - Include one real-world example or analogy.  \n",
    "            - Be concise and motivating.  \n",
    "            - Keep answers between **50 and 150 words**.  \n",
    "            \"\"\"\n",
    "        )\n",
    "        return {\n",
    "            \"question\": RunnablePassthrough(),\n",
    "            # Corrected: Use RunnableLambda to select the 'question' key.\n",
    "            \"content\": RunnableLambda(lambda x: x[\"question\"]) | self.retriever\n",
    "        } | prompt_template | self.llm | StrOutputParser()\n",
    "    \n",
    "    @traceable(run_type=\"chain\")\n",
    "    def _quiz_generation_chain(self):\n",
    "        prompt_template = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            Create 5 multiple-choice questions (MCQs) about the given topic.  \n",
    "            Each must include:\n",
    "            - Question text  \n",
    "            - Four options (A, B, C, D)  \n",
    "            - The correct answer clearly labeled  \n",
    "\n",
    "            Use ONLY the provided 'Content'.  \n",
    "            If content lacks enough info, say: \"Not enough information to create a quiz.\"\n",
    "\n",
    "            Topic: {topic}  \n",
    "            Content: {content}  \n",
    "            \"\"\"\n",
    "        )\n",
    "        return {\n",
    "            \"topic\": RunnablePassthrough(),\n",
    "            # Corrected: Use RunnableLambda to select the 'topic' key.\n",
    "            \"content\": RunnableLambda(lambda x: x[\"topic\"]) | self.retriever\n",
    "        } | prompt_template | self.llm | StrOutputParser()\n",
    "\n",
    "    @traceable(run_type=\"chain\")\n",
    "    def answer_generator(self, question: str) -> str:\n",
    "        return self.answer_chain.invoke({\"question\": question})\n",
    "\n",
    "    @traceable(run_type=\"chain\")\n",
    "    def generate_quiz(self, topic: str) -> str:\n",
    "        return self.quiz_chain.invoke({\"topic\": topic})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "6fa83244",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = ContentGenerator(llm, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "40bd7fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generator.answer_generator(\"Do you know fastapi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "5f1050b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes! FastAPI is a modern, high‑performance web framework for building APIs with Python.\\n\\n**Key points from the material**\\n\\n- **Fast** – Very high performance (built on Starlette & Pydantic).  \\n- **Low‑code, easy to learn** – Uses Python type hints and annotations.  \\n- **Robust** – Production‑ready with automatic documentation.  \\n- **Standards‑based** – Generates OpenAPI & JSON Schema docs automatically.  \\n\\n**Real‑world analogy**  \\nThink of FastAPI as a “smart kitchen” where you can quickly whip up a dish (API) that’s both fast and well‑documented, while the kitchen’s tools (type hints, auto‑docs) make cooking easier and safer.\\n\\nSo, if you’re looking to create clean, fast APIs in Python, FastAPI is the go‑to choice.'"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "a1ff71cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**1. What programming language is primarily highlighted as the framework for LangChain?**  \\nA) Python  \\nB) Ruby  \\nC) Go  \\nD) Swift  \\n*Correct answer: A)*  \\n\\n**2. Which loader is used in the example to fetch the MLflow documentation pages?**  \\nA) FileLoader  \\nB) WebBaseLoader  \\nC) CSVLoader  \\nD) JSONLoader  \\n*Correct answer: B)*  \\n\\n**3. In the provided code snippet, which embedding function is used to embed the documents before storing them in Chroma?**  \\nA) SentenceTransformerEmbeddings  \\nB) DatabricksEmbeddings  \\nC) OpenAIEmbeddings  \\nD) HuggingFaceEmbeddings  \\n*Correct answer: B)*  \\n\\n**4. The `RetrievalQA` chain in the example is set up to perform which type of task?**  \\nA) Text summarization  \\nB) Sentiment analysis  \\nC) Question‑answering  \\nD) Language translation  \\n*Correct answer: C)*  \\n\\n**5. Which of the following is listed as an advantage of LangChain chains?**  \\nA) Simple  \\nB) Legacy Class  \\nC) Optimized parallel execution  \\nD) Streaming  \\n*Correct answer: C)*'"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.generate_quiz(\"How to use langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "7d2f5ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningAnalyzer:\n",
    "    \"\"\"\n",
    "    A class that manages student progress and performance logs for multiple students.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Using a dictionary to store profiles for multiple students\n",
    "        self.student_data = {}\n",
    "\n",
    "    def get_profile(self, user_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Returns the profile for a specific user, creating one if it doesn't exist.\"\"\"\n",
    "        if user_id not in self.student_data:\n",
    "            # Create a new profile for the user if they don't exist\n",
    "            self.student_data[user_id] = {\"completed_quizzes\": [], \"struggling_topics\": []}\n",
    "        return self.student_data[user_id]\n",
    "        \n",
    "    def log_performance(self, user_id: str, topic: str, performance: str):\n",
    "        \"\"\"Logs and updates a specific student's data based on a new interaction.\"\"\"\n",
    "        profile = self.get_profile(user_id)  # This now gets the correct profile\n",
    "        if performance == \"correct\":\n",
    "            if topic not in profile[\"completed_quizzes\"]:\n",
    "                 profile[\"completed_quizzes\"].append(topic)\n",
    "        else:\n",
    "            if topic not in profile[\"struggling_topics\"]:\n",
    "                profile[\"struggling_topics\"].append(topic)\n",
    "\n",
    "        print(f\"\\n--- Log for User ID: {user_id} on {topic} ({performance}) ---\")\n",
    "        print(\"Updated Student Profile:\")\n",
    "        print(profile)\n",
    "        print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "e6e3f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = LearningAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c449e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "8f35533f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Log for User ID: 3 on Postuyan (incorrect) ---\n",
      "Updated Student Profile:\n",
      "{'completed_quizzes': [], 'struggling_topics': ['Postuyan']}\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "analyzer.log_performance(3, \"Postuyan\", \"incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "00211a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "a735204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "educational_retriver = EducationalRetriever(retriever)\n",
    "content_generator = ContentGenerator(llm, educational_retriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "e0b2e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(run_type=\"chain\")\n",
    "def run_educational_assistant(request: str, user_id:str,analyzer: LearningAnalyzer, is_instructor: bool = False) -> Dict[str, Any]:\n",
    "\n",
    "    router = PromptTemplate.from_template(\n",
    "        \"Analyze the request: '{request}'. Is it asking for a quiz, or a general explanation (tutoring)? Respond with 'QUIZ' or 'TUTORING'.\"\n",
    "    ) | llm | StrOutputParser()\n",
    "\n",
    "    try:\n",
    "        user_intent = router.invoke({\"request\": request}).strip().upper()\n",
    "        \n",
    "        output_content = \"\"\n",
    "        content_type = \"\"\n",
    "\n",
    "        if \"QUIZ\" in user_intent:\n",
    "            output_content = content_generator.generate_quiz(request)\n",
    "            content_type = \"QUIZ\"\n",
    "        else:\n",
    "            output_content = content_generator.answer_generator(request)\n",
    "            content_type = \"TUTORING\"\n",
    "\n",
    "        performance = \"correct\" if \"quiz\" in request.lower() else \"incorrect\"\n",
    "        analyzer.log_performance(user_id, request, performance)\n",
    "        response = {\n",
    "            \"user_type\": \"Instructor\" if is_instructor else \"Student\",\n",
    "            \"content_type\": content_type,\n",
    "            \"output\": output_content,\n",
    "            \"updated_profile\": analyzer.get_profile(user_id)\n",
    "        }\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return {\"Error!\": \"An error occurred during execution.\", \"details\": str(e)}\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "7bc66f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test Case 1: Student asks for an explanation.\n",
      "\n",
      "--- Log for User ID: student_123 on Explain the langchain's use using flashcards (incorrect) ---\n",
      "Updated Student Profile:\n",
      "{'completed_quizzes': [], 'struggling_topics': [\"Explain the langchain's use using flashcards\"]}\n",
      "-----------------------------------\n",
      "\n",
      "--- Final Response 1 ---\n",
      "{\n",
      "  \"user_type\": \"Student\",\n",
      "  \"content_type\": \"TUTORING\",\n",
      "  \"output\": \"I\\u2019m sorry, but the provided content does not mention flashcards or explain LangChain\\u2019s use with them.\",\n",
      "  \"updated_profile\": {\n",
      "    \"completed_quizzes\": [],\n",
      "    \"struggling_topics\": [\n",
      "      \"Explain the langchain's use using flashcards\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "==================================================\n",
      "\n",
      "Running Test Case 2: Student asks for a quiz.\n",
      "\n",
      "--- Log for User ID: student_123 on Give me a quiz about design. (correct) ---\n",
      "Updated Student Profile:\n",
      "{'completed_quizzes': ['Give me a quiz about design.'], 'struggling_topics': [\"Explain the langchain's use using flashcards\"]}\n",
      "-----------------------------------\n",
      "\n",
      "--- Final Response 2 ---\n",
      "{\n",
      "  \"user_type\": \"Student\",\n",
      "  \"content_type\": \"QUIZ\",\n",
      "  \"output\": \"**1.** Which of the following best describes the principle of prompt design mentioned in the content?  \\nA) Prompts should be concise, clear, and easy to understand for both the user and the model.  \\nB) Prompts should be long and detailed to cover all possible scenarios.  \\nC) Prompts should use complex language to challenge the model.  \\nD) Prompts should include unnecessary information to test model robustness.  \\n**Answer: A**\\n\\n---\\n\\n**2.** According to the content, what should you avoid when writing prompts?  \\nA) Using action verbs  \\nB) Providing concise instructions  \\nC) Using complex language and unnecessary information  \\nD) Keeping prompts short and clear  \\n**Answer: C**\\n\\n---\\n\\n**3.** Which verb is NOT listed in the set of example action verbs provided?  \\nA) Act  \\nB) Summarize  \\nC) Retrieve  \\nD) Fly  \\n**Answer: D**\\n\\n---\\n\\n**4.** In the before/after rewriting example, what was the main change made to the original prompt?  \\nA) The location was changed from New York to Manhattan.  \\nB) The instruction was made more concise and focused on action verbs.  \\nC) The age of the children was increased to 5 years old.  \\nD) The prompt was turned into a question.  \\n**Answer: B**\\n\\n---\\n\\n**5.** In the email classification example, what is the instruction given to the model?  \\nA) Generate a new email.  \\nB) Rewrite the email in a polite tone.  \\nC) Classify the email as IMPORTANT or NOT IMPORTANT and explain why.  \\nD) Summarize the email content.  \\n**Answer: C**\",\n",
      "  \"updated_profile\": {\n",
      "    \"completed_quizzes\": [\n",
      "      \"Give me a quiz about design.\"\n",
      "    ],\n",
      "    \"struggling_topics\": [\n",
      "      \"Explain the langchain's use using flashcards\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import json\n",
    "    analyzer = LearningAnalyzer()\n",
    "    \n",
    "    # Test Case 1: Student requesting tutoring\n",
    "    print(\"Running Test Case 1: Student asks for an explanation.\")\n",
    "    response1 = run_educational_assistant(\n",
    "        \"Explain the langchain's use using flashcards\", \n",
    "        \"student_123\", \n",
    "        analyzer\n",
    "    )\n",
    "    print(\"\\n--- Final Response 1 ---\")\n",
    "    print(json.dumps(response1, indent=2))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Test Case 2: Student requesting a quiz\n",
    "    print(\"Running Test Case 2: Student asks for a quiz.\")\n",
    "    response2 = run_educational_assistant(\n",
    "        \"Give me a quiz about design.\", \n",
    "        \"student_123\", \n",
    "        analyzer\n",
    "    )\n",
    "    print(\"\\n--- Final Response 2 ---\")\n",
    "    print(json.dumps(response2, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d616816a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "83fc0ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! 👋 How can I assist you today?', additional_kwargs={'reasoning_content': 'The user says \"Hello world:\" and nothing else. They might be prompting for a response? They might want to see a greeting? The user wrote \"Hello world:\" maybe expecting a response that says \"Hello world!\"? Could be a test. The user likely wants the assistant to respond with \"Hello world!\" or similar. But the colon suggests maybe they want to continue? But nothing else. The instruction: \"You are ChatGPT\". The user likely wants a response. So respond with \"Hello world!\" or maybe ask what they want? The user says \"Hello world:\" - maybe it\\'s a prompt to start a conversation. We can respond with a friendly greeting: \"Hello! How can I help you today?\" That seems safe.'}, response_metadata={'token_usage': {'completion_tokens': 170, 'prompt_tokens': 74, 'total_tokens': 244, 'completion_time': 0.150845883, 'prompt_time': 0.004758248, 'queue_time': 0.085395836, 'total_time': 0.155604131}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_a5ac2a5d7b', 'service_tier': 'on_demand', 'reasoning_effort': 'medium', 'finish_reason': 'stop', 'logprobs': None}, id='run--8aa20e7a-6d0e-4eac-bd53-4b531973c616-0', usage_metadata={'input_tokens': 74, 'output_tokens': 170, 'total_tokens': 244})"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hello world:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "7a1d1bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Will watch for changes in these directories: ['/home/theurieric/Desktop/DirectEd/genAI/module4/assessment/directed_va']\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
      "INFO:     Started reloader process [66659] using WatchFiles\n",
      "ERROR:    Error loading ASGI app. Could not import module \"app\".\n",
      "INFO:     Stopping reloader process [66659]\n"
     ]
    }
   ],
   "source": [
    "# app.py\n",
    "\n",
    "# ... (all your existing imports, classes, and functions from your code)\n",
    "# You should have all the classes (EducationalRetriever, ContentGenerator, etc.)\n",
    "# and the function `run_educational_assistant` defined above.\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "import json\n",
    "\n",
    "# --- Pydantic Model for Request Body ---\n",
    "class UserRequest(BaseModel):\n",
    "    user_id: str\n",
    "    topic: str\n",
    "    request_type: str = \"tutoring\"  # Default to tutoring if not provided\n",
    "\n",
    "# --- FastAPI App Instance ---\n",
    "app = FastAPI()\n",
    "\n",
    "# --- POST Endpoint ---\n",
    "@app.post(\"/api/assistant/chat\")\n",
    "def handle_chat_request(request: UserRequest):\n",
    "    \"\"\"\n",
    "    Handles educational requests from the API.\n",
    "\n",
    "    This endpoint receives a POST request, routes it to the\n",
    "    appropriate LangChain component, and returns a structured response.\n",
    "    \"\"\"\n",
    "    # Initialize components here to keep them scoped to the request\n",
    "    # NOTE: In a real-world app, you might use dependency injection\n",
    "    # or a global cache to avoid re-initializing on every request.\n",
    "    llm = ChatGroq(model=\"mixtral-8x7b-32768\", temperature=0)\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    vector_store = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
    "    retriever_instance = vector_store.as_retriever()\n",
    "    educational_retriever = EducationalRetriever(retriever_instance)\n",
    "    content_generator = ContentGenerator(llm, educational_retriever)\n",
    "    analyzer = LearningAnalyzer()\n",
    "\n",
    "    # Create the request data dictionary to pass to the core function\n",
    "    request_data = request.dict()\n",
    "    \n",
    "    # Dynamic Educational Prompt Injection and Routing\n",
    "    router = PromptTemplate.from_template(\n",
    "        \"Analyze the request: '{request}'. Is it asking for a quiz, or a general explanation (tutoring)? Respond with 'QUIZ' or 'TUTORING'.\"\n",
    "    ) | llm | StrOutputParser()\n",
    "    user_intent = request_data.get(\"request_type\", \"\").upper()\n",
    "    if not user_intent or user_intent not in [\"QUIZ\", \"TUTORING\"]:\n",
    "        user_intent = router.invoke({\"request\": request.topic}).strip().upper()\n",
    "\n",
    "    try:\n",
    "        output_content = \"\"\n",
    "        content_type = \"\"\n",
    "        \n",
    "        # Invoke LangChain components based on user intent\n",
    "        if \"QUIZ\" in user_intent:\n",
    "            output_content = content_generator.generate_quiz(request.topic)\n",
    "            content_type = \"QUIZ\"\n",
    "        else:\n",
    "            output_content = content_generator.answer_generator(request.topic)\n",
    "            content_type = \"TUTORING\"\n",
    "        \n",
    "        # Log and return a personalized response\n",
    "        performance = \"correct\" if \"quiz\" in request.topic.lower() else \"incorrect\"\n",
    "        analyzer.log_performance(request.user_id, request.topic, performance)\n",
    "        \n",
    "        response = {\n",
    "            \"user_type\": \"Student\",  # Assuming all API users are students for simplicity\n",
    "            \"content_type\": content_type,\n",
    "            \"output\": output_content,\n",
    "            \"updated_profile\": analyzer.get_profile(request.user_id)\n",
    "        }\n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"Error!\": \"An error occurred during execution.\", \"details\": str(e)}\n",
    "\n",
    "# --- Run the application (for development) ---\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(\"app:app\", host=\"127.0.0.1\", port=8000, reload=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1a346f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
